{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Workshop Title \u00b6 Welcome to our workshop! In this workshop we'll be using foo to accomplish bar. The goals of this workshop are: Goals! Have fun! About this workshop \u00b6 The introductory page of the workshop is broken down into the following sections: Agenda Compatibility Technology Used Credits Agenda \u00b6 Lab 0: Pre-work Pre-work for the project Lab 1: Some Title Exercise to do the thing Compatibility \u00b6 This workshop has been tested on the following platforms: osName : version X, version Y Technology Used \u00b6 Brief Description Credits \u00b6 Full Name","title":"About the workshop"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#workshop-title","text":"Welcome to our workshop! In this workshop we'll be using foo to accomplish bar. The goals of this workshop are: Goals! Have fun!","title":"Workshop Title"},{"location":"#about-this-workshop","text":"The introductory page of the workshop is broken down into the following sections: Agenda Compatibility Technology Used Credits","title":"About this workshop"},{"location":"#agenda","text":"Lab 0: Pre-work Pre-work for the project Lab 1: Some Title Exercise to do the thing","title":"Agenda"},{"location":"#compatibility","text":"This workshop has been tested on the following platforms: osName : version X, version Y","title":"Compatibility"},{"location":"#technology-used","text":"Brief Description","title":"Technology Used"},{"location":"#credits","text":"Full Name","title":"Credits"},{"location":"generatedContent/","text":"This content is generated! Do not edit directly! Please run aggregate-labs.sh to repopulate with latest content from agenda.txt!","title":"Index"},{"location":"generatedContent/app-modernization-cicd-lab-iks/","text":"Creating a CI/CD Pipeline for deployment to IBM Cloud Kubernetes Service using Jenkins \u00b6 Overview \u00b6 In this lab you will be enabling CI/CD connecting your Git repository with the guestbook app to a Continuous Integration/Continuous Deployment pipeline built with Jenkins that will deploy to a IBM Cloud Kubernetes Service cluster. Note: You will need to create a GitHub account if you don't have one already. Setup \u00b6 If you haven't already, set up the guestbook application that will be deployed: Open a new browser window or tab and go to the guestbook application repository Click on the Fork icon. Note: you need to fork the repo to have full access to turn of Git WebHooks feature for the repo in later section. If you have Git installed on your machine, go ahead and clone your fork of the guestbook application, then cd into that directory. From a terminal window, execute the following commands (replace the url with your forked repo url): git clone https://github.com/ [ git username ] /guestbook cd guestbook Lab Steps \u00b6 We will now configure the CI/CD pipeline with Jenkins to automate application deployment to a IBM Kubernetes cluster. Step 0: Create and collect: API Key, Registry Namespace and Cluster Name \u00b6 We will need all these values when we configure our Jenkins pipeline later. Login to the IBM Cloud console . Make sure you have the right account selected in the dropdown (the one with your Kubernetes cluster which you want to deploy to) and open the cloud shell by clicking the icon on the top right of the screen. Create an API key using the following command (replace [key name] with a name of your choosing). Save the key value by Copy and Pasting it to a text editor, we will use it later in our Jenkins Pipeline. ibmcloud iam api-key-create [ key name ] Create or access a container registry namespace. First, see if you have access to an existing namespace already. ibmcloud cr namespace-list If you get a value above, copy and paste to a text editor for later. If you have no namespaces created, run the following command to create one (replace [namespace name] with a name of your choosing). ibmcloud cr namespace-add [ namespace name ] Access and save the name of your Kubernetes Cluster on IBM Cloud. ibmcloud ks clusters Save the cluster name to a variable by using the following command (replace [cluster name] with your cluster name from above step): export CLUSTER_NAME =[ cluster name ] Step 1: Add Jenkinsfile to the Guestbook App \u00b6 We will add a JenkinsFile to your guestbook repository. Download this JenkinsFile to your machine. Inspect the JenkinsFile to learn what stages we will setup in the next steps. For your convenience, here is a curl command you can use to download the file. curl https://raw.githubusercontent.com/IBMAppModernization/app-modernization-cicd-lab-iks/guestbook-app/Jenkinsfile.ext > Jenkinsfile.ext To add the file your guestbook repo, you can either (option 1) use the git CLI if you have it installed or (option 2) do it from the browser. (Option 1) Save/move the Jenkinsfile to the root of your guestbook project (where you cloned it). Configure git client (if needed): git config --global user.email \"[your email]\" git config --global user.name \"[your first and last name]\" Add the Jenkinsfile and commit the changes: git add . git commit -m \"adding jenkinsfile\" Push the changes to your repo: git push (Option 2) Upload the file and commit using a web browser. From your fork of the github repo, click on Add file , then Upload files . Choose the Jenkinsfile.ext you download earlier and click the Commit changes button. Step 2: Set up the CI/CD pipeline \u00b6 In this section we will be connecting your forked Git repo of this app to set up a Continuous Integration/Continuous Deployment pipeline built with Jenkins. This pipeline contains 3 main steps as follows: Stage Purpose Build Docker Image Builds the Docker image based on the Dockerfile Push Docker Image to Registry Uploads the Docker image to the Docker image registry within ICP Deploy New Docker Image Updates the image tag in the Kubernetes deployment triggering a rolling update More details of this pipeline can be found in the Jenkinsfile.ext . Log into Jenkins using the URL provided to you by your instructor with the credentials provided to you. The pipeline should have already been created for you. Click on your pipeline to open it and then click on the Configure link in the navigation area at the left to change it's properties. Scroll down to the This project is parameterized section, here you will have to set some values to connect this pipeline to your cluster. Set the value of API_KEY to the API_KEY you created and saved earlier for your ibmcloud account. We will be using this key to give Jenkins access to deploy to your cluster and to push images to your container registry. To update the value, click on the 'Change Password' button next to the field and paste your API key. Set the value of the CLUSTER_NAME to the name of your Kubernetes cluster you want to deploy to. Set the value of the REGISTRY_NS to the name of your container registry namespace you viewed (or created) earlier. We will deploy our application image to this location. Update REGION to match the location of your Kubernetes cluster. You can view the location by running ibmcloud ks clusters . Scroll down to the Build Trigger section and select GitHub hook trigger for GIT SCM polling . Scroll down to the Pipeline section and find the Definition drop down menu. Select Pipeline script from SCM and for SCM select Git . For Repository URL enter the url to the cloned repository that you forked earlier (i.e. https://github.com/[your username]/guestbook.git ) Change the Script Path to Jenkinsfile.ext . Click Save . Step 3: Manually trigger a build to test pipeline \u00b6 In Jenkins in the navigation area on the left click on Build with Parameters . Accept the defaults of the parameters and click on Build To see the console output, click on the build number in the Build History and then click on Console Output If the build is successful the end of the console output should look like the following: The Stage View of the pipeline should look like the following: When the pipeline is finish deploying, launch the app to verify the it has been deployed and is running. Run the following command to get the port number of your deployed app: kubectl --namespace default get service guestbook -o jsonpath = '{.spec.ports[0].nodePort}' Run the following command to get the external IP address of the first worker node in your cluster: ibmcloud ks workers --cluster $CLUSTER_NAME | grep -v '^*' | egrep -v \"(ID|OK)\" | awk '{print $2;}' | head -n1 Your app's URL is the IP address of the first worker node with the port number of the deployed app. For example if your external IP is 169.61.73.182 and the port is 30961 the URL will be http://169.61.73.182:30961 Enter the URL in your browser's address bar and verify that the application loads. Step 4: Trigger a build via a commit to Github \u00b6 Now you'll configure Github to trigger your pipeline whenever code is committed. Go back to Github and find your cloned repository Click on the repository settings Under Options select Webhooks and click Add webhook For the Payload URL use <Jenkins URL>/github-webhook/ where <Jenkins URL> is the URL you used to login to Jenkins ( Note Don't forget the trailing / ) Change content type to application/json Accept the other defaults and click Add webhook In the Github file browser drill down to /v1/guestbook/public/index.html Click on the pencil icon to edit index.html and on line 12 locate the header of the page Change Guestbook - v1 to Guestbook - updated! ... or whatever you want! At the bottom of the UI window add a commit message and click on Commit changes Switch back to Jenkins and open the pipeline that you were working on earlier. Verify that your pipeline starts building. When the pipeline is finish deploying, force-refresh ( \u2318 + shift + R on mac) the browser window where you previously loaded the app to verify the change you made. Note: If you closed the browser window, follow steps 5 - 9 of the previous section to get the URL of the application again. Summary \u00b6 You created a Jenkins pipeline to automatically build and deploy an app that has been updated in Github.","title":"Jenkins Lab"},{"location":"generatedContent/app-modernization-cicd-lab-iks/#creating-a-cicd-pipeline-for-deployment-to-ibm-cloud-kubernetes-service-using-jenkins","text":"","title":"Creating a CI/CD Pipeline for deployment to IBM Cloud Kubernetes Service using Jenkins"},{"location":"generatedContent/app-modernization-cicd-lab-iks/#overview","text":"In this lab you will be enabling CI/CD connecting your Git repository with the guestbook app to a Continuous Integration/Continuous Deployment pipeline built with Jenkins that will deploy to a IBM Cloud Kubernetes Service cluster. Note: You will need to create a GitHub account if you don't have one already.","title":"Overview"},{"location":"generatedContent/app-modernization-cicd-lab-iks/#setup","text":"If you haven't already, set up the guestbook application that will be deployed: Open a new browser window or tab and go to the guestbook application repository Click on the Fork icon. Note: you need to fork the repo to have full access to turn of Git WebHooks feature for the repo in later section. If you have Git installed on your machine, go ahead and clone your fork of the guestbook application, then cd into that directory. From a terminal window, execute the following commands (replace the url with your forked repo url): git clone https://github.com/ [ git username ] /guestbook cd guestbook","title":"Setup"},{"location":"generatedContent/app-modernization-cicd-lab-iks/#lab-steps","text":"We will now configure the CI/CD pipeline with Jenkins to automate application deployment to a IBM Kubernetes cluster.","title":"Lab Steps"},{"location":"generatedContent/app-modernization-cicd-lab-iks/#step-0-create-and-collect-api-key-registry-namespace-and-cluster-name","text":"We will need all these values when we configure our Jenkins pipeline later. Login to the IBM Cloud console . Make sure you have the right account selected in the dropdown (the one with your Kubernetes cluster which you want to deploy to) and open the cloud shell by clicking the icon on the top right of the screen. Create an API key using the following command (replace [key name] with a name of your choosing). Save the key value by Copy and Pasting it to a text editor, we will use it later in our Jenkins Pipeline. ibmcloud iam api-key-create [ key name ] Create or access a container registry namespace. First, see if you have access to an existing namespace already. ibmcloud cr namespace-list If you get a value above, copy and paste to a text editor for later. If you have no namespaces created, run the following command to create one (replace [namespace name] with a name of your choosing). ibmcloud cr namespace-add [ namespace name ] Access and save the name of your Kubernetes Cluster on IBM Cloud. ibmcloud ks clusters Save the cluster name to a variable by using the following command (replace [cluster name] with your cluster name from above step): export CLUSTER_NAME =[ cluster name ]","title":"Step 0: Create and collect: API Key, Registry Namespace and Cluster Name"},{"location":"generatedContent/app-modernization-cicd-lab-iks/#step-1-add-jenkinsfile-to-the-guestbook-app","text":"We will add a JenkinsFile to your guestbook repository. Download this JenkinsFile to your machine. Inspect the JenkinsFile to learn what stages we will setup in the next steps. For your convenience, here is a curl command you can use to download the file. curl https://raw.githubusercontent.com/IBMAppModernization/app-modernization-cicd-lab-iks/guestbook-app/Jenkinsfile.ext > Jenkinsfile.ext To add the file your guestbook repo, you can either (option 1) use the git CLI if you have it installed or (option 2) do it from the browser. (Option 1) Save/move the Jenkinsfile to the root of your guestbook project (where you cloned it). Configure git client (if needed): git config --global user.email \"[your email]\" git config --global user.name \"[your first and last name]\" Add the Jenkinsfile and commit the changes: git add . git commit -m \"adding jenkinsfile\" Push the changes to your repo: git push (Option 2) Upload the file and commit using a web browser. From your fork of the github repo, click on Add file , then Upload files . Choose the Jenkinsfile.ext you download earlier and click the Commit changes button.","title":"Step 1: Add Jenkinsfile to the Guestbook App"},{"location":"generatedContent/app-modernization-cicd-lab-iks/#step-2-set-up-the-cicd-pipeline","text":"In this section we will be connecting your forked Git repo of this app to set up a Continuous Integration/Continuous Deployment pipeline built with Jenkins. This pipeline contains 3 main steps as follows: Stage Purpose Build Docker Image Builds the Docker image based on the Dockerfile Push Docker Image to Registry Uploads the Docker image to the Docker image registry within ICP Deploy New Docker Image Updates the image tag in the Kubernetes deployment triggering a rolling update More details of this pipeline can be found in the Jenkinsfile.ext . Log into Jenkins using the URL provided to you by your instructor with the credentials provided to you. The pipeline should have already been created for you. Click on your pipeline to open it and then click on the Configure link in the navigation area at the left to change it's properties. Scroll down to the This project is parameterized section, here you will have to set some values to connect this pipeline to your cluster. Set the value of API_KEY to the API_KEY you created and saved earlier for your ibmcloud account. We will be using this key to give Jenkins access to deploy to your cluster and to push images to your container registry. To update the value, click on the 'Change Password' button next to the field and paste your API key. Set the value of the CLUSTER_NAME to the name of your Kubernetes cluster you want to deploy to. Set the value of the REGISTRY_NS to the name of your container registry namespace you viewed (or created) earlier. We will deploy our application image to this location. Update REGION to match the location of your Kubernetes cluster. You can view the location by running ibmcloud ks clusters . Scroll down to the Build Trigger section and select GitHub hook trigger for GIT SCM polling . Scroll down to the Pipeline section and find the Definition drop down menu. Select Pipeline script from SCM and for SCM select Git . For Repository URL enter the url to the cloned repository that you forked earlier (i.e. https://github.com/[your username]/guestbook.git ) Change the Script Path to Jenkinsfile.ext . Click Save .","title":"Step 2: Set up the CI/CD pipeline"},{"location":"generatedContent/app-modernization-cicd-lab-iks/#step-3-manually-trigger-a-build-to-test-pipeline","text":"In Jenkins in the navigation area on the left click on Build with Parameters . Accept the defaults of the parameters and click on Build To see the console output, click on the build number in the Build History and then click on Console Output If the build is successful the end of the console output should look like the following: The Stage View of the pipeline should look like the following: When the pipeline is finish deploying, launch the app to verify the it has been deployed and is running. Run the following command to get the port number of your deployed app: kubectl --namespace default get service guestbook -o jsonpath = '{.spec.ports[0].nodePort}' Run the following command to get the external IP address of the first worker node in your cluster: ibmcloud ks workers --cluster $CLUSTER_NAME | grep -v '^*' | egrep -v \"(ID|OK)\" | awk '{print $2;}' | head -n1 Your app's URL is the IP address of the first worker node with the port number of the deployed app. For example if your external IP is 169.61.73.182 and the port is 30961 the URL will be http://169.61.73.182:30961 Enter the URL in your browser's address bar and verify that the application loads.","title":"Step 3: Manually trigger a build to test pipeline"},{"location":"generatedContent/app-modernization-cicd-lab-iks/#step-4-trigger-a-build-via-a-commit-to-github","text":"Now you'll configure Github to trigger your pipeline whenever code is committed. Go back to Github and find your cloned repository Click on the repository settings Under Options select Webhooks and click Add webhook For the Payload URL use <Jenkins URL>/github-webhook/ where <Jenkins URL> is the URL you used to login to Jenkins ( Note Don't forget the trailing / ) Change content type to application/json Accept the other defaults and click Add webhook In the Github file browser drill down to /v1/guestbook/public/index.html Click on the pencil icon to edit index.html and on line 12 locate the header of the page Change Guestbook - v1 to Guestbook - updated! ... or whatever you want! At the bottom of the UI window add a commit message and click on Commit changes Switch back to Jenkins and open the pipeline that you were working on earlier. Verify that your pipeline starts building. When the pipeline is finish deploying, force-refresh ( \u2318 + shift + R on mac) the browser window where you previously loaded the app to verify the change you made. Note: If you closed the browser window, follow steps 5 - 9 of the previous section to get the URL of the application again.","title":"Step 4: Trigger a build via a commit to Github"},{"location":"generatedContent/app-modernization-cicd-lab-iks/#summary","text":"You created a Jenkins pipeline to automatically build and deploy an app that has been updated in Github.","title":"Summary"},{"location":"generatedContent/app-modernization-cicd-lab-iks/SUMMARY/","text":"Table of contents \u00b6 Lab - CICD on IKS","title":"Table of contents"},{"location":"generatedContent/app-modernization-cicd-lab-iks/SUMMARY/#table-of-contents","text":"Lab - CICD on IKS","title":"Table of contents"},{"location":"generatedContent/helm101/","text":"Helm 101 \u00b6 Helm is often described as the Kubernetes application package manager. So, what does Helm give you over using kubectl directly? Objectives \u00b6 These labs provide an insight on the advantages of using Helm over using Kubernetes directly through kubectl . In several of the labs there are two scenarios. The first scenario gives an example of how to perform the task using kubectl , the second scenario, using helm . When you complete all the labs, you'll: Understand the core concepts of Helm Understand the advantages of deployment using Helm over Kubernetes directly, looking at: Application management Updates Configuration Revision management Repositories and chart sharing Prerequisites \u00b6 Have a running Kubernetes cluster. See the IBM Cloud Kubernetes Service or Kubernetes Getting Started Guide for details about creating a cluster. Have Helm installed and initialized with the Kubernetes cluster. See Installing Helm on IBM Cloud Kubernetes Service or the Helm Quickstart Guide for getting started with Helm. Helm Overview \u00b6 Helm is a tool that streamlines installation and management of Kubernetes applications. It uses a packaging format called \"charts\", which are a collection of files that describe Kubernetes resources. It can run anywhere (laptop, CI/CD, etc.) and is available for various operating systems, like OSX, Linux and Windows. Helm 3 pivoted from the Helm 2 client-server architecture to a client architecture. The client is still called helm and, there is an improved Go library which encapsulates the Helm logic so that it can be leveraged by different clients. The client is a CLI which users interact with to perform different operations like install/upgrade/delete etc. The client interacts with the Kubernetes API server and the chart repository. It renders Helm template files into Kubernetes manifest files which it uses to perform operations on the Kubernetes cluster via the Kubernetes API. See the Helm Architecture for more details. A chart is organized as a collection of files inside of a directory where the directory name is the name of the chart. It contains template YAML files which facilitates providing configuration values at runtime and eliminates the need of modifying YAML files. These templates provide programming logic as they are based on the Go template language , functions from the Sprig lib and other specialized functions . The chart repository is a location where packaged charts can be stored and shared. This is akin to the image repository in Docker. Refer to The Chart Repository Guide for more details. Helm Abstractions \u00b6 Helm terms: Chart - It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. A chart is basically a package of pre-configured Kubernetes resources. Config - Contains configuration information that can be merged into a packaged chart to create a releasable object. helm - Helm client. It renders charts into manifest files. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. Release - An instance of a chart running in a Kubernetes cluster. Repository - Place where charts reside and can be shared with others. To get started, head on over to Lab 1 .","title":"About the workshop"},{"location":"generatedContent/helm101/#helm-101","text":"Helm is often described as the Kubernetes application package manager. So, what does Helm give you over using kubectl directly?","title":"Helm 101"},{"location":"generatedContent/helm101/#objectives","text":"These labs provide an insight on the advantages of using Helm over using Kubernetes directly through kubectl . In several of the labs there are two scenarios. The first scenario gives an example of how to perform the task using kubectl , the second scenario, using helm . When you complete all the labs, you'll: Understand the core concepts of Helm Understand the advantages of deployment using Helm over Kubernetes directly, looking at: Application management Updates Configuration Revision management Repositories and chart sharing","title":"Objectives"},{"location":"generatedContent/helm101/#prerequisites","text":"Have a running Kubernetes cluster. See the IBM Cloud Kubernetes Service or Kubernetes Getting Started Guide for details about creating a cluster. Have Helm installed and initialized with the Kubernetes cluster. See Installing Helm on IBM Cloud Kubernetes Service or the Helm Quickstart Guide for getting started with Helm.","title":"Prerequisites"},{"location":"generatedContent/helm101/#helm-overview","text":"Helm is a tool that streamlines installation and management of Kubernetes applications. It uses a packaging format called \"charts\", which are a collection of files that describe Kubernetes resources. It can run anywhere (laptop, CI/CD, etc.) and is available for various operating systems, like OSX, Linux and Windows. Helm 3 pivoted from the Helm 2 client-server architecture to a client architecture. The client is still called helm and, there is an improved Go library which encapsulates the Helm logic so that it can be leveraged by different clients. The client is a CLI which users interact with to perform different operations like install/upgrade/delete etc. The client interacts with the Kubernetes API server and the chart repository. It renders Helm template files into Kubernetes manifest files which it uses to perform operations on the Kubernetes cluster via the Kubernetes API. See the Helm Architecture for more details. A chart is organized as a collection of files inside of a directory where the directory name is the name of the chart. It contains template YAML files which facilitates providing configuration values at runtime and eliminates the need of modifying YAML files. These templates provide programming logic as they are based on the Go template language , functions from the Sprig lib and other specialized functions . The chart repository is a location where packaged charts can be stored and shared. This is akin to the image repository in Docker. Refer to The Chart Repository Guide for more details.","title":"Helm Overview"},{"location":"generatedContent/helm101/#helm-abstractions","text":"Helm terms: Chart - It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. A chart is basically a package of pre-configured Kubernetes resources. Config - Contains configuration information that can be merged into a packaged chart to create a releasable object. helm - Helm client. It renders charts into manifest files. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. Release - An instance of a chart running in a Kubernetes cluster. Repository - Place where charts reside and can be shared with others. To get started, head on over to Lab 1 .","title":"Helm Abstractions"},{"location":"generatedContent/helm101/SUMMARY/","text":"Summary \u00b6 Workshop \u00b6 Lab 0 Lab 1 Lab 2 Lab 3 Lab 4 Resources \u00b6 IBM Developer","title":"Summary"},{"location":"generatedContent/helm101/SUMMARY/#summary","text":"","title":"Summary"},{"location":"generatedContent/helm101/SUMMARY/#workshop","text":"Lab 0 Lab 1 Lab 2 Lab 3 Lab 4","title":"Workshop"},{"location":"generatedContent/helm101/SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"generatedContent/helm101/Lab0/","text":"Lab 0. Installing Helm on IBM Cloud Kubernetes Service \u00b6 The Helm client ( helm ) can be installed from source or pre-built binary releases. In this lab, we are going to use the pre-built binary release (Linux amd64) from the Helm community. Refer to the Helm install docs for more details. Prerequisites \u00b6 Create a Kubernetes cluster with IBM Cloud Kubernetes Service , following the steps to also configure the IBM Cloud CLI with the Kubernetes Service plug-in. Installing the Helm Client (helm) \u00b6 Download the latest release of Helm v3 for your environment, the steps below are for Linux amd64 , adjust the examples as needed for your environment. Unpack it: $ tar -zxvf helm-v3.<x>.<y>-linux-amd64.tgz . Find the helm binary in the unpacked directory, and move it to its desired location: mv linux-amd64/helm /usr/local/bin/helm . It is best if the location you copy to is pathed, as it avoids having to path the helm commands. The Helm client is now installed and can be tested with the command, helm help . Conclusion \u00b6 You are now ready to start using Helm.","title":"Lab 0. Installing Helm on IKS"},{"location":"generatedContent/helm101/Lab0/#lab-0-installing-helm-on-ibm-cloud-kubernetes-service","text":"The Helm client ( helm ) can be installed from source or pre-built binary releases. In this lab, we are going to use the pre-built binary release (Linux amd64) from the Helm community. Refer to the Helm install docs for more details.","title":"Lab 0. Installing Helm on IBM Cloud Kubernetes Service"},{"location":"generatedContent/helm101/Lab0/#prerequisites","text":"Create a Kubernetes cluster with IBM Cloud Kubernetes Service , following the steps to also configure the IBM Cloud CLI with the Kubernetes Service plug-in.","title":"Prerequisites"},{"location":"generatedContent/helm101/Lab0/#installing-the-helm-client-helm","text":"Download the latest release of Helm v3 for your environment, the steps below are for Linux amd64 , adjust the examples as needed for your environment. Unpack it: $ tar -zxvf helm-v3.<x>.<y>-linux-amd64.tgz . Find the helm binary in the unpacked directory, and move it to its desired location: mv linux-amd64/helm /usr/local/bin/helm . It is best if the location you copy to is pathed, as it avoids having to path the helm commands. The Helm client is now installed and can be tested with the command, helm help .","title":"Installing the Helm Client (helm)"},{"location":"generatedContent/helm101/Lab0/#conclusion","text":"You are now ready to start using Helm.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab1/","text":"Lab 1. Deploy with Helm \u00b6 Let's investigate how Helm can help us focus on other things by letting a chart do the work for us. We'll first deploy an application to a Kubernetes cluster by using kubectl and then show how we can offload the work to a chart by deploying the same app with Helm. The application is the Guestbook App , which is a sample multi-tier web application. Scenario 1: Deploy the application using kubectl \u00b6 In this part of the lab, we will deploy the application using the Kubernetes client kubectl . We will use Version 1 of the app for deploying here. If you already have a copy of the guestbook application installed from the kube101 lab , skip this section and go the helm example in Scenario 2 . Clone the Guestbook App repo to get the files: git clone https://github.com/IBM/guestbook.git Use the configuration files in the cloned Git repository to deploy the containers and create services for them by using the following commands: $ cd guestbook/v1 $ kubectl create -f redis-master-deployment.yaml deployment.apps/redis-master created $ kubectl create -f redis-master-service.yaml service/redis-master created $ kubectl create -f redis-slave-deployment.yaml deployment.apps/redis-slave created $ kubectl create -f redis-slave-service.yaml service/redis-slave created $ kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created $ kubectl create -f guestbook-service.yaml service/guestbook created Refer to the guestbook README for more details. View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). * **Local Host:** If you are running Kubernetes locally, view the guestbook by navigating to `http://localhost:3000` in your browser. * **Remote Host:** 1. To view the guestbook on a remote host, locate the external IP and port of the load balancer in the **EXTERNAL-IP** and **PORTS** columns of the `$ kubectl get services` output. ```console $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook LoadBalancer 172.21.252.107 50.23.5.136 3000:31838/TCP 14m redis-master ClusterIP 172.21.97.222 <none> 6379/TCP 14m redis-slave ClusterIP 172.21.43.70 <none> 6379/TCP 14m ......... ``` In this scenario the URL is `http://50.23.5.136:31838`. Note: If no external IP is assigned, then you can get the external IP with the following command: ```console $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 ``` In this scenario the URL is `http://173.193.92.112:31838`. 2. Navigate to the output given (for example `http://50.23.5.136:31838`) in your browser. You should see the guestbook now displaying in your browser: ![Guestbook](../images/guestbook-page.png) Scenario 2: Deploy the application using Helm \u00b6 In this part of the lab, we will deploy the application by using Helm. We will set a release name of guestbook-demo to distinguish it from the previous deployment. The Helm chart is available here . Clone the Helm 101 repo to get the files: git clone https://github.com/IBM/helm101 A chart is defined as a collection of files that describe a related set of Kubernetes resources. We probably then should take a look at the the files before we go and install the chart. The files for the guestbook chart are as follows: . \u251c\u2500\u2500 Chart.yaml \\\\ A YAML file containing information about the chart \u251c\u2500\u2500 LICENSE \\\\ A plain text file containing the license for the chart \u251c\u2500\u2500 README.md \\\\ A README providing information about the chart usage, configuration, installation etc. \u251c\u2500\u2500 templates \\\\ A directory of templates that will generate valid Kubernetes manifest files when combined with values.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl \\\\ Template helpers/definitions that are re-used throughout the chart \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \\\\ Guestbook app container resource \u2502 \u251c\u2500\u2500 guestbook-service.yaml \\\\ Guestbook app service resource \u2502 \u251c\u2500\u2500 NOTES.txt \\\\ A plain text file containing short usage notes about how to access the app post install \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \\\\ Redis master container resource \u2502 \u251c\u2500\u2500 redis-master-service.yaml \\\\ Redis master service resource \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \\\\ Redis slave container resource \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \\\\ Redis slave service resource \u2514\u2500\u2500 values.yaml \\\\ The default configuration values for the chart Note: The template files shown above will be rendered into Kubernetes manifest files before being passed to the Kubernetes API server. Therefore, they map to the manifest files that we deployed when we used kubectl (minus the helper and notes files). Let's go ahead and install the chart now. If the helm-demo namespace does not exist, you will need to create it using: kubectl create namespace helm-demo Install the app as a Helm chart: $ cd helm101/charts $ helm install guestbook-demo ./guestbook/ --namespace helm-demo NAME: guestbook-demo ... You should see output similar to the following: NAME: guestbook-demo LAST DEPLOYED: Mon Feb 24 18:08:02 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace helm-demo' export SERVICE_IP=$(kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 The chart install performs the Kubernetes deployments and service creations of the redis master and slaves, and the guestbook app, as one. This is because the chart is a collection of files that describe a related set of Kubernetes resources and Helm manages the creation of these resources via the Kubernetes API. Check the deployment: kubectl get deployment guestbook-demo --namespace helm-demo You should see output similar to the following: $ kubectl get deployment guestbook-demo --namespace helm-dem NAME READY UP-TO-DATE AVAILABLE AGE guestbook-demo 2/2 2 2 51m To check the status of the running application pods, use: kubectl get pods --namespace helm-demo You should see output similar to the following: $ kubectl get pods --namespace helm-demo NAME READY STATUS RESTARTS AGE guestbook-demo-6c9cf8b9-jwbs9 1/1 Running 0 52m guestbook-demo-6c9cf8b9-qk4fb 1/1 Running 0 52m redis-master-5d8b66464f-j72jf 1/1 Running 0 52m redis-slave-586b4c847c-2xt99 1/1 Running 0 52m redis-slave-586b4c847c-q7rq5 1/1 Running 0 52m To check the services, use: kubectl get services --namespace helm-demo $ kubectl get services --namespace helm-demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31367/TCP 52m redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 52m redis-slave ClusterIP 172.21.176.148 <none> 6379/TCP 52m View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). * **Local Host:** If you are running Kubernetes locally, view the guestbook by navigating to `http://localhost:3000` in your browser. * **Remote Host:** 1. To view the guestbook on a remote host, locate the external IP and the port of the load balancer by following the \"NOTES\" section in the install output. The commands will be similar to the following: ```console $ export SERVICE_IP=$(kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo http://$SERVICE_IP http://50.23.5.136 ``` Combine the service IP with the port of the service printed earlier. In this scenario the URL is `http://50.23.5.136:31367`. Note: If no external IP is assigned, then you can get the external IP with the following command: ```console $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 ``` In this scenario the URL is `http://173.193.92.112:31367`. 2. Navigate to the output given (for example `http://50.23.5.136:31367`) in your browser. You should see the guestbook now displaying in your browser: ![Guestbook](../images/guestbook-page.png) Conclusion \u00b6 Congratulations, you have now deployed an application by using two different methods to Kubernetes! From this lab, you can see that using Helm required less commands and less to think about (by giving it the chart path and not the individual files) versus using kubectl . Helm's application management provides the user with this simplicity. Move on to the next lab, Lab2 , to learn how to update our running app when the chart has been changed.","title":"Lab 1. Deploy with Helm"},{"location":"generatedContent/helm101/Lab1/#lab-1-deploy-with-helm","text":"Let's investigate how Helm can help us focus on other things by letting a chart do the work for us. We'll first deploy an application to a Kubernetes cluster by using kubectl and then show how we can offload the work to a chart by deploying the same app with Helm. The application is the Guestbook App , which is a sample multi-tier web application.","title":"Lab 1. Deploy with Helm"},{"location":"generatedContent/helm101/Lab1/#scenario-1-deploy-the-application-using-kubectl","text":"In this part of the lab, we will deploy the application using the Kubernetes client kubectl . We will use Version 1 of the app for deploying here. If you already have a copy of the guestbook application installed from the kube101 lab , skip this section and go the helm example in Scenario 2 . Clone the Guestbook App repo to get the files: git clone https://github.com/IBM/guestbook.git Use the configuration files in the cloned Git repository to deploy the containers and create services for them by using the following commands: $ cd guestbook/v1 $ kubectl create -f redis-master-deployment.yaml deployment.apps/redis-master created $ kubectl create -f redis-master-service.yaml service/redis-master created $ kubectl create -f redis-slave-deployment.yaml deployment.apps/redis-slave created $ kubectl create -f redis-slave-service.yaml service/redis-slave created $ kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created $ kubectl create -f guestbook-service.yaml service/guestbook created Refer to the guestbook README for more details. View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). * **Local Host:** If you are running Kubernetes locally, view the guestbook by navigating to `http://localhost:3000` in your browser. * **Remote Host:** 1. To view the guestbook on a remote host, locate the external IP and port of the load balancer in the **EXTERNAL-IP** and **PORTS** columns of the `$ kubectl get services` output. ```console $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook LoadBalancer 172.21.252.107 50.23.5.136 3000:31838/TCP 14m redis-master ClusterIP 172.21.97.222 <none> 6379/TCP 14m redis-slave ClusterIP 172.21.43.70 <none> 6379/TCP 14m ......... ``` In this scenario the URL is `http://50.23.5.136:31838`. Note: If no external IP is assigned, then you can get the external IP with the following command: ```console $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 ``` In this scenario the URL is `http://173.193.92.112:31838`. 2. Navigate to the output given (for example `http://50.23.5.136:31838`) in your browser. You should see the guestbook now displaying in your browser: ![Guestbook](../images/guestbook-page.png)","title":"Scenario 1: Deploy the application using kubectl"},{"location":"generatedContent/helm101/Lab1/#scenario-2-deploy-the-application-using-helm","text":"In this part of the lab, we will deploy the application by using Helm. We will set a release name of guestbook-demo to distinguish it from the previous deployment. The Helm chart is available here . Clone the Helm 101 repo to get the files: git clone https://github.com/IBM/helm101 A chart is defined as a collection of files that describe a related set of Kubernetes resources. We probably then should take a look at the the files before we go and install the chart. The files for the guestbook chart are as follows: . \u251c\u2500\u2500 Chart.yaml \\\\ A YAML file containing information about the chart \u251c\u2500\u2500 LICENSE \\\\ A plain text file containing the license for the chart \u251c\u2500\u2500 README.md \\\\ A README providing information about the chart usage, configuration, installation etc. \u251c\u2500\u2500 templates \\\\ A directory of templates that will generate valid Kubernetes manifest files when combined with values.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl \\\\ Template helpers/definitions that are re-used throughout the chart \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \\\\ Guestbook app container resource \u2502 \u251c\u2500\u2500 guestbook-service.yaml \\\\ Guestbook app service resource \u2502 \u251c\u2500\u2500 NOTES.txt \\\\ A plain text file containing short usage notes about how to access the app post install \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \\\\ Redis master container resource \u2502 \u251c\u2500\u2500 redis-master-service.yaml \\\\ Redis master service resource \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \\\\ Redis slave container resource \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \\\\ Redis slave service resource \u2514\u2500\u2500 values.yaml \\\\ The default configuration values for the chart Note: The template files shown above will be rendered into Kubernetes manifest files before being passed to the Kubernetes API server. Therefore, they map to the manifest files that we deployed when we used kubectl (minus the helper and notes files). Let's go ahead and install the chart now. If the helm-demo namespace does not exist, you will need to create it using: kubectl create namespace helm-demo Install the app as a Helm chart: $ cd helm101/charts $ helm install guestbook-demo ./guestbook/ --namespace helm-demo NAME: guestbook-demo ... You should see output similar to the following: NAME: guestbook-demo LAST DEPLOYED: Mon Feb 24 18:08:02 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace helm-demo' export SERVICE_IP=$(kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 The chart install performs the Kubernetes deployments and service creations of the redis master and slaves, and the guestbook app, as one. This is because the chart is a collection of files that describe a related set of Kubernetes resources and Helm manages the creation of these resources via the Kubernetes API. Check the deployment: kubectl get deployment guestbook-demo --namespace helm-demo You should see output similar to the following: $ kubectl get deployment guestbook-demo --namespace helm-dem NAME READY UP-TO-DATE AVAILABLE AGE guestbook-demo 2/2 2 2 51m To check the status of the running application pods, use: kubectl get pods --namespace helm-demo You should see output similar to the following: $ kubectl get pods --namespace helm-demo NAME READY STATUS RESTARTS AGE guestbook-demo-6c9cf8b9-jwbs9 1/1 Running 0 52m guestbook-demo-6c9cf8b9-qk4fb 1/1 Running 0 52m redis-master-5d8b66464f-j72jf 1/1 Running 0 52m redis-slave-586b4c847c-2xt99 1/1 Running 0 52m redis-slave-586b4c847c-q7rq5 1/1 Running 0 52m To check the services, use: kubectl get services --namespace helm-demo $ kubectl get services --namespace helm-demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31367/TCP 52m redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 52m redis-slave ClusterIP 172.21.176.148 <none> 6379/TCP 52m View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). * **Local Host:** If you are running Kubernetes locally, view the guestbook by navigating to `http://localhost:3000` in your browser. * **Remote Host:** 1. To view the guestbook on a remote host, locate the external IP and the port of the load balancer by following the \"NOTES\" section in the install output. The commands will be similar to the following: ```console $ export SERVICE_IP=$(kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo http://$SERVICE_IP http://50.23.5.136 ``` Combine the service IP with the port of the service printed earlier. In this scenario the URL is `http://50.23.5.136:31367`. Note: If no external IP is assigned, then you can get the external IP with the following command: ```console $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 ``` In this scenario the URL is `http://173.193.92.112:31367`. 2. Navigate to the output given (for example `http://50.23.5.136:31367`) in your browser. You should see the guestbook now displaying in your browser: ![Guestbook](../images/guestbook-page.png)","title":"Scenario 2: Deploy the application using Helm"},{"location":"generatedContent/helm101/Lab1/#conclusion","text":"Congratulations, you have now deployed an application by using two different methods to Kubernetes! From this lab, you can see that using Helm required less commands and less to think about (by giving it the chart path and not the individual files) versus using kubectl . Helm's application management provides the user with this simplicity. Move on to the next lab, Lab2 , to learn how to update our running app when the chart has been changed.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab2/","text":"Lab 2. Make changes with Helm \u00b6 In Lab 1 , we installed the guestbook sample app by using Helm and saw the benefits over using kubectl . You probably think that you're done and know enough to use Helm. But what about updates or improvements to the chart? How do you update your running app to pick up these changes? In this lab, we're going to look at how to update our running app when the chart has been changed. To demonstrate this, we're going to make changes to the original guestbook chart by: Removing the Redis slaves and using just the in-memory DB Changing the type from LoadBalancer to NodePort . It seems contrived but the goal of this lab is to show you how to update your apps with Kubernetes and Helm. So, how easy is it to do this? Let's take a look below. Scenario 1: Update the application using kubectl \u00b6 In this part of the lab we will update the previously deployed application Guestbook , using Kubernetes directly. This is an optional step that is not technically required to update your running app. The reason for doing this step is \"house keeping\" - you want to have the correct files for the current configuration that you have deployed. This avoids making mistakes if you have future updates or even rollbacks. In this updated configuration, we remove the Redis slaves. To have the directory match the configuration, move/archive or simply remove the Redis slave files from the guestbook repo tree: cd guestbook/v1 rm redis-slave-service.yaml rm redis-slave-deployment.yaml Note: you can reclaim these files later with a git checkout -- <filename> command, if desired Delete the Redis slave service and pods: $ kubectl delete svc redis-slave --namespace default service \"redis-slave\" deleted $ kubectl delete deployment redis-slave --namespace default deployment.extensions \"redis-slave\" deleted Update the guestbook service from LoadBalancer to NodePort type: sed -i.bak 's/LoadBalancer/NodePort/g' guestbook-service.yaml Note: you can reset the files later with a git checkout -- <filename> command, if desired Delete the guestbook service: kubectl delete svc guestbook --namespace default Re-create the service with NodePort type: kubectl create -f guestbook-service.yaml Check the updates, using kubectl get all --namespace default $ kubectl get all --namespace default NAME READY STATUS RESTARTS AGE pod/guestbook-v1-7fc76dc46-9r4s7 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-hspnk 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-sxzkt 1/1 Running 0 1h pod/redis-master-5d8b66464f-pvbl9 1/1 Running 0 1h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook NodePort 172.21.45.29 <none> 3000:31989/TCP 31s service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 9d service/redis-master ClusterIP 172.21.232.61 <none> 6379/TCP 1h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 3/3 3 3 1h deployment.apps/redis-master 1/1 1 1 1h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-7fc76dc46 3 3 3 1h replicaset.apps/redis-master-5d8b66464f 1 1 1 1h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31989 in this output case) to the guestbook service. All redis-slave resources have been removed. View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier. Scenario 2: Update the application using Helm \u00b6 In this section, we'll update the previously deployed guestbook-demo application by using Helm. Before we start, let's take a few minutes to see how Helm simplifies the process compared to using Kubernetes directly. Helm's use of a template language provides great flexibility and power to chart authors, which removes the complexity to the chart user. In the guestbook example, we'll use the following capabilities of templating: Values: An object that provides access to the values passed into the chart. An example of this is in guestbook-service , which contains the line type: {{ .Values.service.type }} . This line provides the capability to set the service type during an upgrade or install. Control structures: Also called \u201cactions\u201d in template parlance, control structures provide the template author with the ability to control the flow of a template\u2019s generation. An example of this is in redis-slave-service , which contains the line {{- if .Values.redis.slaveEnabled -}} . This line allows us to enable/disable the REDIS master/slave during an upgrade or install. The complete redis-slave-service.yaml file shown below, demonstrates how the file becomes redundant when the slaveEnabled flag is disabled and also how the port value is set. There are more examples of templating functionality in the other chart files. {{ - if .Values.redis.slaveEnabled - }} apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : {{ .Values.redis.port }} targetPort : redis-server selector : app : redis role : slave {{ - end }} Enough talking about the theory. Now let's give it a go! First, lets check the app we deployed in Lab 1 with Helm. This can be done by checking the Helm releases: helm list -n helm-demo Note that we specify the namespace. If not specified, it uses the current namespace context. You should see output similar to the following: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 1 2020-02-24 18:08:02.017401264 +0000 UTC deployed guestbook-0.2.0 The list command provides the list of deployed charts (releases) giving information of chart version, namespace, number of updates (revisions) etc. We now know the release is there from step 1., so we can update the application: $ cd helm101/charts $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! ... A Helm upgrade takes an existing release and upgrades it according to the information you provide. You should see output similar to the following: $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 14:23:27 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace helm-demo -o jsonpath=\"{.spec.ports[0].nodePort}\" services guestbook-demo) export NODE_IP=$(kubectl get nodes --namespace helm-demo -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT The upgrade command upgrades the app to a specified version of a chart, removes the redis-slave resources, and updates the app service.type to NodePort . Check the updates, using kubectl get all --namespace helm-demo : $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn2 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo NodePort 172.21.43.244 <none> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-6c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31202 in this output case) to the guestbook service. All redis-slave resources have been removed. When you check the Helm release with helm list -n helm-demo , you will see the revision and date has been updated: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 2 2020-02-25 14:23:27.06732381 +0000 UTC deployed guestbook-0.2.0 View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier. Conclusion \u00b6 Congratulations, you have now updated the applications! Helm does not require any manual changing of resources and is therefore so much easier to upgrade! All configurations can be set on the fly on the command line or by using override files. This is made possible from when the logic was added to the template files, which enables or disables the capability, depending on the flag set. Check out Lab 3 to get an insight into revision management.","title":"Lab 2. Make changes with Helm"},{"location":"generatedContent/helm101/Lab2/#lab-2-make-changes-with-helm","text":"In Lab 1 , we installed the guestbook sample app by using Helm and saw the benefits over using kubectl . You probably think that you're done and know enough to use Helm. But what about updates or improvements to the chart? How do you update your running app to pick up these changes? In this lab, we're going to look at how to update our running app when the chart has been changed. To demonstrate this, we're going to make changes to the original guestbook chart by: Removing the Redis slaves and using just the in-memory DB Changing the type from LoadBalancer to NodePort . It seems contrived but the goal of this lab is to show you how to update your apps with Kubernetes and Helm. So, how easy is it to do this? Let's take a look below.","title":"Lab 2. Make changes with Helm"},{"location":"generatedContent/helm101/Lab2/#scenario-1-update-the-application-using-kubectl","text":"In this part of the lab we will update the previously deployed application Guestbook , using Kubernetes directly. This is an optional step that is not technically required to update your running app. The reason for doing this step is \"house keeping\" - you want to have the correct files for the current configuration that you have deployed. This avoids making mistakes if you have future updates or even rollbacks. In this updated configuration, we remove the Redis slaves. To have the directory match the configuration, move/archive or simply remove the Redis slave files from the guestbook repo tree: cd guestbook/v1 rm redis-slave-service.yaml rm redis-slave-deployment.yaml Note: you can reclaim these files later with a git checkout -- <filename> command, if desired Delete the Redis slave service and pods: $ kubectl delete svc redis-slave --namespace default service \"redis-slave\" deleted $ kubectl delete deployment redis-slave --namespace default deployment.extensions \"redis-slave\" deleted Update the guestbook service from LoadBalancer to NodePort type: sed -i.bak 's/LoadBalancer/NodePort/g' guestbook-service.yaml Note: you can reset the files later with a git checkout -- <filename> command, if desired Delete the guestbook service: kubectl delete svc guestbook --namespace default Re-create the service with NodePort type: kubectl create -f guestbook-service.yaml Check the updates, using kubectl get all --namespace default $ kubectl get all --namespace default NAME READY STATUS RESTARTS AGE pod/guestbook-v1-7fc76dc46-9r4s7 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-hspnk 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-sxzkt 1/1 Running 0 1h pod/redis-master-5d8b66464f-pvbl9 1/1 Running 0 1h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook NodePort 172.21.45.29 <none> 3000:31989/TCP 31s service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 9d service/redis-master ClusterIP 172.21.232.61 <none> 6379/TCP 1h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 3/3 3 3 1h deployment.apps/redis-master 1/1 1 1 1h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-7fc76dc46 3 3 3 1h replicaset.apps/redis-master-5d8b66464f 1 1 1 1h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31989 in this output case) to the guestbook service. All redis-slave resources have been removed. View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier.","title":"Scenario 1: Update the application using kubectl"},{"location":"generatedContent/helm101/Lab2/#scenario-2-update-the-application-using-helm","text":"In this section, we'll update the previously deployed guestbook-demo application by using Helm. Before we start, let's take a few minutes to see how Helm simplifies the process compared to using Kubernetes directly. Helm's use of a template language provides great flexibility and power to chart authors, which removes the complexity to the chart user. In the guestbook example, we'll use the following capabilities of templating: Values: An object that provides access to the values passed into the chart. An example of this is in guestbook-service , which contains the line type: {{ .Values.service.type }} . This line provides the capability to set the service type during an upgrade or install. Control structures: Also called \u201cactions\u201d in template parlance, control structures provide the template author with the ability to control the flow of a template\u2019s generation. An example of this is in redis-slave-service , which contains the line {{- if .Values.redis.slaveEnabled -}} . This line allows us to enable/disable the REDIS master/slave during an upgrade or install. The complete redis-slave-service.yaml file shown below, demonstrates how the file becomes redundant when the slaveEnabled flag is disabled and also how the port value is set. There are more examples of templating functionality in the other chart files. {{ - if .Values.redis.slaveEnabled - }} apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : {{ .Values.redis.port }} targetPort : redis-server selector : app : redis role : slave {{ - end }} Enough talking about the theory. Now let's give it a go! First, lets check the app we deployed in Lab 1 with Helm. This can be done by checking the Helm releases: helm list -n helm-demo Note that we specify the namespace. If not specified, it uses the current namespace context. You should see output similar to the following: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 1 2020-02-24 18:08:02.017401264 +0000 UTC deployed guestbook-0.2.0 The list command provides the list of deployed charts (releases) giving information of chart version, namespace, number of updates (revisions) etc. We now know the release is there from step 1., so we can update the application: $ cd helm101/charts $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! ... A Helm upgrade takes an existing release and upgrades it according to the information you provide. You should see output similar to the following: $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 14:23:27 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace helm-demo -o jsonpath=\"{.spec.ports[0].nodePort}\" services guestbook-demo) export NODE_IP=$(kubectl get nodes --namespace helm-demo -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT The upgrade command upgrades the app to a specified version of a chart, removes the redis-slave resources, and updates the app service.type to NodePort . Check the updates, using kubectl get all --namespace helm-demo : $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn2 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo NodePort 172.21.43.244 <none> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-6c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31202 in this output case) to the guestbook service. All redis-slave resources have been removed. When you check the Helm release with helm list -n helm-demo , you will see the revision and date has been updated: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 2 2020-02-25 14:23:27.06732381 +0000 UTC deployed guestbook-0.2.0 View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier.","title":"Scenario 2: Update the application using Helm"},{"location":"generatedContent/helm101/Lab2/#conclusion","text":"Congratulations, you have now updated the applications! Helm does not require any manual changing of resources and is therefore so much easier to upgrade! All configurations can be set on the fly on the command line or by using override files. This is made possible from when the logic was added to the template files, which enables or disables the capability, depending on the flag set. Check out Lab 3 to get an insight into revision management.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab3/","text":"Lab 3. Keeping track of the deployed application \u00b6 Let's say you deployed different release versions of your application (i.e., you upgraded the running application). How do you keep track of the versions and how can you do a rollback? Scenario 1: Revision management using Kubernetes \u00b6 In this part of the lab, we should illustrate revision management of guestbook by using Kubernetes directly, but we can't. This is because Kubernetes does not provide any support for revision management. The onus is on you to manage your systems and any updates or changes you make. However, we can use Helm to conduct revision management. Scenario 2: Revision management using Helm \u00b6 In this part of the lab, we illustrate revision management on the deployed application guestbook-demo by using Helm. With Helm, every time an install, upgrade, or rollback happens, the revision number is incremented by 1. The first revision number is always 1. Helm persists release metadata in Secrets (default) or ConfigMaps, stored in the Kubernetes cluster. Every time your release changes, it appends that to the existing data. This provides Helm with the capability to rollback to a previous release. Let's see how this works in practice. Check the number of deployments: helm history guestbook-demo -n helm-demo You should see output similar to the following because we did an upgrade in Lab 2 after the initial install in Lab 1 : $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 deployed guestbook-0.2.0 Upgrade complete Roll back to the previous revision: In this rollback, Helm checks the changes that occured when upgrading from the revision 1 to revision 2. This information enables it to makes the calls to the Kubernetes API server, to update the deployed application as per the initial deployment - in other words with Redis slaves and using a load balancer. Rollback with this command: helm rollback guestbook-demo 1 -n helm-demo $ helm rollback guestbook-demo 1 -n helm-demo Rollback was a success! Happy Helming! Check the history again: helm history guestbook-demo -n helm-demo You should see output similar to the following: $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 superseded guestbook-0.2.0 Upgrade complete 3 Tue Feb 25 14:53:45 2020 deployed guestbook-0.2.0 Rollback to 1 Check the rollback, using: kubectl get all --namespace helm-demo $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h pod/redis-slave-586b4c847c-tkfj5 1/1 Running 0 5m15s pod/redis-slave-586b4c847c-xxrdn 1/1 Running 0 5m15s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h service/redis-slave ClusterIP 172.21.232.16 <none> 6379/TCP 5m15s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h deployment.apps/redis-slave 2/2 2 2 5m15s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-26c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h replicaset.apps/redis-slave-586b4c847c 2 2 2 5m15s You can see from the output that the app service is the service type of LoadBalancer again and the Redis master/slave deployment has returned. This shows a complete rollback from the upgrade in Lab 2 Conclusion \u00b6 From this lab, we can say that Helm does revision management well and Kubernetes does not have the capability built in! You might be wondering why we need helm rollback when you could just re-run the helm upgrade from a previous version. And that's a good question. Technically, you should end up with the same resources (with same parameters) deployed. However, the advantage of using helm rollback is that helm manages (ie. remembers) all of the variations/parameters of the previous helm install\\upgrade for you. Doing the rollback via a helm upgrade requires you (and your entire team) to manually track how the command was previously executed. That's not only tedious but very error prone. It is much easier, safer and reliable to let Helm manage all of that for you and all you need to do it tell it which previous version to go back to, and it does the rest. Lab 4 awaits.","title":"Lab 3. Keeping track of the deployed application"},{"location":"generatedContent/helm101/Lab3/#lab-3-keeping-track-of-the-deployed-application","text":"Let's say you deployed different release versions of your application (i.e., you upgraded the running application). How do you keep track of the versions and how can you do a rollback?","title":"Lab 3. Keeping track of the deployed application"},{"location":"generatedContent/helm101/Lab3/#scenario-1-revision-management-using-kubernetes","text":"In this part of the lab, we should illustrate revision management of guestbook by using Kubernetes directly, but we can't. This is because Kubernetes does not provide any support for revision management. The onus is on you to manage your systems and any updates or changes you make. However, we can use Helm to conduct revision management.","title":"Scenario 1: Revision management using Kubernetes"},{"location":"generatedContent/helm101/Lab3/#scenario-2-revision-management-using-helm","text":"In this part of the lab, we illustrate revision management on the deployed application guestbook-demo by using Helm. With Helm, every time an install, upgrade, or rollback happens, the revision number is incremented by 1. The first revision number is always 1. Helm persists release metadata in Secrets (default) or ConfigMaps, stored in the Kubernetes cluster. Every time your release changes, it appends that to the existing data. This provides Helm with the capability to rollback to a previous release. Let's see how this works in practice. Check the number of deployments: helm history guestbook-demo -n helm-demo You should see output similar to the following because we did an upgrade in Lab 2 after the initial install in Lab 1 : $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 deployed guestbook-0.2.0 Upgrade complete Roll back to the previous revision: In this rollback, Helm checks the changes that occured when upgrading from the revision 1 to revision 2. This information enables it to makes the calls to the Kubernetes API server, to update the deployed application as per the initial deployment - in other words with Redis slaves and using a load balancer. Rollback with this command: helm rollback guestbook-demo 1 -n helm-demo $ helm rollback guestbook-demo 1 -n helm-demo Rollback was a success! Happy Helming! Check the history again: helm history guestbook-demo -n helm-demo You should see output similar to the following: $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 superseded guestbook-0.2.0 Upgrade complete 3 Tue Feb 25 14:53:45 2020 deployed guestbook-0.2.0 Rollback to 1 Check the rollback, using: kubectl get all --namespace helm-demo $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h pod/redis-slave-586b4c847c-tkfj5 1/1 Running 0 5m15s pod/redis-slave-586b4c847c-xxrdn 1/1 Running 0 5m15s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h service/redis-slave ClusterIP 172.21.232.16 <none> 6379/TCP 5m15s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h deployment.apps/redis-slave 2/2 2 2 5m15s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-26c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h replicaset.apps/redis-slave-586b4c847c 2 2 2 5m15s You can see from the output that the app service is the service type of LoadBalancer again and the Redis master/slave deployment has returned. This shows a complete rollback from the upgrade in Lab 2","title":"Scenario 2: Revision management using Helm"},{"location":"generatedContent/helm101/Lab3/#conclusion","text":"From this lab, we can say that Helm does revision management well and Kubernetes does not have the capability built in! You might be wondering why we need helm rollback when you could just re-run the helm upgrade from a previous version. And that's a good question. Technically, you should end up with the same resources (with same parameters) deployed. However, the advantage of using helm rollback is that helm manages (ie. remembers) all of the variations/parameters of the previous helm install\\upgrade for you. Doing the rollback via a helm upgrade requires you (and your entire team) to manually track how the command was previously executed. That's not only tedious but very error prone. It is much easier, safer and reliable to let Helm manage all of that for you and all you need to do it tell it which previous version to go back to, and it does the rest. Lab 4 awaits.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab4/","text":"Lab 4. Share Helm Charts \u00b6 A key aspect of providing an application means sharing with others. Sharing can be direct counsumption (by users or in CI/CD pipelines) or as a dependency for other charts. If people can't find your app then they can't use it. A means of sharing is a chart repository, which is a location where packaged charts can be stored and shared. As the chart repository only applies to Helm, we will just look at the usage and storage of Helm charts. Using charts from a public repository \u00b6 Helm charts can be available on a remote repository or in a local environment/repository. The remote repositories can be public like Bitnami Charts or IBM Helm Charts , or hosted repositories like on Google Cloud Storage or GitHub. Refer to Helm Chart Repository Guide for more details. You can learn more about the structure of a chart repository by examining the chart index file in this lab. In this part of the lab, we show you how to install the guestbook chart from the Helm101 repo . Check the repositories configured on your system: helm repo list The output should be similar to the following: $ helm repo list Error: no repositories to show Note: Chart repositories are not installed by default with Helm v3. It is expected that you add the repositories for the charts you want to use. The Helm Hub provides a centralized search for publicly available distributed charts. Using the hub you can identify the chart with its hosted repository and then add it to your local respoistory list. The Helm chart repository like Helm v2 is in \"maintenance mode\" and will be deprecated by November 13, 2020. See the project status for more details. Add helm101 repo: helm repo add helm101 https://ibm.github.io/helm101/ Should generate an output as follows: $ helm repo add helm101 https://ibm.github.io/helm101/ \"helm101\" has been added to your repositories You can also search your repositories for charts by running the following command: helm search repo helm101 $ helm search repo helm101 NAME CHART VERSION APP VERSION DESCRIPTION helm101/guestbook 0.2.1 A Helm chart to deploy Guestbook three tier web... Install the chart As mentioned we are going to install the guestbook chart from the Helm101 repo . As the repo is added to our local respoitory list we can reference the chart using the repo name/chart name , in other words helm101/guestbook . To see this in action, you will install the application to a new namespace called repo-demo . If the repo-demo namespace does not exist, create it using: kubectl create namespace repo-demo Now install the chart using this command: helm install guestbook-demo helm101/guestbook --namespace repo-demo The output should be similar to the following: $ helm install guestbook-demo helm101/guestbook --namespace repo-demo NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 15:40:17 2020 NAMESPACE: repo-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace repo-demo' export SERVICE_IP=$(kubectl get svc --namespace repo-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 Check that release deployed as expected as follows: $ helm list -n repo-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo repo-demo 1 2020-02-25 15:40:17.627745329 +0000 UTC deployed guestbook-0.2.1 Conclusion \u00b6 This lab provided you with a brief introduction to the Helm repositories to show how charts can be installed. The ability to share your chart means ease of use to both you and your consumers.","title":"Lab 4. Share Helm Charts"},{"location":"generatedContent/helm101/Lab4/#lab-4-share-helm-charts","text":"A key aspect of providing an application means sharing with others. Sharing can be direct counsumption (by users or in CI/CD pipelines) or as a dependency for other charts. If people can't find your app then they can't use it. A means of sharing is a chart repository, which is a location where packaged charts can be stored and shared. As the chart repository only applies to Helm, we will just look at the usage and storage of Helm charts.","title":"Lab 4. Share Helm Charts"},{"location":"generatedContent/helm101/Lab4/#using-charts-from-a-public-repository","text":"Helm charts can be available on a remote repository or in a local environment/repository. The remote repositories can be public like Bitnami Charts or IBM Helm Charts , or hosted repositories like on Google Cloud Storage or GitHub. Refer to Helm Chart Repository Guide for more details. You can learn more about the structure of a chart repository by examining the chart index file in this lab. In this part of the lab, we show you how to install the guestbook chart from the Helm101 repo . Check the repositories configured on your system: helm repo list The output should be similar to the following: $ helm repo list Error: no repositories to show Note: Chart repositories are not installed by default with Helm v3. It is expected that you add the repositories for the charts you want to use. The Helm Hub provides a centralized search for publicly available distributed charts. Using the hub you can identify the chart with its hosted repository and then add it to your local respoistory list. The Helm chart repository like Helm v2 is in \"maintenance mode\" and will be deprecated by November 13, 2020. See the project status for more details. Add helm101 repo: helm repo add helm101 https://ibm.github.io/helm101/ Should generate an output as follows: $ helm repo add helm101 https://ibm.github.io/helm101/ \"helm101\" has been added to your repositories You can also search your repositories for charts by running the following command: helm search repo helm101 $ helm search repo helm101 NAME CHART VERSION APP VERSION DESCRIPTION helm101/guestbook 0.2.1 A Helm chart to deploy Guestbook three tier web... Install the chart As mentioned we are going to install the guestbook chart from the Helm101 repo . As the repo is added to our local respoitory list we can reference the chart using the repo name/chart name , in other words helm101/guestbook . To see this in action, you will install the application to a new namespace called repo-demo . If the repo-demo namespace does not exist, create it using: kubectl create namespace repo-demo Now install the chart using this command: helm install guestbook-demo helm101/guestbook --namespace repo-demo The output should be similar to the following: $ helm install guestbook-demo helm101/guestbook --namespace repo-demo NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 15:40:17 2020 NAMESPACE: repo-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace repo-demo' export SERVICE_IP=$(kubectl get svc --namespace repo-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 Check that release deployed as expected as follows: $ helm list -n repo-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo repo-demo 1 2020-02-25 15:40:17.627745329 +0000 UTC deployed guestbook-0.2.1","title":"Using charts from a public repository"},{"location":"generatedContent/helm101/Lab4/#conclusion","text":"This lab provided you with a brief introduction to the Helm repositories to show how charts can be installed. The ability to share your chart means ease of use to both you and your consumers.","title":"Conclusion"},{"location":"generatedContent/istio101/","text":"Beyond the Basics: Istio and IBM Cloud Kubernetes Service \u00b6 Istio is an open platform to connect, secure, control and observe microservices, also known as a service mesh, on cloud platforms such as Kubernetes in IBM Cloud Kubernetes Service and VMs. With Istio, You can manage network traffic, load balance across microservices, enforce access policies, verify service identity, secure service communication and observe what exactly is going on with your services. YouTube: Istio Service Mesh Explained: In this course, you can see how to install Istio alongside microservices for a simple mock app called Guestbook . When you deploy Guestbook's microservices into an IBM Cloud Kubernetes Service cluster where Istio is installed, you can choose to inject the Istio Envoy sidecar proxies in the pods of certain microservices. Estimated completion time: 2 hours Objectives \u00b6 After you complete this course, you'll be able to: Download and install Istio in your cluster Deploy the Guestbook sample app Use metrics, logging and tracing to observe services Set up the Istio Ingress Gateway Perform simple traffic management, such as A/B tests and canary deployments Secure your service mesh Enforce policies for your microservices Prerequisites \u00b6 You must you must have a Pay-As-You-Go, or Subscription IBM Cloud account to complete all the modules in this course. You must have already created a Standard 1.16+ cluster in IBM Cloud Kubernetes Service. FREE Cluster is not supported for this lab You should have a basic understanding of containers, IBM Cloud Kubernetes Service, and Istio. If you have no experience with those, take the following courses: Get started with Kubernetes and IBM Cloud Kubernetes Service Get started with Istio and IBM Cloud Kubernetes Service Workshop setup \u00b6 Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application Creating a service mesh with Istio \u00b6 Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh Cleaning up the Workshop \u00b6 Script to uninstall ibmcloud CLI: clean_your_local_machine.sh and unset KUBECONFIG . Script to delete Istio and Guestbook: clean_your_k8s_cluster.sh .","title":"Beyond the Basics: Istio and IBM Cloud Kubernetes Service"},{"location":"generatedContent/istio101/#beyond-the-basics-istio-and-ibm-cloud-kubernetes-service","text":"Istio is an open platform to connect, secure, control and observe microservices, also known as a service mesh, on cloud platforms such as Kubernetes in IBM Cloud Kubernetes Service and VMs. With Istio, You can manage network traffic, load balance across microservices, enforce access policies, verify service identity, secure service communication and observe what exactly is going on with your services. YouTube: Istio Service Mesh Explained: In this course, you can see how to install Istio alongside microservices for a simple mock app called Guestbook . When you deploy Guestbook's microservices into an IBM Cloud Kubernetes Service cluster where Istio is installed, you can choose to inject the Istio Envoy sidecar proxies in the pods of certain microservices. Estimated completion time: 2 hours","title":"Beyond the Basics: Istio and IBM Cloud Kubernetes Service"},{"location":"generatedContent/istio101/#objectives","text":"After you complete this course, you'll be able to: Download and install Istio in your cluster Deploy the Guestbook sample app Use metrics, logging and tracing to observe services Set up the Istio Ingress Gateway Perform simple traffic management, such as A/B tests and canary deployments Secure your service mesh Enforce policies for your microservices","title":"Objectives"},{"location":"generatedContent/istio101/#prerequisites","text":"You must you must have a Pay-As-You-Go, or Subscription IBM Cloud account to complete all the modules in this course. You must have already created a Standard 1.16+ cluster in IBM Cloud Kubernetes Service. FREE Cluster is not supported for this lab You should have a basic understanding of containers, IBM Cloud Kubernetes Service, and Istio. If you have no experience with those, take the following courses: Get started with Kubernetes and IBM Cloud Kubernetes Service Get started with Istio and IBM Cloud Kubernetes Service","title":"Prerequisites"},{"location":"generatedContent/istio101/#workshop-setup","text":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application","title":"Workshop setup"},{"location":"generatedContent/istio101/#creating-a-service-mesh-with-istio","text":"Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh","title":"Creating a service mesh with Istio"},{"location":"generatedContent/istio101/#cleaning-up-the-workshop","text":"Script to uninstall ibmcloud CLI: clean_your_local_machine.sh and unset KUBECONFIG . Script to delete Istio and Guestbook: clean_your_k8s_cluster.sh .","title":"Cleaning up the Workshop"},{"location":"generatedContent/istio101/SUMMARY/","text":"Table of contents \u00b6 About this workshop \u00b6 Overview Workshop setup \u00b6 Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application Creating a service mesh with Istio \u00b6 Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh","title":"Table of contents"},{"location":"generatedContent/istio101/SUMMARY/#table-of-contents","text":"","title":"Table of contents"},{"location":"generatedContent/istio101/SUMMARY/#about-this-workshop","text":"Overview","title":"About this workshop"},{"location":"generatedContent/istio101/SUMMARY/#workshop-setup","text":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application","title":"Workshop setup"},{"location":"generatedContent/istio101/SUMMARY/#creating-a-service-mesh-with-istio","text":"Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh","title":"Creating a service mesh with Istio"},{"location":"generatedContent/istio101/exercise-1/","text":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service \u00b6 You must already have a cluster created . Your cluster must have 3 or more worker nodes with at least 4 cores and 16GB RAM , and run Kubernetes version 1.16 or later. Install IBM Cloud Kubernetes Service command line utilities \u00b6 Download and install the required CLI tools. curl -sL https://ibm.biz/idt-installer | bash Log in to the IBM Cloud CLI. (If you have a federated account, include the --sso flag.) ibmcloud login Access your cluster \u00b6 Learn how to set the context to work with your cluster by using the kubectl CLI, access the Kubernetes dashboard, and gather basic information about your cluster. Set the context for your cluster in your CLI. Every time you log in to the IBM Cloud Kubernetes Service CLI to work with the cluster, you must run these commands to set the path to the cluster's configuration file as a session variable. The Kubernetes CLI uses this variable to find a local configuration file and certificates that are necessary to connect with the cluster in IBM Cloud. a. List the available clusters. ibmcloud ks clusters b. Set an environment variable for your cluster name: export MYCLUSTER = <your_cluster_name> c. Download the configuration file and certificates for your cluster using the cluster-config command. ibmcloud ks cluster config --cluster $MYCLUSTER Get basic information about your cluster and its worker nodes. This information can help you manage your cluster and troubleshoot issues. a. View details of your cluster. ibmcloud ks cluster get --cluster $MYCLUSTER b. Verify the worker nodes in the cluster. ibmcloud ks workers --cluster $MYCLUSTER Validate access to your cluster by viewing the nodes in the cluster. kubectl get nodes Clone the lab repo \u00b6 From your command line, run: git clone https://github.com/IBM/istio101 cd istio101/workshop This is the working directory for the workshop. You will use the example .yaml files that are located in the workshop/plans directory in the following exercises. Continue to Exercise 2 - Installing Istio \u00b6","title":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service"},{"location":"generatedContent/istio101/exercise-1/#exercise-1-accessing-a-kubernetes-cluster-with-ibm-cloud-kubernetes-service","text":"You must already have a cluster created . Your cluster must have 3 or more worker nodes with at least 4 cores and 16GB RAM , and run Kubernetes version 1.16 or later.","title":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service"},{"location":"generatedContent/istio101/exercise-1/#install-ibm-cloud-kubernetes-service-command-line-utilities","text":"Download and install the required CLI tools. curl -sL https://ibm.biz/idt-installer | bash Log in to the IBM Cloud CLI. (If you have a federated account, include the --sso flag.) ibmcloud login","title":"Install IBM Cloud Kubernetes Service command line utilities"},{"location":"generatedContent/istio101/exercise-1/#access-your-cluster","text":"Learn how to set the context to work with your cluster by using the kubectl CLI, access the Kubernetes dashboard, and gather basic information about your cluster. Set the context for your cluster in your CLI. Every time you log in to the IBM Cloud Kubernetes Service CLI to work with the cluster, you must run these commands to set the path to the cluster's configuration file as a session variable. The Kubernetes CLI uses this variable to find a local configuration file and certificates that are necessary to connect with the cluster in IBM Cloud. a. List the available clusters. ibmcloud ks clusters b. Set an environment variable for your cluster name: export MYCLUSTER = <your_cluster_name> c. Download the configuration file and certificates for your cluster using the cluster-config command. ibmcloud ks cluster config --cluster $MYCLUSTER Get basic information about your cluster and its worker nodes. This information can help you manage your cluster and troubleshoot issues. a. View details of your cluster. ibmcloud ks cluster get --cluster $MYCLUSTER b. Verify the worker nodes in the cluster. ibmcloud ks workers --cluster $MYCLUSTER Validate access to your cluster by viewing the nodes in the cluster. kubectl get nodes","title":"Access your cluster"},{"location":"generatedContent/istio101/exercise-1/#clone-the-lab-repo","text":"From your command line, run: git clone https://github.com/IBM/istio101 cd istio101/workshop This is the working directory for the workshop. You will use the example .yaml files that are located in the workshop/plans directory in the following exercises.","title":"Clone the lab repo"},{"location":"generatedContent/istio101/exercise-1/#continue-to-exercise-2-installing-istio","text":"","title":"Continue to Exercise 2 - Installing Istio"},{"location":"generatedContent/istio101/exercise-2/","text":"Exercise 2 - Installing Istio on IBM Cloud Kubernetes Service \u00b6 In this module, you will use the Managed Istio add-on to install Istio on your cluster. Managed Istio is available as part of IBM Cloud\u2122 Kubernetes Service. The service provides seamless installation of Istio, automatic updates and lifecycle management of control plane components, and integration with platform logging and monitoring tools. Download the istioctl CLI and add it to your PATH: curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .5.6 sh - export PATH = $PWD /istio-1.5.6/bin: $PATH Enable Managed Istio on your IKS cluster: ibmcloud ks cluster addon enable istio --cluster $MYCLUSTER The install can take up to 10 minutes. Ensure the corresponding pods are all in Running state before you continue. kubectl get pods -n istio-system Sample output: ```shell NAME READY STATUS RESTARTS AGE istio-egressgateway-6c966469cc-52t6f 1/1 Running 0 69s istio-egressgateway-6c966469cc-qq5qd 1/1 Running 0 55s istio-ingressgateway-7698c7b4f4-69c24 1/1 Running 0 68s istio-ingressgateway-7698c7b4f4-qttzh 1/1 Running 0 54s istiod-cbb98c74d-2wvql 1/1 Running 0 54s istiod-cbb98c74d-kcr4d 1/1 Running 0 67s ``` NOTE Before you continue, make sure all the pods are deployed and either in the Running or Completed state. If they're in pending state, wait a few minutes to let the installation and deployment finish. Check the version of your Istio: istioctl version Sample output: client version: 1 .5.6 control plane version: 1 .5.6 data plane version: 1 .5.6 ( 4 proxies ) Congratulations! You successfully installed Istio into your cluster. Continue to Exercise 3 - Deploy Guestbook with Istio Proxy \u00b6","title":"Exercise 2 - Installing Istio on IBM Cloud Kubernetes Service"},{"location":"generatedContent/istio101/exercise-2/#exercise-2-installing-istio-on-ibm-cloud-kubernetes-service","text":"In this module, you will use the Managed Istio add-on to install Istio on your cluster. Managed Istio is available as part of IBM Cloud\u2122 Kubernetes Service. The service provides seamless installation of Istio, automatic updates and lifecycle management of control plane components, and integration with platform logging and monitoring tools. Download the istioctl CLI and add it to your PATH: curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .5.6 sh - export PATH = $PWD /istio-1.5.6/bin: $PATH Enable Managed Istio on your IKS cluster: ibmcloud ks cluster addon enable istio --cluster $MYCLUSTER The install can take up to 10 minutes. Ensure the corresponding pods are all in Running state before you continue. kubectl get pods -n istio-system Sample output: ```shell NAME READY STATUS RESTARTS AGE istio-egressgateway-6c966469cc-52t6f 1/1 Running 0 69s istio-egressgateway-6c966469cc-qq5qd 1/1 Running 0 55s istio-ingressgateway-7698c7b4f4-69c24 1/1 Running 0 68s istio-ingressgateway-7698c7b4f4-qttzh 1/1 Running 0 54s istiod-cbb98c74d-2wvql 1/1 Running 0 54s istiod-cbb98c74d-kcr4d 1/1 Running 0 67s ``` NOTE Before you continue, make sure all the pods are deployed and either in the Running or Completed state. If they're in pending state, wait a few minutes to let the installation and deployment finish. Check the version of your Istio: istioctl version Sample output: client version: 1 .5.6 control plane version: 1 .5.6 data plane version: 1 .5.6 ( 4 proxies ) Congratulations! You successfully installed Istio into your cluster.","title":"Exercise 2 - Installing Istio on IBM Cloud Kubernetes Service"},{"location":"generatedContent/istio101/exercise-2/#continue-to-exercise-3-deploy-guestbook-with-istio-proxy","text":"","title":"Continue to Exercise 3 - Deploy Guestbook with Istio Proxy"},{"location":"generatedContent/istio101/exercise-3/","text":"Exercise 3 - Deploy the Guestbook app with Istio Proxy \u00b6 The Guestbook app is a sample app for users to leave comments. It consists of a web front end, Redis master for storage, and a replicated set of Redis slaves. We will also integrate the app with Watson Tone Analyzer which detects the sentiment in users' comments and replies with emoticons. Download the Guestbook app \u00b6 Clone the Guestbook app into the workshop directory. git clone -b kubecon2019 https://github.com/IBM/guestbook Navigate into the app directory. cd guestbook/v2 Enable the automatic sidecar injection for the default namespace \u00b6 In Kubernetes, a sidecar is a utility container in the pod, and its purpose is to support the main container. For Istio to work, Envoy proxies must be deployed as sidecars to each pod of the deployment. There are two ways of injecting the Istio sidecar into a pod: manually using the istioctl CLI tool or automatically using the Istio sidecar injector. In this exercise, we will use the automatic sidecar injection provided by Istio. Annotate the default namespace to enable automatic sidecar injection: kubectl label namespace default istio-injection = enabled Validate the namespace is annotated for automatic sidecar injection: kubectl get namespace -L istio-injection Sample output: NAME STATUS AGE ISTIO-INJECTION default Active 271d enabled istio-system Active 5d2h ... Create a Redis database \u00b6 The Redis database is a service that you can use to persist the data of your app. The Redis database comes with a master and slave modules. Create the Redis controllers and services for both the master and the slave. kubectl create -f redis-master-deployment.yaml kubectl create -f redis-master-service.yaml kubectl create -f redis-slave-deployment.yaml kubectl create -f redis-slave-service.yaml Verify that the Redis controllers for the master and the slave are created. kubectl get deployment Output: NAME READY UP-TO-DATE AVAILABLE AGE redis-master 1 /1 1 1 2m16s redis-slave 2 /2 2 2 2m15s Verify that the Redis services for the master and the slave are created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE redis-master ClusterIP 172 .21.85.39 <none> 6379 /TCP 5d redis-slave ClusterIP 172 .21.205.35 <none> 6379 /TCP 5d Verify that the Redis pods for the master and the slave are up and running. kubectl get pods Output: NAME READY STATUS RESTARTS AGE redis-master-4sswq 2 /2 Running 0 5d redis-slave-kj8jp 2 /2 Running 0 5d redis-slave-nslps 2 /2 Running 0 5d Install the Guestbook app \u00b6 Inject the Istio Envoy sidecar into the guestbook pods, and deploy the Guestbook app on to the Kubernetes cluster. Deploy both the v1 and v2 versions of the app: kubectl apply -f ../v1/guestbook-deployment.yaml kubectl apply -f guestbook-deployment.yaml These commands deploy the Guestbook app on to the Kubernetes cluster. Since we enabled automation sidecar injection, these pods will be also include an Envoy sidecar as they are started in the cluster. Here we have two versions of deployments, a new version ( v2 ) in the current directory, and a previous version ( v1 ) in a sibling directory. They will be used in future sections to showcase the Istio traffic routing capabilities. Create the guestbook service. kubectl create -f guestbook-service.yaml Verify that the service was created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook LoadBalancer 172 .21.36.181 169 .61.37.140 80 :32149/TCP 5d ... Verify that the pods are up and running. kubectl get pods Sample output: NAME READY STATUS RESTARTS AGE guestbook-v1-98dd9c654-dz8dq 2 /2 Running 0 30s guestbook-v1-98dd9c654-mgfv6 2 /2 Running 0 30s guestbook-v1-98dd9c654-x8gxx 2 /2 Running 0 30s guestbook-v2-8689f6c559-5ntgv 2 /2 Running 0 28s guestbook-v2-8689f6c559-fpzb7 2 /2 Running 0 28s guestbook-v2-8689f6c559-wqbnl 2 /2 Running 0 28s redis-master-577bc6fbb-zh5v8 2 /2 Running 0 4m47s redis-slave-7779c6f75b-bshvs 2 /2 Running 0 4m46s redis-slave-7779c6f75b-nvsd6 2 /2 Running 0 4m46s Note that each guestbook pod has 2 containers in it. One is the guestbook container, and the other is the Envoy proxy sidecar. Use Watson Tone Analyzer \u00b6 Watson Tone Analyzer detects the tone from the words that users enter into the Guestbook app. The tone is converted to the corresponding emoticons. Create Watson Tone Analyzer in your account. ibmcloud resource service-instance-create my-tone-analyzer-service tone-analyzer lite us-south Create the service key for the Tone Analyzer service. This command should output the credentials you just created. You will need the value for apikey & url later. ibmcloud resource service-key-create tone-analyzer-key Manager --instance-name my-tone-analyzer-service If you need to get the service-keys later, you can use the following command: ibmcloud resource service-key tone-analyzer-key Open the analyzer-deployment.yaml and find the env section near the end of the file. Replace YOUR_API_KEY with your own API key, and replace YOUR_URL with the url value you saved before. YOUR_URL should look something like https://gateway.watsonplatform.net/tone-analyzer/api . Save the file. Deploy the analyzer pods and service, using the analyzer-deployment.yaml and analyzer-service.yaml files found in the guestbook/v2 directory. The analyzer service talks to Watson Tone Analyzer to help analyze the tone of a message. Ensure you are still in the guestbook/v2 directory. kubectl apply -f analyzer-deployment.yaml kubectl apply -f analyzer-service.yaml Great! Your guestbook app is up and running. In Exercise 4, you'll be able to see the app in action by directly accessing the service endpoint. You'll also be able to view Telemetry data for the app. Continue to Exercise 4 - Telemetry \u00b6","title":"Exercise 3 - Deploy the Guestbook app with Istio Proxy"},{"location":"generatedContent/istio101/exercise-3/#exercise-3-deploy-the-guestbook-app-with-istio-proxy","text":"The Guestbook app is a sample app for users to leave comments. It consists of a web front end, Redis master for storage, and a replicated set of Redis slaves. We will also integrate the app with Watson Tone Analyzer which detects the sentiment in users' comments and replies with emoticons.","title":"Exercise 3 - Deploy the Guestbook app with Istio Proxy"},{"location":"generatedContent/istio101/exercise-3/#download-the-guestbook-app","text":"Clone the Guestbook app into the workshop directory. git clone -b kubecon2019 https://github.com/IBM/guestbook Navigate into the app directory. cd guestbook/v2","title":"Download the Guestbook app"},{"location":"generatedContent/istio101/exercise-3/#enable-the-automatic-sidecar-injection-for-the-default-namespace","text":"In Kubernetes, a sidecar is a utility container in the pod, and its purpose is to support the main container. For Istio to work, Envoy proxies must be deployed as sidecars to each pod of the deployment. There are two ways of injecting the Istio sidecar into a pod: manually using the istioctl CLI tool or automatically using the Istio sidecar injector. In this exercise, we will use the automatic sidecar injection provided by Istio. Annotate the default namespace to enable automatic sidecar injection: kubectl label namespace default istio-injection = enabled Validate the namespace is annotated for automatic sidecar injection: kubectl get namespace -L istio-injection Sample output: NAME STATUS AGE ISTIO-INJECTION default Active 271d enabled istio-system Active 5d2h ...","title":"Enable the automatic sidecar injection for the default namespace"},{"location":"generatedContent/istio101/exercise-3/#create-a-redis-database","text":"The Redis database is a service that you can use to persist the data of your app. The Redis database comes with a master and slave modules. Create the Redis controllers and services for both the master and the slave. kubectl create -f redis-master-deployment.yaml kubectl create -f redis-master-service.yaml kubectl create -f redis-slave-deployment.yaml kubectl create -f redis-slave-service.yaml Verify that the Redis controllers for the master and the slave are created. kubectl get deployment Output: NAME READY UP-TO-DATE AVAILABLE AGE redis-master 1 /1 1 1 2m16s redis-slave 2 /2 2 2 2m15s Verify that the Redis services for the master and the slave are created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE redis-master ClusterIP 172 .21.85.39 <none> 6379 /TCP 5d redis-slave ClusterIP 172 .21.205.35 <none> 6379 /TCP 5d Verify that the Redis pods for the master and the slave are up and running. kubectl get pods Output: NAME READY STATUS RESTARTS AGE redis-master-4sswq 2 /2 Running 0 5d redis-slave-kj8jp 2 /2 Running 0 5d redis-slave-nslps 2 /2 Running 0 5d","title":"Create a Redis database"},{"location":"generatedContent/istio101/exercise-3/#install-the-guestbook-app","text":"Inject the Istio Envoy sidecar into the guestbook pods, and deploy the Guestbook app on to the Kubernetes cluster. Deploy both the v1 and v2 versions of the app: kubectl apply -f ../v1/guestbook-deployment.yaml kubectl apply -f guestbook-deployment.yaml These commands deploy the Guestbook app on to the Kubernetes cluster. Since we enabled automation sidecar injection, these pods will be also include an Envoy sidecar as they are started in the cluster. Here we have two versions of deployments, a new version ( v2 ) in the current directory, and a previous version ( v1 ) in a sibling directory. They will be used in future sections to showcase the Istio traffic routing capabilities. Create the guestbook service. kubectl create -f guestbook-service.yaml Verify that the service was created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook LoadBalancer 172 .21.36.181 169 .61.37.140 80 :32149/TCP 5d ... Verify that the pods are up and running. kubectl get pods Sample output: NAME READY STATUS RESTARTS AGE guestbook-v1-98dd9c654-dz8dq 2 /2 Running 0 30s guestbook-v1-98dd9c654-mgfv6 2 /2 Running 0 30s guestbook-v1-98dd9c654-x8gxx 2 /2 Running 0 30s guestbook-v2-8689f6c559-5ntgv 2 /2 Running 0 28s guestbook-v2-8689f6c559-fpzb7 2 /2 Running 0 28s guestbook-v2-8689f6c559-wqbnl 2 /2 Running 0 28s redis-master-577bc6fbb-zh5v8 2 /2 Running 0 4m47s redis-slave-7779c6f75b-bshvs 2 /2 Running 0 4m46s redis-slave-7779c6f75b-nvsd6 2 /2 Running 0 4m46s Note that each guestbook pod has 2 containers in it. One is the guestbook container, and the other is the Envoy proxy sidecar.","title":"Install the Guestbook app"},{"location":"generatedContent/istio101/exercise-3/#use-watson-tone-analyzer","text":"Watson Tone Analyzer detects the tone from the words that users enter into the Guestbook app. The tone is converted to the corresponding emoticons. Create Watson Tone Analyzer in your account. ibmcloud resource service-instance-create my-tone-analyzer-service tone-analyzer lite us-south Create the service key for the Tone Analyzer service. This command should output the credentials you just created. You will need the value for apikey & url later. ibmcloud resource service-key-create tone-analyzer-key Manager --instance-name my-tone-analyzer-service If you need to get the service-keys later, you can use the following command: ibmcloud resource service-key tone-analyzer-key Open the analyzer-deployment.yaml and find the env section near the end of the file. Replace YOUR_API_KEY with your own API key, and replace YOUR_URL with the url value you saved before. YOUR_URL should look something like https://gateway.watsonplatform.net/tone-analyzer/api . Save the file. Deploy the analyzer pods and service, using the analyzer-deployment.yaml and analyzer-service.yaml files found in the guestbook/v2 directory. The analyzer service talks to Watson Tone Analyzer to help analyze the tone of a message. Ensure you are still in the guestbook/v2 directory. kubectl apply -f analyzer-deployment.yaml kubectl apply -f analyzer-service.yaml Great! Your guestbook app is up and running. In Exercise 4, you'll be able to see the app in action by directly accessing the service endpoint. You'll also be able to view Telemetry data for the app.","title":"Use Watson Tone Analyzer"},{"location":"generatedContent/istio101/exercise-3/#continue-to-exercise-4-telemetry","text":"","title":"Continue to Exercise 4 - Telemetry"},{"location":"generatedContent/istio101/exercise-4/","text":"Exercise 4 - Observe service telemetry: metrics and tracing \u00b6 Challenges with microservices \u00b6 We all know that microservice architecture is the perfect fit for cloud native applications and it increases the delivery velocities greatly. Envision you have many microservices that are delivered by multiple teams, how do you observe the the overall platform and each of the service to find out exactly what is going on with each of the services? When something goes wrong, how do you know which service or which communication among the few services are causing the problem? Istio telemetry \u00b6 Istio's tracing and metrics features are designed to provide broad and granular insight into the health of all services. Istio's role as a service mesh makes it the ideal data source for observability information, particularly in a microservices environment. As requests pass through multiple services, identifying performance bottlenecks becomes increasingly difficult using traditional debugging techniques. Distributed tracing provides a holistic view of requests transiting through multiple services, allowing for immediate identification of latency issues. With Istio, distributed tracing comes by default. This will expose latency, retry, and failure information for each hop in a request. You can read more about how Istio mixer enables telemetry reporting . Configure Istio to receive telemetry data \u00b6 Enable Istio monitoring dashboards, by running these two commands: kubectl patch cm managed-istio-custom -n ibm-operators --type = 'json' -p = '[{\"op\": \"add\", \"path\": \"/data/istio-monitoring\", \"value\":\"true\"}]' kubectl annotate iop -n ibm-operators managed-istio --overwrite version = \"custom-applied-at: $( date ) \" Verify that the Grafana, Prometheus, Kiali and Jaeger add-ons were installed successfully. All add-ons are installed into the istio-system namespace. kubectl get services -n istio-system Obtain the guestbook endpoint to access the guestbook. You can access the guestbook via the external IP for your service as guestbook is deployed as a load balancer service. Get the EXTERNAL-IP of the guestbook service via output below: kubectl get service guestbook -n default Go to this external ip address in the browser to try out your guestbook. This service will route you to either v1 or v2, at random. If you wish to see a different version, you'll need to do a hard refresh ( cmd + shift + r on a mac, or ctrl + f5 on a PC). Alternatively, you can curl the address. Generate a small load to the app, replacing guestbook_IP with the EXTERNAL-IP. for i in { 1 ..40 } ; do sleep 0 .2 ; curl -I http://<guestbook_IP>/ ; done View guestbook telemetry data \u00b6 Jaeger \u00b6 Launch the Jaeger dashboard: istioctl dashboard jaeger From the Services menu, select either the guestbook or analyzer service. Scroll to the bottom and click on Find Traces button to see traces. Use Ctrl-C in the terminal to exit the port-foward when you are done. Read more about Jaeger Grafana \u00b6 Create a secret which will be used to set the login credentials for Grafana cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: grafana namespace: istio-system type: Opaque data: username: \"YWRtaW4=\" passphrase: \"YWRtaW4=\" EOF Wait 2 minutes for the secret to be picked up and then launch the dashboard: istioctl dashboard grafana Log in using admin for both username and password. Navigate to the Istio Service Dashboard by clicking on the Home menu on the top left, then Istio, then Istio Service Dashboard. Select guestbook in the Service drop down. In a different tab, visit the guestbook application and refresh the page multiple times to generate some load, or run the load script you used previously. Switch back to the Grafana tab. Use Ctrl-C in the terminal to exit the port-foward when you are done. This Grafana dashboard provides metrics for each workload. Explore the other dashboard provided as well. Read more about Grafana . Prometheus \u00b6 Establish port forwarding from local port 9090 to the Prometheus instance. istioctl dashboard prometheus In the \u201cExpression\u201d input box, enter: istio_request_bytes_count . Click Execute and then select Graph. Then try another query: istio_requests_total{destination_service=\"guestbook.default.svc.cluster.local\", destination_version=\"2.0\"} Use Ctrl-C in the terminal to exit the port-foward when you are done. Kiali \u00b6 Kiali is an open-source project that installs on top of Istio to visualize your service mesh. It provides deeper insight into how your microservices interact with one another, and provides features such as circuit breakers and request rates for your services Create a secret which will be used to set the login credentials for Kiali cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: kiali namespace: istio-system labels: app: kiali type: Opaque data: username: \"YWRtaW4=\" passphrase: \"YWRtaW4=\" EOF Establish port forwarding from local port 20001 to the Kiali instance. istioctl dashboard kiali Login with admin for both username and password. Select Graph and then choose default namespace. You should see a visual service graph of the various services in your Istio mesh. Use the Edge Labels dropdown and select Traffic rate per second to see the request rates as well. Kiali has a number of views to help you visualize your services. Click through the vairous tabs to explore the service graph, and the various views for workloads, applications, and services. Understand what happened \u00b6 Although Istio proxies are able to automatically send spans, they need some hints to tie together the entire trace. Apps need to propagate the appropriate HTTP headers so that when the proxies send span information to Zipkin or Jaeger, the spans can be correlated correctly into a single trace. In the example, when a user visits the Guestbook app, the HTTP request is sent from the guestbook service to Watson Tone Analyzer. In order for the individual spans of guestbook service and Watson Tone Analyzer to be tied together, we have modified the guestbook service to extract the required headers (x-request-id, x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, x-b3-flags, x-ot-span-context) and forward them onto the analyzer service when calling the analyzer service from the guestbook service. The change is in the v2/guestbook/main.go . By using the getForwardHeaders() method, we are able to extract the required headers, and then we use the required headers further when calling the analyzer service via the getPrimaryTone() method. Questions \u00b6 Does a user need to modify their app to get metrics for their apps? A: 1. Yes 2. No. (2 is correct) Does a user need to modify their app to get distributed tracing for their app to work properly? A: 1. Yes 2. No. (1 is correct) What distributed tracing system does Istio support by default? A: 1. Zipkin 2. Kibana 3. LogStash 4. Jaeger. (1 and 4 are correct) Continue to Exercise 5 - Expose the service mesh with the Istio Ingress Gateway \u00b6","title":"Exercise 4 - Observe service telemetry: metrics and tracing"},{"location":"generatedContent/istio101/exercise-4/#exercise-4-observe-service-telemetry-metrics-and-tracing","text":"","title":"Exercise 4 - Observe service telemetry: metrics and tracing"},{"location":"generatedContent/istio101/exercise-4/#challenges-with-microservices","text":"We all know that microservice architecture is the perfect fit for cloud native applications and it increases the delivery velocities greatly. Envision you have many microservices that are delivered by multiple teams, how do you observe the the overall platform and each of the service to find out exactly what is going on with each of the services? When something goes wrong, how do you know which service or which communication among the few services are causing the problem?","title":"Challenges with microservices"},{"location":"generatedContent/istio101/exercise-4/#istio-telemetry","text":"Istio's tracing and metrics features are designed to provide broad and granular insight into the health of all services. Istio's role as a service mesh makes it the ideal data source for observability information, particularly in a microservices environment. As requests pass through multiple services, identifying performance bottlenecks becomes increasingly difficult using traditional debugging techniques. Distributed tracing provides a holistic view of requests transiting through multiple services, allowing for immediate identification of latency issues. With Istio, distributed tracing comes by default. This will expose latency, retry, and failure information for each hop in a request. You can read more about how Istio mixer enables telemetry reporting .","title":"Istio telemetry"},{"location":"generatedContent/istio101/exercise-4/#configure-istio-to-receive-telemetry-data","text":"Enable Istio monitoring dashboards, by running these two commands: kubectl patch cm managed-istio-custom -n ibm-operators --type = 'json' -p = '[{\"op\": \"add\", \"path\": \"/data/istio-monitoring\", \"value\":\"true\"}]' kubectl annotate iop -n ibm-operators managed-istio --overwrite version = \"custom-applied-at: $( date ) \" Verify that the Grafana, Prometheus, Kiali and Jaeger add-ons were installed successfully. All add-ons are installed into the istio-system namespace. kubectl get services -n istio-system Obtain the guestbook endpoint to access the guestbook. You can access the guestbook via the external IP for your service as guestbook is deployed as a load balancer service. Get the EXTERNAL-IP of the guestbook service via output below: kubectl get service guestbook -n default Go to this external ip address in the browser to try out your guestbook. This service will route you to either v1 or v2, at random. If you wish to see a different version, you'll need to do a hard refresh ( cmd + shift + r on a mac, or ctrl + f5 on a PC). Alternatively, you can curl the address. Generate a small load to the app, replacing guestbook_IP with the EXTERNAL-IP. for i in { 1 ..40 } ; do sleep 0 .2 ; curl -I http://<guestbook_IP>/ ; done","title":"Configure Istio to receive telemetry data"},{"location":"generatedContent/istio101/exercise-4/#view-guestbook-telemetry-data","text":"","title":"View guestbook telemetry data"},{"location":"generatedContent/istio101/exercise-4/#jaeger","text":"Launch the Jaeger dashboard: istioctl dashboard jaeger From the Services menu, select either the guestbook or analyzer service. Scroll to the bottom and click on Find Traces button to see traces. Use Ctrl-C in the terminal to exit the port-foward when you are done. Read more about Jaeger","title":"Jaeger"},{"location":"generatedContent/istio101/exercise-4/#grafana","text":"Create a secret which will be used to set the login credentials for Grafana cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: grafana namespace: istio-system type: Opaque data: username: \"YWRtaW4=\" passphrase: \"YWRtaW4=\" EOF Wait 2 minutes for the secret to be picked up and then launch the dashboard: istioctl dashboard grafana Log in using admin for both username and password. Navigate to the Istio Service Dashboard by clicking on the Home menu on the top left, then Istio, then Istio Service Dashboard. Select guestbook in the Service drop down. In a different tab, visit the guestbook application and refresh the page multiple times to generate some load, or run the load script you used previously. Switch back to the Grafana tab. Use Ctrl-C in the terminal to exit the port-foward when you are done. This Grafana dashboard provides metrics for each workload. Explore the other dashboard provided as well. Read more about Grafana .","title":"Grafana"},{"location":"generatedContent/istio101/exercise-4/#prometheus","text":"Establish port forwarding from local port 9090 to the Prometheus instance. istioctl dashboard prometheus In the \u201cExpression\u201d input box, enter: istio_request_bytes_count . Click Execute and then select Graph. Then try another query: istio_requests_total{destination_service=\"guestbook.default.svc.cluster.local\", destination_version=\"2.0\"} Use Ctrl-C in the terminal to exit the port-foward when you are done.","title":"Prometheus"},{"location":"generatedContent/istio101/exercise-4/#kiali","text":"Kiali is an open-source project that installs on top of Istio to visualize your service mesh. It provides deeper insight into how your microservices interact with one another, and provides features such as circuit breakers and request rates for your services Create a secret which will be used to set the login credentials for Kiali cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: kiali namespace: istio-system labels: app: kiali type: Opaque data: username: \"YWRtaW4=\" passphrase: \"YWRtaW4=\" EOF Establish port forwarding from local port 20001 to the Kiali instance. istioctl dashboard kiali Login with admin for both username and password. Select Graph and then choose default namespace. You should see a visual service graph of the various services in your Istio mesh. Use the Edge Labels dropdown and select Traffic rate per second to see the request rates as well. Kiali has a number of views to help you visualize your services. Click through the vairous tabs to explore the service graph, and the various views for workloads, applications, and services.","title":"Kiali"},{"location":"generatedContent/istio101/exercise-4/#understand-what-happened","text":"Although Istio proxies are able to automatically send spans, they need some hints to tie together the entire trace. Apps need to propagate the appropriate HTTP headers so that when the proxies send span information to Zipkin or Jaeger, the spans can be correlated correctly into a single trace. In the example, when a user visits the Guestbook app, the HTTP request is sent from the guestbook service to Watson Tone Analyzer. In order for the individual spans of guestbook service and Watson Tone Analyzer to be tied together, we have modified the guestbook service to extract the required headers (x-request-id, x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, x-b3-flags, x-ot-span-context) and forward them onto the analyzer service when calling the analyzer service from the guestbook service. The change is in the v2/guestbook/main.go . By using the getForwardHeaders() method, we are able to extract the required headers, and then we use the required headers further when calling the analyzer service via the getPrimaryTone() method.","title":"Understand what happened"},{"location":"generatedContent/istio101/exercise-4/#questions","text":"Does a user need to modify their app to get metrics for their apps? A: 1. Yes 2. No. (2 is correct) Does a user need to modify their app to get distributed tracing for their app to work properly? A: 1. Yes 2. No. (1 is correct) What distributed tracing system does Istio support by default? A: 1. Zipkin 2. Kibana 3. LogStash 4. Jaeger. (1 and 4 are correct)","title":"Questions"},{"location":"generatedContent/istio101/exercise-4/#continue-to-exercise-5-expose-the-service-mesh-with-the-istio-ingress-gateway","text":"","title":"Continue to Exercise 5 - Expose the service mesh with the Istio Ingress Gateway"},{"location":"generatedContent/istio101/exercise-5/","text":"Exercise 5 - Expose the service mesh with the Istio Ingress Gateway \u00b6 The components deployed on the service mesh by default are not exposed outside the cluster. External access to individual services so far has been provided by creating an external load balancer or node port on each service. An Ingress Gateway resource can be created to allow external requests through the Istio Ingress Gateway to the backing services. Expose the Guestbook app with Ingress Gateway \u00b6 Configure the guestbook default route with the Istio Ingress Gateway. The guestbook-gateway.yaml file is in this repository (istio101) in the workshop/plans directory. cd ../../plans kubectl create -f guestbook-gateway.yaml Get the EXTERNAL-IP of the Istio Ingress Gateway. kubectl get service istio-ingressgateway -n istio-system Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 172 .21.254.53 169 .6.1.1 80 :31380/TCP,443:31390/TCP,31400:31400/TCP 1m 2d Make note of the external IP address that you retrieved in the previous step, as it will be used to access the Guestbook app in later parts of the course. Create an environment variable called $INGRESS_IP with your IP address. Example: export INGRESS_IP = 169 .6.1.1 Connect Istio Ingress Gateway to the IBM Cloud Kubernetes Service NLB Host Name \u00b6 NLB host names are the DNS host names you can generate for each IBM Cloud Kubernetes deployment exposed with the Network LoadBalancer(NLB) service. These host names come with SSL certificate, the DNS registration, and health checks so you can benefit from them for any deployments that you expose via the NLB on IBM Cloud Kubernetes Service. You can run the IBM Cloud Kubernetes Service ALB, an API gateway of your choice, an Istio ingress gateway, and an MQTT server in parallel in your IBM Cloud Kubernetes Service cluster. Each one will have its own: 1. Publicly available wildcard host name 2. Wildcard SSL certificate associated with the host name 3. Health checks that you can configure if you use multizone deployments. Let's leverage this feature with Istio ingress gateway: Let's first check if you have any NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER If you haven't used this feature before, you will get an empty list. Obtain the Istio ingress gateway's external IP. Get the EXTERNAL-IP of the istio-ingressgateway service via output below: kubectl get service istio-ingressgateway -n istio-system Create the NLB host with the Istio ingress gateway's public IP address: ibmcloud ks nlb-dns create classic --cluster $MYCLUSTER --ip $INGRESS_IP List the NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER Example output: shell Retrieving host names, certificates, IPs, and health check monitors for network load balancer (NLB) pods in cluster <cluster_name>... OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud [\"169.1.1.1\"] None created mycluster-85f044fc29ce613c264409c04a76c95d-0001 Make note of the NLB host name ( ), as it will be used to access your Guestbook app in later parts of the course. Create an environment variable for it and test using curl or visit in your browser. Example: export NLB_HOSTNAME = mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud curl $NLB_HOSTNAME Enable health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor configure --cluster $MYCLUSTER --nlb-host $NLB_HOSTNAME --type HTTP --description \"Istio ingress gateway health check\" --path \"/healthz/ready\" --port 15021 --enable Monitor the health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor status --cluster $MYCLUSTER After waiting for a bit, you should start to see the health monitor's status changed to Enabled. Example output: Retrieving health check monitor statuses for NLB pods... OK Hostname IP Health Monitor H.Monitor Status mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud 169 .1.1.1 Enabled Healthy Congratulations! You extended the base Ingress features by providing a DNS entry to the Istio service. References \u00b6 Kubernetes Ingress Istio Ingress Bring your own ALB Continue to Exercise 6 - Traffic Management \u00b6","title":"Exercise 5 - Expose the service mesh with the Istio Ingress Gateway"},{"location":"generatedContent/istio101/exercise-5/#exercise-5-expose-the-service-mesh-with-the-istio-ingress-gateway","text":"The components deployed on the service mesh by default are not exposed outside the cluster. External access to individual services so far has been provided by creating an external load balancer or node port on each service. An Ingress Gateway resource can be created to allow external requests through the Istio Ingress Gateway to the backing services.","title":"Exercise 5 - Expose the service mesh with the Istio Ingress Gateway"},{"location":"generatedContent/istio101/exercise-5/#expose-the-guestbook-app-with-ingress-gateway","text":"Configure the guestbook default route with the Istio Ingress Gateway. The guestbook-gateway.yaml file is in this repository (istio101) in the workshop/plans directory. cd ../../plans kubectl create -f guestbook-gateway.yaml Get the EXTERNAL-IP of the Istio Ingress Gateway. kubectl get service istio-ingressgateway -n istio-system Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 172 .21.254.53 169 .6.1.1 80 :31380/TCP,443:31390/TCP,31400:31400/TCP 1m 2d Make note of the external IP address that you retrieved in the previous step, as it will be used to access the Guestbook app in later parts of the course. Create an environment variable called $INGRESS_IP with your IP address. Example: export INGRESS_IP = 169 .6.1.1","title":"Expose the Guestbook app with Ingress Gateway"},{"location":"generatedContent/istio101/exercise-5/#connect-istio-ingress-gateway-to-the-ibm-cloud-kubernetes-service-nlb-host-name","text":"NLB host names are the DNS host names you can generate for each IBM Cloud Kubernetes deployment exposed with the Network LoadBalancer(NLB) service. These host names come with SSL certificate, the DNS registration, and health checks so you can benefit from them for any deployments that you expose via the NLB on IBM Cloud Kubernetes Service. You can run the IBM Cloud Kubernetes Service ALB, an API gateway of your choice, an Istio ingress gateway, and an MQTT server in parallel in your IBM Cloud Kubernetes Service cluster. Each one will have its own: 1. Publicly available wildcard host name 2. Wildcard SSL certificate associated with the host name 3. Health checks that you can configure if you use multizone deployments. Let's leverage this feature with Istio ingress gateway: Let's first check if you have any NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER If you haven't used this feature before, you will get an empty list. Obtain the Istio ingress gateway's external IP. Get the EXTERNAL-IP of the istio-ingressgateway service via output below: kubectl get service istio-ingressgateway -n istio-system Create the NLB host with the Istio ingress gateway's public IP address: ibmcloud ks nlb-dns create classic --cluster $MYCLUSTER --ip $INGRESS_IP List the NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER Example output: shell Retrieving host names, certificates, IPs, and health check monitors for network load balancer (NLB) pods in cluster <cluster_name>... OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud [\"169.1.1.1\"] None created mycluster-85f044fc29ce613c264409c04a76c95d-0001 Make note of the NLB host name ( ), as it will be used to access your Guestbook app in later parts of the course. Create an environment variable for it and test using curl or visit in your browser. Example: export NLB_HOSTNAME = mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud curl $NLB_HOSTNAME Enable health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor configure --cluster $MYCLUSTER --nlb-host $NLB_HOSTNAME --type HTTP --description \"Istio ingress gateway health check\" --path \"/healthz/ready\" --port 15021 --enable Monitor the health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor status --cluster $MYCLUSTER After waiting for a bit, you should start to see the health monitor's status changed to Enabled. Example output: Retrieving health check monitor statuses for NLB pods... OK Hostname IP Health Monitor H.Monitor Status mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud 169 .1.1.1 Enabled Healthy Congratulations! You extended the base Ingress features by providing a DNS entry to the Istio service.","title":"Connect Istio Ingress Gateway to the IBM Cloud Kubernetes Service NLB Host Name"},{"location":"generatedContent/istio101/exercise-5/#references","text":"Kubernetes Ingress Istio Ingress Bring your own ALB","title":"References"},{"location":"generatedContent/istio101/exercise-5/#continue-to-exercise-6-traffic-management","text":"","title":"Continue to Exercise 6 - Traffic Management"},{"location":"generatedContent/istio101/exercise-6/","text":"Exercise 6 - Perform traffic management \u00b6 Using rules to manage traffic \u00b6 The core component used for traffic management in Istio is Pilot, which manages and configures all the Envoy proxy instances deployed in a particular Istio service mesh. It lets you specify what rules you want to use to route traffic between Envoy proxies, which run as sidecars to each service in the mesh. Each service consists of any number of instances running on pods, containers, VMs etc. Each service can have any number of versions (a.k.a. subsets). There can be distinct subsets of service instances running different variants of the app binary. These variants are not necessarily different API versions. They could be iterative changes to the same service, deployed in different environments (prod, staging, dev, etc.). Pilot translates high-level rules into low-level configurations and distributes this config to Envoy instances. Pilot uses three types of configuration resources to manage traffic within its service mesh: Virtual Services, Destination Rules, and Service Entries. Virtual Services \u00b6 A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset or version of it) defined in the service registry. Destination Rules \u00b6 A DestinationRule defines policies that apply to traffic intended for a service after routing has occurred. These rules specify configuration for load balancing, connection pool size from the sidecar, and outlier detection settings to detect and evict unhealthy hosts from the load balancing pool. Any destination host and subset referenced in a VirtualService rule must be defined in a corresponding DestinationRule . Service Entries \u00b6 A ServiceEntry configuration enables services within the mesh to access a service not necessarily managed by Istio. The rule describes the endpoints, ports and protocols of a white-listed set of mesh-external domains and IP blocks that services in the mesh are allowed to access. The Guestbook app \u00b6 In the Guestbook app, there is one service: guestbook. The guestbook service has two distinct versions: the base version (version 1) and the modernized version (version 2). Each version of the service has three instances based on the number of replicas in guestbook-deployment.yaml and guestbook-v2-deployment.yaml . By default, prior to creating any rules, Istio will route requests equally across version 1 and version 2 of the guestbook service and their respective instances in a round robin manner. However, new versions of a service can easily introduce bugs to the service mesh, so following A/B Testing and Canary Deployments is good practice. A/B testing with Istio \u00b6 A/B testing is a method of performing identical tests against two separate service versions in order to determine which performs better. To prevent Istio from performing the default routing behavior between the original and modernized guestbook service, define the following rules (found in istio101/workshop/plans ): kubectl create -f guestbook-destination.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : destination-rule-guestbook spec : host : guestbook subsets : - name : v1 labels : version : '1.0' - name : v2 labels : version : '2.0' Next, apply the VirtualService kubectl replace -f virtualservice-all-v1.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 The VirtualService defines a rule that captures all HTTP traffic coming in through the Istio ingress gateway, guestbook-gateway , and routes 100% of the traffic to pods of the guestbook service with label \"version: v1\". A subset or version of a route destination is identified with a reference to a named service subset which must be declared in a corresponding DestinationRule . Since there are three instances matching the criteria of hostname guestbook and subset version: v1 , by default Envoy will send traffic to all three instances in a round robin manner. View the guestbook application using the $NLB_HOSTNAME specified in Exercise 5 and enter it as a URL in Firefox or Chrome web browsers. You can use the echo command to get this value, if you don't remember it. echo $NLB_HOSTNAME To enable the Istio service mesh for A/B testing against the new service version, modify the original VirtualService rule: kubectl replace -f virtualservice-test.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - match : - headers : user-agent : regex : '.*Firefox.*' route : - destination : host : guestbook subset : v2 - route : - destination : host : guestbook subset : v1 In Istio VirtualService rules, there can be only one rule for each service and therefore when defining multiple HTTPRoute blocks, the order in which they are defined in the yaml matters. Hence, the original VirtualService rule is modified rather than creating a new rule. With the modified rule, incoming requests originating from Firefox browsers will go to the newer version of guestbook. All other requests fall-through to the next block, which routes all traffic to the original version of guestbook. Canary deployment \u00b6 In Canary Deployments , newer versions of services are incrementally rolled out to users to minimize the risk and impact of any bugs introduced by the newer version. To begin incrementally routing traffic to the newer version of the guestbook service, modify the original VirtualService rule: kubectl replace -f virtualservice-80-20.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 weight : 80 - destination : host : guestbook subset : v2 weight : 20 In the modified rule, the routed traffic is split between two different subsets of the guestbook service. In this manner, traffic to the modernized version 2 of guestbook is controlled on a percentage basis to limit the impact of any unforeseen bugs. This rule can be modified over time until eventually all traffic is directed to the newer version of the service. View the guestbook application using the $NLB_HOSTNAME specified in Exercise 5 and enter it as a URL in Firefox or Chrome web browsers. Ensure that you are using a hard refresh (command + Shift + R on Mac or Ctrl + F5 on windows) to remove any browser caching. You should notice that the guestbook should swap between V1 or V2 at about the weight you specified. Route all traffic to v2 \u00b6 For the following exercises, we'll be working with Guestbook v2. Route all traffic to guestbook v2 with a new VirtualService rule: cat <<EOF | kubectl replace -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: virtual-service-guestbook spec: hosts: - '*' gateways: - guestbook-gateway http: - route: - destination: host: guestbook subset: v2 EOF Implementing circuit breakers with destination rules \u00b6 Istio DestinationRules allow users to configure Envoy's implementation of circuit breakers . Circuit breakers are critical for defining the behavior for service-to-service communication in the service mesh. In the event of a failure for a particular service, circuit breakers allow users to set global defaults for failure recovery on a per service and/or per service version basis. Users can apply a traffic policy at the top level of the DestinationRule to create circuit breaker settings for an entire service, or it can be defined at the subset level to create settings for a particular version of a service. Depending on whether a service handles HTTP requests or TCP connections, DestinationRules expose a number of ways for Envoy to limit traffic to a particular service as well as define failure recovery behavior for services initiating the connection to an unhealthy service. Further reading \u00b6 Istio Concept Istio Rules API Istio Proxy Debug Tool Traffic Management Circuit Breaking Timeouts and Retries Questions \u00b6 Where are routing rules defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: VirtualService Where are service versions (subsets) defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: DestinationRule Which Istio component is responsible for sending traffic management configurations to Istio sidecars? Options: (Mixer, Citadel, Pilot, Kubernetes) Answer: Pilot What is the name of the default proxy that runs in Istio sidecars and routes requests within the service mesh? Options: (NGINX, Envoy, HAProxy) Answer: Envoy Continue to Exercise 7 - Security \u00b6","title":"Exercise 6 - Perform traffic management"},{"location":"generatedContent/istio101/exercise-6/#exercise-6-perform-traffic-management","text":"","title":"Exercise 6 - Perform traffic management"},{"location":"generatedContent/istio101/exercise-6/#using-rules-to-manage-traffic","text":"The core component used for traffic management in Istio is Pilot, which manages and configures all the Envoy proxy instances deployed in a particular Istio service mesh. It lets you specify what rules you want to use to route traffic between Envoy proxies, which run as sidecars to each service in the mesh. Each service consists of any number of instances running on pods, containers, VMs etc. Each service can have any number of versions (a.k.a. subsets). There can be distinct subsets of service instances running different variants of the app binary. These variants are not necessarily different API versions. They could be iterative changes to the same service, deployed in different environments (prod, staging, dev, etc.). Pilot translates high-level rules into low-level configurations and distributes this config to Envoy instances. Pilot uses three types of configuration resources to manage traffic within its service mesh: Virtual Services, Destination Rules, and Service Entries.","title":"Using rules to manage traffic"},{"location":"generatedContent/istio101/exercise-6/#virtual-services","text":"A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset or version of it) defined in the service registry.","title":"Virtual Services"},{"location":"generatedContent/istio101/exercise-6/#destination-rules","text":"A DestinationRule defines policies that apply to traffic intended for a service after routing has occurred. These rules specify configuration for load balancing, connection pool size from the sidecar, and outlier detection settings to detect and evict unhealthy hosts from the load balancing pool. Any destination host and subset referenced in a VirtualService rule must be defined in a corresponding DestinationRule .","title":"Destination Rules"},{"location":"generatedContent/istio101/exercise-6/#service-entries","text":"A ServiceEntry configuration enables services within the mesh to access a service not necessarily managed by Istio. The rule describes the endpoints, ports and protocols of a white-listed set of mesh-external domains and IP blocks that services in the mesh are allowed to access.","title":"Service Entries"},{"location":"generatedContent/istio101/exercise-6/#the-guestbook-app","text":"In the Guestbook app, there is one service: guestbook. The guestbook service has two distinct versions: the base version (version 1) and the modernized version (version 2). Each version of the service has three instances based on the number of replicas in guestbook-deployment.yaml and guestbook-v2-deployment.yaml . By default, prior to creating any rules, Istio will route requests equally across version 1 and version 2 of the guestbook service and their respective instances in a round robin manner. However, new versions of a service can easily introduce bugs to the service mesh, so following A/B Testing and Canary Deployments is good practice.","title":"The Guestbook app"},{"location":"generatedContent/istio101/exercise-6/#ab-testing-with-istio","text":"A/B testing is a method of performing identical tests against two separate service versions in order to determine which performs better. To prevent Istio from performing the default routing behavior between the original and modernized guestbook service, define the following rules (found in istio101/workshop/plans ): kubectl create -f guestbook-destination.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : destination-rule-guestbook spec : host : guestbook subsets : - name : v1 labels : version : '1.0' - name : v2 labels : version : '2.0' Next, apply the VirtualService kubectl replace -f virtualservice-all-v1.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 The VirtualService defines a rule that captures all HTTP traffic coming in through the Istio ingress gateway, guestbook-gateway , and routes 100% of the traffic to pods of the guestbook service with label \"version: v1\". A subset or version of a route destination is identified with a reference to a named service subset which must be declared in a corresponding DestinationRule . Since there are three instances matching the criteria of hostname guestbook and subset version: v1 , by default Envoy will send traffic to all three instances in a round robin manner. View the guestbook application using the $NLB_HOSTNAME specified in Exercise 5 and enter it as a URL in Firefox or Chrome web browsers. You can use the echo command to get this value, if you don't remember it. echo $NLB_HOSTNAME To enable the Istio service mesh for A/B testing against the new service version, modify the original VirtualService rule: kubectl replace -f virtualservice-test.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - match : - headers : user-agent : regex : '.*Firefox.*' route : - destination : host : guestbook subset : v2 - route : - destination : host : guestbook subset : v1 In Istio VirtualService rules, there can be only one rule for each service and therefore when defining multiple HTTPRoute blocks, the order in which they are defined in the yaml matters. Hence, the original VirtualService rule is modified rather than creating a new rule. With the modified rule, incoming requests originating from Firefox browsers will go to the newer version of guestbook. All other requests fall-through to the next block, which routes all traffic to the original version of guestbook.","title":"A/B testing with Istio"},{"location":"generatedContent/istio101/exercise-6/#canary-deployment","text":"In Canary Deployments , newer versions of services are incrementally rolled out to users to minimize the risk and impact of any bugs introduced by the newer version. To begin incrementally routing traffic to the newer version of the guestbook service, modify the original VirtualService rule: kubectl replace -f virtualservice-80-20.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 weight : 80 - destination : host : guestbook subset : v2 weight : 20 In the modified rule, the routed traffic is split between two different subsets of the guestbook service. In this manner, traffic to the modernized version 2 of guestbook is controlled on a percentage basis to limit the impact of any unforeseen bugs. This rule can be modified over time until eventually all traffic is directed to the newer version of the service. View the guestbook application using the $NLB_HOSTNAME specified in Exercise 5 and enter it as a URL in Firefox or Chrome web browsers. Ensure that you are using a hard refresh (command + Shift + R on Mac or Ctrl + F5 on windows) to remove any browser caching. You should notice that the guestbook should swap between V1 or V2 at about the weight you specified.","title":"Canary deployment"},{"location":"generatedContent/istio101/exercise-6/#route-all-traffic-to-v2","text":"For the following exercises, we'll be working with Guestbook v2. Route all traffic to guestbook v2 with a new VirtualService rule: cat <<EOF | kubectl replace -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: virtual-service-guestbook spec: hosts: - '*' gateways: - guestbook-gateway http: - route: - destination: host: guestbook subset: v2 EOF","title":"Route all traffic to v2"},{"location":"generatedContent/istio101/exercise-6/#implementing-circuit-breakers-with-destination-rules","text":"Istio DestinationRules allow users to configure Envoy's implementation of circuit breakers . Circuit breakers are critical for defining the behavior for service-to-service communication in the service mesh. In the event of a failure for a particular service, circuit breakers allow users to set global defaults for failure recovery on a per service and/or per service version basis. Users can apply a traffic policy at the top level of the DestinationRule to create circuit breaker settings for an entire service, or it can be defined at the subset level to create settings for a particular version of a service. Depending on whether a service handles HTTP requests or TCP connections, DestinationRules expose a number of ways for Envoy to limit traffic to a particular service as well as define failure recovery behavior for services initiating the connection to an unhealthy service.","title":"Implementing circuit breakers with destination rules"},{"location":"generatedContent/istio101/exercise-6/#further-reading","text":"Istio Concept Istio Rules API Istio Proxy Debug Tool Traffic Management Circuit Breaking Timeouts and Retries","title":"Further reading"},{"location":"generatedContent/istio101/exercise-6/#questions","text":"Where are routing rules defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: VirtualService Where are service versions (subsets) defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: DestinationRule Which Istio component is responsible for sending traffic management configurations to Istio sidecars? Options: (Mixer, Citadel, Pilot, Kubernetes) Answer: Pilot What is the name of the default proxy that runs in Istio sidecars and routes requests within the service mesh? Options: (NGINX, Envoy, HAProxy) Answer: Envoy","title":"Questions"},{"location":"generatedContent/istio101/exercise-6/#continue-to-exercise-7-security","text":"","title":"Continue to Exercise 7 - Security"},{"location":"generatedContent/istio101/exercise-7/","text":"Exercise 7 - Secure your services \u00b6 Mutual authentication with Transport Layer Security (mTLS) \u00b6 Istio can secure the communication between microservices without requiring application code changes. Security is provided by authenticating and encrypting communication paths within the cluster. This is becoming a common security and compliance requirement. Delegating communication security to Istio (as opposed to implementing TLS in each microservice), ensures that your application will be deployed with consistent and manageable security policies. Istio Citadel is an optional part of Istio's control plane components. When enabled, it provides each Envoy sidecar proxy with a strong (cryptographic) identity, in the form of a certificate. Identity is based on the microservice's service account and is independent of its specific network location, such as cluster or current IP address. Envoys then use the certificates to identify each other and establish an authenticated and encrypted communication channel between them. Citadel is responsible for: Providing each service with an identity representing its role. Providing a common trust root to allow Envoys to validate and authenticate each other. Providing a key management system, automating generation, distribution, and rotation of certificates and keys. When an application microservice connects to another microservice, the communication is redirected through the client side and server side Envoys. The end-to-end communication path is: Local TCP connection (i.e., localhost , not reaching the \"wire\") between the application and Envoy (client- and server-side); Mutually authenticated and encrypted connection between Envoy proxies. When Envoy proxies establish a connection, they exchange and validate certificates to confirm that each is indeed connected to a valid and expected peer. The established identities can later be used as basis for policy checks (e.g., access authorization). Enforce mTLS between all Istio services \u00b6 To enforce a mesh-wide authentication policy that requires mutual TLS, submit the following policy. This policy specifies that all workloads in the mesh will only accept encrypted requests using TLS. kubectl apply -f - <<EOF apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"istio-system\" spec: mtls: mode: STRICT EOF Visit your guestbook application by going to it in your browser. Everything should be working as expected! To confirm mTLS is infact enabled, you can run: istioctl x describe service guestbook Example output: Service : guestbook Port : http 80/HTTP targets pod port 3000 DestinationRule : destination-rule-guestbook for \"guestbook\" Matching subsets : v1,v2 No Traffic Policy Pod is STRICT, clients configured automatically Configure access control for workloads using HTTP traffic \u00b6 Modify guestbook and analyzer deployments to use leverage the service accounts. Navigate to your guestbook dir first, for example: cd ../guestbook Add serviceaccount to your guestbook and analyzer deployments echo \" serviceAccountName: guestbook\" >> v1/guestbook-deployment.yaml echo \" serviceAccountName: guestbook\" >> v2/guestbook-deployment.yaml echo \" serviceAccountName: analyzer\" >> v2/analyzer-deployment.yaml redeploy the guestbook and analyzer deployments kubectl replace -f v1/guestbook-deployment.yaml kubectl replace -f v2/guestbook-deployment.yaml kubectl replace -f v2/analyzer-deployment.yaml Create a AuthorizationPolicy to disable all access to analyzer service. This will effectively not allow guestbook or any services to access it. cat <<EOF | kubectl create -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: analyzeraccess spec: selector: matchLabels: app: analyzer EOF Output: authorizationpolicy.security.istio.io/analyzeraccess created Visit the Guestbook app from your favorite browser and validate that Guestbook V1 continues to work while Guestbook V2 will not run correctly. For every new message you write on the Guestbook v2 app, you will get a message such as \"Error - unable to detect Tone from the Analyzer service\". It can take up to 15 seconds for the change to propogate to the envoy sidecar(s) so you may not see the error right away. Configure the Analyzer service to only allow access from the Guestbook service using the added rules section: cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: analyzeraccess spec: selector: matchLabels: app: analyzer rules: - from: - source: principals: [\"cluster.local/ns/default/sa/guestbook\"] to: - operation: methods: [\"POST\"] EOF Visit the Guestbook app from your favorite browser and validate that Guestbook V2 works now. It can take a few seconds for the change to propogate to the envoy sidecar(s) so you may not observe Guestbook V2 to function right away. Cleanup \u00b6 Run the following commands to clean up the Istio configuration resources as part of this exercise: kubectl delete PeerAuthentication default kubectl delete dr default kubectl delete dr destination-rule-guestbook kubectl delete sa guestbook analyzer kubectl delete AuthorizationPolicy analyzeraccess Quiz \u00b6 True or False? Istio Citadel provides each microservice with a strong, cryptographic, identity in the form of a certificate. The certificates' life cycle is fully managed by Istio. (True) Istio provides microservices with mutually authenticated connections, without requiring app code changes. (True) Mutual authentication must be on or off for the entire cluster, gradual adoption is not possible. (False) Further Reading \u00b6 Basic TLS/SSL Terminology TLS Handshake Explained Istio Task Istio Concept","title":"Exercise 7 - Secure your services"},{"location":"generatedContent/istio101/exercise-7/#exercise-7-secure-your-services","text":"","title":"Exercise 7 - Secure your services"},{"location":"generatedContent/istio101/exercise-7/#mutual-authentication-with-transport-layer-security-mtls","text":"Istio can secure the communication between microservices without requiring application code changes. Security is provided by authenticating and encrypting communication paths within the cluster. This is becoming a common security and compliance requirement. Delegating communication security to Istio (as opposed to implementing TLS in each microservice), ensures that your application will be deployed with consistent and manageable security policies. Istio Citadel is an optional part of Istio's control plane components. When enabled, it provides each Envoy sidecar proxy with a strong (cryptographic) identity, in the form of a certificate. Identity is based on the microservice's service account and is independent of its specific network location, such as cluster or current IP address. Envoys then use the certificates to identify each other and establish an authenticated and encrypted communication channel between them. Citadel is responsible for: Providing each service with an identity representing its role. Providing a common trust root to allow Envoys to validate and authenticate each other. Providing a key management system, automating generation, distribution, and rotation of certificates and keys. When an application microservice connects to another microservice, the communication is redirected through the client side and server side Envoys. The end-to-end communication path is: Local TCP connection (i.e., localhost , not reaching the \"wire\") between the application and Envoy (client- and server-side); Mutually authenticated and encrypted connection between Envoy proxies. When Envoy proxies establish a connection, they exchange and validate certificates to confirm that each is indeed connected to a valid and expected peer. The established identities can later be used as basis for policy checks (e.g., access authorization).","title":"Mutual authentication with Transport Layer Security (mTLS)"},{"location":"generatedContent/istio101/exercise-7/#enforce-mtls-between-all-istio-services","text":"To enforce a mesh-wide authentication policy that requires mutual TLS, submit the following policy. This policy specifies that all workloads in the mesh will only accept encrypted requests using TLS. kubectl apply -f - <<EOF apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"istio-system\" spec: mtls: mode: STRICT EOF Visit your guestbook application by going to it in your browser. Everything should be working as expected! To confirm mTLS is infact enabled, you can run: istioctl x describe service guestbook Example output: Service : guestbook Port : http 80/HTTP targets pod port 3000 DestinationRule : destination-rule-guestbook for \"guestbook\" Matching subsets : v1,v2 No Traffic Policy Pod is STRICT, clients configured automatically","title":"Enforce mTLS between all Istio services"},{"location":"generatedContent/istio101/exercise-7/#configure-access-control-for-workloads-using-http-traffic","text":"Modify guestbook and analyzer deployments to use leverage the service accounts. Navigate to your guestbook dir first, for example: cd ../guestbook Add serviceaccount to your guestbook and analyzer deployments echo \" serviceAccountName: guestbook\" >> v1/guestbook-deployment.yaml echo \" serviceAccountName: guestbook\" >> v2/guestbook-deployment.yaml echo \" serviceAccountName: analyzer\" >> v2/analyzer-deployment.yaml redeploy the guestbook and analyzer deployments kubectl replace -f v1/guestbook-deployment.yaml kubectl replace -f v2/guestbook-deployment.yaml kubectl replace -f v2/analyzer-deployment.yaml Create a AuthorizationPolicy to disable all access to analyzer service. This will effectively not allow guestbook or any services to access it. cat <<EOF | kubectl create -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: analyzeraccess spec: selector: matchLabels: app: analyzer EOF Output: authorizationpolicy.security.istio.io/analyzeraccess created Visit the Guestbook app from your favorite browser and validate that Guestbook V1 continues to work while Guestbook V2 will not run correctly. For every new message you write on the Guestbook v2 app, you will get a message such as \"Error - unable to detect Tone from the Analyzer service\". It can take up to 15 seconds for the change to propogate to the envoy sidecar(s) so you may not see the error right away. Configure the Analyzer service to only allow access from the Guestbook service using the added rules section: cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: analyzeraccess spec: selector: matchLabels: app: analyzer rules: - from: - source: principals: [\"cluster.local/ns/default/sa/guestbook\"] to: - operation: methods: [\"POST\"] EOF Visit the Guestbook app from your favorite browser and validate that Guestbook V2 works now. It can take a few seconds for the change to propogate to the envoy sidecar(s) so you may not observe Guestbook V2 to function right away.","title":"Configure access control for workloads using HTTP traffic"},{"location":"generatedContent/istio101/exercise-7/#cleanup","text":"Run the following commands to clean up the Istio configuration resources as part of this exercise: kubectl delete PeerAuthentication default kubectl delete dr default kubectl delete dr destination-rule-guestbook kubectl delete sa guestbook analyzer kubectl delete AuthorizationPolicy analyzeraccess","title":"Cleanup"},{"location":"generatedContent/istio101/exercise-7/#quiz","text":"True or False? Istio Citadel provides each microservice with a strong, cryptographic, identity in the form of a certificate. The certificates' life cycle is fully managed by Istio. (True) Istio provides microservices with mutually authenticated connections, without requiring app code changes. (True) Mutual authentication must be on or off for the entire cluster, gradual adoption is not possible. (False)","title":"Quiz"},{"location":"generatedContent/istio101/exercise-7/#further-reading","text":"Basic TLS/SSL Terminology TLS Handshake Explained Istio Task Istio Concept","title":"Further Reading"},{"location":"generatedContent/kube101/","text":"IBM Cloud Kubernetes Service Lab \u00b6 An introduction to containers \u00b6 Hey, are you looking for a containers 101 course? Check out our Docker Essentials . Containers allow you to run securely isolated applications with quotas on system resources. Containers started out as an individual feature delivered with the linux kernel. Docker launched with making containers easy to use and developers quickly latched onto that idea. Containers have also sparked an interest in microservice architecture, a design pattern for developing applications in which complex applications are down into smaller, composable pieces which work together. Watch this video to learn about production uses of containers. Objectives \u00b6 This lab is an introduction to using Docker containers on Kubernetes in the IBM Cloud Kubernetes Service. By the end of the course, you'll achieve these objectives: Understand core concepts of Kubernetes Build a Docker image and deploy an application on Kubernetes in the IBM Cloud Kubernetes Service Control application deployments, while minimizing your time with infrastructure management Add AI services to extend your app Secure and monitor your cluster and app Prerequisites \u00b6 A Pay-As-You-Go or Subscription IBM Cloud account Virtual machines \u00b6 Prior to containers, most infrastructure ran not on bare metal, but atop hypervisors managing multiple virtualized operating systems (OSes). This arrangement allowed isolation of applications from one another on a higher level than that provided by the OS. These virtualized operating systems see what looks like their own exclusive hardware. However, this also means that each of these virtual operating systems are replicating an entire OS, taking up disk space. Containers \u00b6 Containers provide isolation similar to VMs, except provided by the OS and at the process level. Each container is a process or group of processes run in isolation. Typical containers explicitly run only a single process, as they have no need for the standard system services. What they usually need to do can be provided by system calls to the base OS kernel. The isolation on linux is provided by a feature called 'namespaces'. Each different kind of isolation (IE user, cgroups) is provided by a different namespace. This is a list of some of the namespaces that are commonly used and visible to the user: PID - process IDs USER - user and group IDs UTS - hostname and domain name NS - mount points NET - network devices, stacks, and ports CGROUPS - control limits and monitoring of resources VM vs container \u00b6 Traditional applications are run on native hardware. A single application does not typically use the full resources of a single machine. We try to run multiple applications on a single machine to avoid wasting resources. We could run multiple copies of the same application, but to provide isolation we use VMs to run multiple application instances (VMs) on the same hardware. These VMs have full operating system stacks which make them relatively large and inefficient due to duplication both at runtime and on disk. Containers allow you to share the host OS. This reduces duplication while still providing the isolation. Containers also allow you to drop unneeded files such as system libraries and binaries to save space and reduce your attack surface. If SSHD or LIBC are not installed, they cannot be exploited. Get set up \u00b6 Before we dive into Kubernetes, you need to provision a cluster for your containerized app. Then you won't have to wait for it to be ready for the subsequent labs. You must install the CLIs per https://console.ng.bluemix.net/docs/containers/cs_cli_install.html . If you do not yet have these CLIs and the Kubernetes CLI, do lab 0 before starting the course. If you haven't already, provision a cluster. This can take a few minutes, so let it start first: ibmcloud cs cluster-create --name <name-of-cluster> After creation, before using the cluster, make sure it has completed provisioning and is ready for use. Run ibmcloud cs clusters and make sure that your cluster is in state \"deployed\". Then use ibmcloud cs workers <name-of-cluster> and make sure that all worker nodes are in state \"normal\" with Status \"Ready\". Kubernetes and containers: an overview \u00b6 Let's talk about Kubernetes orchestration for containers before we build an application on it. We need to understand the following facts about it: What is Kubernetes, exactly? How was Kubernetes created? Kubernetes architecture Kubernetes resource model Kubernetes at IBM Let's get started What is Kubernetes? \u00b6 Now that we know what containers are, let's define what Kubernetes is. Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Your applications may need many other resources to run such as Volumes, Networks, and Secrets that will help you to do things such as connect to databases, talk to firewalled backends, and secure keys. Kubernetes helps you add these resources into your application. Infrastructure resources needed by applications are managed declaratively. Fast fact: Other orchestration technologies are Mesos and Swarm. The key paradigm of kubernetes is it\u2019s Declarative model. The user provides the \"desired state\" and Kubernetes will do it's best make it happen. If you need 5 instances, you do not start 5 separate instances on your own but rather tell Kubernetes that you need 5 instances and Kubernetes will reconcile the state automatically. Simply at this point you need to know that you declare the state you want and Kubernetes makes that happen. If something goes wrong with one of your instances and it crashes, Kubernetes still knows the desired state and creates a new instances on an available node. Fun to know: Kubernetes goes by many names. Sometimes it is shortened to k8s (losing the internal 8 letters), or kube . The word is rooted in ancient Greek and means \"Helmsman\". A helmsman is the person who steers a ship. We hope you can seen the analogy between directing a ship and the decisions made to orchestrate containers on a cluster. How was Kubernetes created? \u00b6 Google wanted to open source their knowledge of creating and running the internal tools Borg & Omega. It adopted Open Governance for Kubernetes by starting the Cloud Native Computing Foundation (CNCF) and giving Kubernetes to that foundation, therefore making it less influenced by Google directly. Many companies such as RedHat, Microsoft, IBM and Amazon quickly joined the foundation. Main entry point for the kubernetes project is at http://kubernetes.io and the source code can be found at https://github.com/kubernetes . Kubernetes architecture \u00b6 At its core, Kubernetes is a data store (etcd). The declarative model is stored in the data store as objects, that means when you say I want 5 instances of a container then that request is stored into the data store. This information change is watched and delegated to Controllers to take action. Controllers then react to the model and attempt to take action to achieve the desired state. The power of Kubernetes is in its simplistic model. As shown, API server is a simple HTTP server handling create/read/update/delete(CRUD) operations on the data store. Then the controller picks up the change you wanted and makes that happen. Controllers are responsible for instantiating the actual resource represented by any Kubernetes resource. These actual resources are what your application needs to allow it to run successfully. Kubernetes resource model \u00b6 Kubernetes Infrastructure defines a resource for every purpose. Each resource is monitored and processed by a controller. When you define your application, it contains a collection of these resources. This collection will then be read by Controllers to build your applications actual backing instances. Some of resources that you may work with are listed below for your reference, for a full list you should go to https://kubernetes.io/docs/concepts/ . In this class we will only use a few of them, like Pod, Deployment, etc. Config Maps holds configuration data for pods to consume. Daemon Sets ensure that each node in the cluster runs this Pod Deployments defines a desired state of a deployment object Events provides lifecycle events on Pods and other deployment objects Endpoints allows a inbound connections to reach the cluster services Ingress is a collection of rules that allow inbound connections to reach the cluster services Jobs creates one or more pods and as they complete successfully the job is marked as completed. Node is a worker machine in Kubernetes Namespaces are multiple virtual clusters backed by the same physical cluster Pods are the smallest deployable units of computing that can be created and managed in Kubernetes Persistent Volumes provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed Replica Sets ensures that a specified number of pod replicas are running at any given time Secrets are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys Service Accounts provides an identity for processes that run in a Pod Services is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. Stateful Sets is the workload API object used to manage stateful applications. and more... Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model. Key resources \u00b6 A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. A Pod typically represent a process in your cluster. Pods contain at least one container that runs the job and additionally may have other containers in it called sidecars for monitoring, logging, etc. Essentially a Pod is a group of containers. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application. We will see why separating the application logic and backend database into separate Pods will scale better when we build an application shortly. Services define how to expose your app as a DNS entry to have a stable reference. We use query based selector to choose which pods are supplying that service. The user directly manipulates resources via yaml: kubectl ( create | get | apply | delete ) -f myResource.yaml Kubernetes provides us with a client interface through \u2018kubectl\u2019. Kubectl commands allow you to manage your applications, manage cluster and cluster resources, by modifying the model in the data store. Kubernetes application deployment workflow \u00b6 User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods - including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact. Lab information \u00b6 IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Kubernetes Service runs Kubernetes clusters which deliver the following: Powerful tools Intuitive user experience Built-in security and isolation to enable rapid delivery of secure applications Cloud services including cognitive capabilities from Watson Capability to manage dedicated cluster resources for both stateless applications and stateful workloads Lab overview \u00b6 Lab 0 (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine. Lab 1 : This lab walks through creating and deploying a simple \"guestbook\" app written in Go as a net/http Server and accessing it. Lab 2 : Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Kubernetes Service Lab 3 : Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices. Lab 4 : How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention. Lab D : Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs.","title":"About the workshop"},{"location":"generatedContent/kube101/#ibm-cloud-kubernetes-service-lab","text":"","title":"IBM Cloud Kubernetes Service Lab"},{"location":"generatedContent/kube101/#an-introduction-to-containers","text":"Hey, are you looking for a containers 101 course? Check out our Docker Essentials . Containers allow you to run securely isolated applications with quotas on system resources. Containers started out as an individual feature delivered with the linux kernel. Docker launched with making containers easy to use and developers quickly latched onto that idea. Containers have also sparked an interest in microservice architecture, a design pattern for developing applications in which complex applications are down into smaller, composable pieces which work together. Watch this video to learn about production uses of containers.","title":"An introduction to containers"},{"location":"generatedContent/kube101/#objectives","text":"This lab is an introduction to using Docker containers on Kubernetes in the IBM Cloud Kubernetes Service. By the end of the course, you'll achieve these objectives: Understand core concepts of Kubernetes Build a Docker image and deploy an application on Kubernetes in the IBM Cloud Kubernetes Service Control application deployments, while minimizing your time with infrastructure management Add AI services to extend your app Secure and monitor your cluster and app","title":"Objectives"},{"location":"generatedContent/kube101/#prerequisites","text":"A Pay-As-You-Go or Subscription IBM Cloud account","title":"Prerequisites"},{"location":"generatedContent/kube101/#virtual-machines","text":"Prior to containers, most infrastructure ran not on bare metal, but atop hypervisors managing multiple virtualized operating systems (OSes). This arrangement allowed isolation of applications from one another on a higher level than that provided by the OS. These virtualized operating systems see what looks like their own exclusive hardware. However, this also means that each of these virtual operating systems are replicating an entire OS, taking up disk space.","title":"Virtual machines"},{"location":"generatedContent/kube101/#containers","text":"Containers provide isolation similar to VMs, except provided by the OS and at the process level. Each container is a process or group of processes run in isolation. Typical containers explicitly run only a single process, as they have no need for the standard system services. What they usually need to do can be provided by system calls to the base OS kernel. The isolation on linux is provided by a feature called 'namespaces'. Each different kind of isolation (IE user, cgroups) is provided by a different namespace. This is a list of some of the namespaces that are commonly used and visible to the user: PID - process IDs USER - user and group IDs UTS - hostname and domain name NS - mount points NET - network devices, stacks, and ports CGROUPS - control limits and monitoring of resources","title":"Containers"},{"location":"generatedContent/kube101/#vm-vs-container","text":"Traditional applications are run on native hardware. A single application does not typically use the full resources of a single machine. We try to run multiple applications on a single machine to avoid wasting resources. We could run multiple copies of the same application, but to provide isolation we use VMs to run multiple application instances (VMs) on the same hardware. These VMs have full operating system stacks which make them relatively large and inefficient due to duplication both at runtime and on disk. Containers allow you to share the host OS. This reduces duplication while still providing the isolation. Containers also allow you to drop unneeded files such as system libraries and binaries to save space and reduce your attack surface. If SSHD or LIBC are not installed, they cannot be exploited.","title":"VM vs container"},{"location":"generatedContent/kube101/#get-set-up","text":"Before we dive into Kubernetes, you need to provision a cluster for your containerized app. Then you won't have to wait for it to be ready for the subsequent labs. You must install the CLIs per https://console.ng.bluemix.net/docs/containers/cs_cli_install.html . If you do not yet have these CLIs and the Kubernetes CLI, do lab 0 before starting the course. If you haven't already, provision a cluster. This can take a few minutes, so let it start first: ibmcloud cs cluster-create --name <name-of-cluster> After creation, before using the cluster, make sure it has completed provisioning and is ready for use. Run ibmcloud cs clusters and make sure that your cluster is in state \"deployed\". Then use ibmcloud cs workers <name-of-cluster> and make sure that all worker nodes are in state \"normal\" with Status \"Ready\".","title":"Get set up"},{"location":"generatedContent/kube101/#kubernetes-and-containers-an-overview","text":"Let's talk about Kubernetes orchestration for containers before we build an application on it. We need to understand the following facts about it: What is Kubernetes, exactly? How was Kubernetes created? Kubernetes architecture Kubernetes resource model Kubernetes at IBM Let's get started","title":"Kubernetes and containers: an overview"},{"location":"generatedContent/kube101/#what-is-kubernetes","text":"Now that we know what containers are, let's define what Kubernetes is. Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Your applications may need many other resources to run such as Volumes, Networks, and Secrets that will help you to do things such as connect to databases, talk to firewalled backends, and secure keys. Kubernetes helps you add these resources into your application. Infrastructure resources needed by applications are managed declaratively. Fast fact: Other orchestration technologies are Mesos and Swarm. The key paradigm of kubernetes is it\u2019s Declarative model. The user provides the \"desired state\" and Kubernetes will do it's best make it happen. If you need 5 instances, you do not start 5 separate instances on your own but rather tell Kubernetes that you need 5 instances and Kubernetes will reconcile the state automatically. Simply at this point you need to know that you declare the state you want and Kubernetes makes that happen. If something goes wrong with one of your instances and it crashes, Kubernetes still knows the desired state and creates a new instances on an available node. Fun to know: Kubernetes goes by many names. Sometimes it is shortened to k8s (losing the internal 8 letters), or kube . The word is rooted in ancient Greek and means \"Helmsman\". A helmsman is the person who steers a ship. We hope you can seen the analogy between directing a ship and the decisions made to orchestrate containers on a cluster.","title":"What is Kubernetes?"},{"location":"generatedContent/kube101/#how-was-kubernetes-created","text":"Google wanted to open source their knowledge of creating and running the internal tools Borg & Omega. It adopted Open Governance for Kubernetes by starting the Cloud Native Computing Foundation (CNCF) and giving Kubernetes to that foundation, therefore making it less influenced by Google directly. Many companies such as RedHat, Microsoft, IBM and Amazon quickly joined the foundation. Main entry point for the kubernetes project is at http://kubernetes.io and the source code can be found at https://github.com/kubernetes .","title":"How was Kubernetes created?"},{"location":"generatedContent/kube101/#kubernetes-architecture","text":"At its core, Kubernetes is a data store (etcd). The declarative model is stored in the data store as objects, that means when you say I want 5 instances of a container then that request is stored into the data store. This information change is watched and delegated to Controllers to take action. Controllers then react to the model and attempt to take action to achieve the desired state. The power of Kubernetes is in its simplistic model. As shown, API server is a simple HTTP server handling create/read/update/delete(CRUD) operations on the data store. Then the controller picks up the change you wanted and makes that happen. Controllers are responsible for instantiating the actual resource represented by any Kubernetes resource. These actual resources are what your application needs to allow it to run successfully.","title":"Kubernetes architecture"},{"location":"generatedContent/kube101/#kubernetes-resource-model","text":"Kubernetes Infrastructure defines a resource for every purpose. Each resource is monitored and processed by a controller. When you define your application, it contains a collection of these resources. This collection will then be read by Controllers to build your applications actual backing instances. Some of resources that you may work with are listed below for your reference, for a full list you should go to https://kubernetes.io/docs/concepts/ . In this class we will only use a few of them, like Pod, Deployment, etc. Config Maps holds configuration data for pods to consume. Daemon Sets ensure that each node in the cluster runs this Pod Deployments defines a desired state of a deployment object Events provides lifecycle events on Pods and other deployment objects Endpoints allows a inbound connections to reach the cluster services Ingress is a collection of rules that allow inbound connections to reach the cluster services Jobs creates one or more pods and as they complete successfully the job is marked as completed. Node is a worker machine in Kubernetes Namespaces are multiple virtual clusters backed by the same physical cluster Pods are the smallest deployable units of computing that can be created and managed in Kubernetes Persistent Volumes provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed Replica Sets ensures that a specified number of pod replicas are running at any given time Secrets are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys Service Accounts provides an identity for processes that run in a Pod Services is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. Stateful Sets is the workload API object used to manage stateful applications. and more... Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model.","title":"Kubernetes resource model"},{"location":"generatedContent/kube101/#key-resources","text":"A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. A Pod typically represent a process in your cluster. Pods contain at least one container that runs the job and additionally may have other containers in it called sidecars for monitoring, logging, etc. Essentially a Pod is a group of containers. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application. We will see why separating the application logic and backend database into separate Pods will scale better when we build an application shortly. Services define how to expose your app as a DNS entry to have a stable reference. We use query based selector to choose which pods are supplying that service. The user directly manipulates resources via yaml: kubectl ( create | get | apply | delete ) -f myResource.yaml Kubernetes provides us with a client interface through \u2018kubectl\u2019. Kubectl commands allow you to manage your applications, manage cluster and cluster resources, by modifying the model in the data store.","title":"Key resources"},{"location":"generatedContent/kube101/#kubernetes-application-deployment-workflow","text":"User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods - including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact.","title":"Kubernetes application deployment workflow"},{"location":"generatedContent/kube101/#lab-information","text":"IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Kubernetes Service runs Kubernetes clusters which deliver the following: Powerful tools Intuitive user experience Built-in security and isolation to enable rapid delivery of secure applications Cloud services including cognitive capabilities from Watson Capability to manage dedicated cluster resources for both stateless applications and stateful workloads","title":"Lab information"},{"location":"generatedContent/kube101/#lab-overview","text":"Lab 0 (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine. Lab 1 : This lab walks through creating and deploying a simple \"guestbook\" app written in Go as a net/http Server and accessing it. Lab 2 : Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Kubernetes Service Lab 3 : Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices. Lab 4 : How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention. Lab D : Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs.","title":"Lab overview"},{"location":"generatedContent/kube101/CONTRIBUTING/","text":"Contributing In General \u00b6 Our project welcomes external contributions! If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request to the GitHub repository . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . Before embarking on a more ambitious contribution, please quickly get in touch with us via an issue. We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in the queue for a long time, or cannot be accepted at all! Proposing new features \u00b6 If you would like to implement a new feature, please raise an issue before sending a pull request so the feature can be discussed. This is to avoid you spending your valuable time working on a feature that the project developers are not willing to accept into the code base. Fixing bugs \u00b6 If you would like to fix a bug, please raise an issue before sending a pull request so it can be discussed. If the fix is trivial or non controversial then this is not usually necessary. Merge approval \u00b6 The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected. Note that if your initial push does not pass TravisCI your change will not be approved. For more details, see the MAINTAINERS page.","title":"Contributing In General"},{"location":"generatedContent/kube101/CONTRIBUTING/#contributing-in-general","text":"Our project welcomes external contributions! If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request to the GitHub repository . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . Before embarking on a more ambitious contribution, please quickly get in touch with us via an issue. We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in the queue for a long time, or cannot be accepted at all!","title":"Contributing In General"},{"location":"generatedContent/kube101/CONTRIBUTING/#proposing-new-features","text":"If you would like to implement a new feature, please raise an issue before sending a pull request so the feature can be discussed. This is to avoid you spending your valuable time working on a feature that the project developers are not willing to accept into the code base.","title":"Proposing new features"},{"location":"generatedContent/kube101/CONTRIBUTING/#fixing-bugs","text":"If you would like to fix a bug, please raise an issue before sending a pull request so it can be discussed. If the fix is trivial or non controversial then this is not usually necessary.","title":"Fixing bugs"},{"location":"generatedContent/kube101/CONTRIBUTING/#merge-approval","text":"The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected. Note that if your initial push does not pass TravisCI your change will not be approved. For more details, see the MAINTAINERS page.","title":"Merge approval"},{"location":"generatedContent/kube101/MAINTAINERS/","text":"Maintainers Guide \u00b6 This guide is intended for maintainers - anybody with commit access to one or more Developer Technology repositories. Maintainers \u00b6 Name GitHub email Nathan Fritze nfritze nfritz@us.ibm.com Nathan LeViere nathanleviere njlevier@gmail.com Methodoology \u00b6 A master branch. This branch MUST be releasable at all times. Commits and merges against this branch MUST contain only bugfixes and/or security fixes. Maintenance releases are tagged against master. A develop branch. This branch contains your proposed changes The remainder of this document details how to merge pull requests to the repositories. Merge approval \u00b6 The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from one of the maintainers of each component affected. Reviewing Pull Requests \u00b6 We recommend reviewing pull requests directly within GitHub. This allows a public commentary on changes, providing transparency for all users. When providing feedback be civil, courteous, and kind. Disagreement is fine, so long as the discourse is carried out politely. If we see a record of uncivil or abusive comments, we will revoke your commit privileges and invite you to leave the project. During your review, consider the following points: Does the change have impact? \u00b6 While fixing typos is nice as it adds to the overall quality of the project, merging a typo fix at a time can be a waste of effort. (Merging many typo fixes because somebody reviewed the entire component, however, is useful!) Other examples to be wary of: Changes in variable names. Ask whether or not the change will make understanding the code easier, or if it could simply a personal preference on the part of the author. Essentially: feel free to close issues that do not have impact. Do the changes make sense? \u00b6 If you do not understand what the changes are or what they accomplish, ask the author for clarification. Ask the author to add comments and/or clarify test case names to make the intentions clear. At times, such clarification will reveal that the author may not be using the code correctly, or is unaware of features that accommodate their needs. If you feel this is the case, work up a code sample that would address the issue for them, and feel free to close the issue once they confirm. Is this a new feature? If so \u00b6 Does the issue contain narrative indicating the need for the feature? If not, ask them to provide that information. Since the issue will be linked in the changelog, this will often be a user's first introduction to it. Are new unit tests in place that test all new behaviors introduced? If not, do not merge the feature until they are! Is documentation in place for the new feature? (See the documentation guidelines). If not do not merge the feature until it is! Is the feature necessary for general use cases? Try and keep the scope of any given component narrow. If a proposed feature does not fit that scope, recommend to the user that they maintain the feature on their own, and close the request. You may also recommend that they see if the feature gains traction amongst other users, and suggest they re-submit when they can show such support.","title":"Maintainers Guide"},{"location":"generatedContent/kube101/MAINTAINERS/#maintainers-guide","text":"This guide is intended for maintainers - anybody with commit access to one or more Developer Technology repositories.","title":"Maintainers Guide"},{"location":"generatedContent/kube101/MAINTAINERS/#maintainers","text":"Name GitHub email Nathan Fritze nfritze nfritz@us.ibm.com Nathan LeViere nathanleviere njlevier@gmail.com","title":"Maintainers"},{"location":"generatedContent/kube101/MAINTAINERS/#methodoology","text":"A master branch. This branch MUST be releasable at all times. Commits and merges against this branch MUST contain only bugfixes and/or security fixes. Maintenance releases are tagged against master. A develop branch. This branch contains your proposed changes The remainder of this document details how to merge pull requests to the repositories.","title":"Methodoology"},{"location":"generatedContent/kube101/MAINTAINERS/#merge-approval","text":"The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from one of the maintainers of each component affected.","title":"Merge approval"},{"location":"generatedContent/kube101/MAINTAINERS/#reviewing-pull-requests","text":"We recommend reviewing pull requests directly within GitHub. This allows a public commentary on changes, providing transparency for all users. When providing feedback be civil, courteous, and kind. Disagreement is fine, so long as the discourse is carried out politely. If we see a record of uncivil or abusive comments, we will revoke your commit privileges and invite you to leave the project. During your review, consider the following points:","title":"Reviewing Pull Requests"},{"location":"generatedContent/kube101/MAINTAINERS/#does-the-change-have-impact","text":"While fixing typos is nice as it adds to the overall quality of the project, merging a typo fix at a time can be a waste of effort. (Merging many typo fixes because somebody reviewed the entire component, however, is useful!) Other examples to be wary of: Changes in variable names. Ask whether or not the change will make understanding the code easier, or if it could simply a personal preference on the part of the author. Essentially: feel free to close issues that do not have impact.","title":"Does the change have impact?"},{"location":"generatedContent/kube101/MAINTAINERS/#do-the-changes-make-sense","text":"If you do not understand what the changes are or what they accomplish, ask the author for clarification. Ask the author to add comments and/or clarify test case names to make the intentions clear. At times, such clarification will reveal that the author may not be using the code correctly, or is unaware of features that accommodate their needs. If you feel this is the case, work up a code sample that would address the issue for them, and feel free to close the issue once they confirm.","title":"Do the changes make sense?"},{"location":"generatedContent/kube101/MAINTAINERS/#is-this-a-new-feature-if-so","text":"Does the issue contain narrative indicating the need for the feature? If not, ask them to provide that information. Since the issue will be linked in the changelog, this will often be a user's first introduction to it. Are new unit tests in place that test all new behaviors introduced? If not, do not merge the feature until they are! Is documentation in place for the new feature? (See the documentation guidelines). If not do not merge the feature until it is! Is the feature necessary for general use cases? Try and keep the scope of any given component narrow. If a proposed feature does not fit that scope, recommend to the user that they maintain the feature on their own, and close the request. You may also recommend that they see if the feature gains traction amongst other users, and suggest they re-submit when they can show such support.","title":"Is this a new feature? If so"},{"location":"generatedContent/kube101/SUMMARY/","text":"Summary \u00b6 Getting Started \u00b6 Lab 0: Get the IBM Cloud Container Service Labs \u00b6 Lab 1. Set up and deploy your first application Lab 2: Scale and Update Deployments Lab 3: Scale and update apps natively, building multi-tier applications Resources \u00b6 IBM Developer","title":"Summary"},{"location":"generatedContent/kube101/SUMMARY/#summary","text":"","title":"Summary"},{"location":"generatedContent/kube101/SUMMARY/#getting-started","text":"Lab 0: Get the IBM Cloud Container Service","title":"Getting Started"},{"location":"generatedContent/kube101/SUMMARY/#labs","text":"Lab 1. Set up and deploy your first application Lab 2: Scale and Update Deployments Lab 3: Scale and update apps natively, building multi-tier applications","title":"Labs"},{"location":"generatedContent/kube101/SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"generatedContent/kube101/Lab0/","text":"Lab 0. Access a Kubernetes cluster \u00b6 Set up your kubernetes environment \u00b6 For the hands-on labs in this tutorial repository, you will need a kubernetes cluster. One option for creating a cluster is to make use of the Kubernetes as-a-service from the IBM Cloud Kubernetes Service as outlined below. Use the IBM Cloud Kubernetes Service \u00b6 You will need either a paid IBM Cloud account or an IBM Cloud account which is a Trial account (not a Lite account). If you have one of these accounts, use the Getting Started Guide to create your cluster. Use a hosted trial environment \u00b6 There are a few services that are accessible over the Internet for temporary use. As these are free services, they can sometimes experience periods of limited availablity/quality. On the other hand, they can be a quick way to get started! Kubernetes playground on Katacoda This environment starts with a master and worker node pre-configured. You can run the steps from Labs 1 and onward from the master node. Play with Kubernetes After signing in with your github or docker hub id, click on Start , then Add New Instance and follow steps shown in terminal to spin up the cluster and add workers. Set up on your own workstation \u00b6 If you would like to configure kubernetes to run on your local workstation for non-production, learning use, there are several options. Minikube This solution requires the installation of a supported VM provider (KVM, VirtualBox, HyperKit, Hyper-V - depending on platform) Kubernetes in Docker (kind) Runs a kubernetes cluster on Docker containers Docker Desktop (Mac) Docker Desktop (Windows) Docker Desktop includes a kubernetes environment Microk8s Installable kubernetes packaged as an Ubuntu snap image. Install the IBM Cloud command-line interface \u00b6 As a prerequisite for the IBM Cloud Kubernetes Service plug-in, install the IBM Cloud command-line interface . Once installed, you can access IBM Cloud from your command-line with the prefix bx . Log in to the IBM Cloud CLI: ibmcloud login . Enter your IBM Cloud credentials when prompted. Note: If you have a federated ID, use ibmcloud login --sso to log in to the IBM Cloud CLI. Enter your user name, and use the provided URL in your CLI output to retrieve your one-time passcode. You know you have a federated ID when the login fails without the --sso and succeeds with the --sso option. Install the IBM Cloud Kubernetes Service plug-in \u00b6 To create Kubernetes clusters and manage worker nodes, install the IBM Cloud Kubernetes Service plug-in: ibmcloud plugin install container-service -r Bluemix Note: The prefix for running commands by using the IBM Cloud Kubernetes Service plug-in is bx cs . To verify that the plug-in is installed properly, run the following command: ibmcloud plugin list The IBM Cloud Kubernetes Service plug-in is displayed in the results as container-service . Download the Kubernetes CLI \u00b6 To view a local version of the Kubernetes dashboard and to deploy apps into your clusters, you will need to install the Kubernetes CLI that corresponds with your operating system: OS X Linux Windows For Windows users: Install the Kubernetes CLI in the same directory as the IBM Cloud CLI. This setup saves you some filepath changes when you run commands later. For OS X and Linux users: Move the executable file to the /usr/local/bin directory using the command mv /<path_to_file>/kubectl /usr/local/bin/kubectl . Make sure that /usr/local/bin is listed in your PATH system variable. $ echo $PATH /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin Convert the binary file to an executable: chmod +x /usr/local/bin/kubectl Configure Kubectl to point to IBM Cloud Kubernetes Service \u00b6 List the clusters in your account: ibmcloud ks clusters Set an environment variable that will be used in subsequent commands in this lab. export CLUSTER_NAME = <your_cluster_name> Configure kubectl to point to your cluster ibmcloud ks cluster config --cluster $CLUSTER_NAME Validate proper configuration kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m Download the Workshop Source Code \u00b6 Repo guestbook has the application that we'll be deploying. While we're not going to build it we will use the deployment configuration files from that repo. Guestbook application has two versions v1 and v2 which we will use to demonstrate some rollout functionality later. All the configuration files we use are under the directory guestbook/v1. Repo kube101 contains the step by step instructions to run the workshop. git clone https://github.com/IBM/guestbook.git git clone https://github.com/IBM/kube101.git","title":"Lab 0. Access a Kubernetes cluster"},{"location":"generatedContent/kube101/Lab0/#lab-0-access-a-kubernetes-cluster","text":"","title":"Lab 0. Access a Kubernetes cluster"},{"location":"generatedContent/kube101/Lab0/#set-up-your-kubernetes-environment","text":"For the hands-on labs in this tutorial repository, you will need a kubernetes cluster. One option for creating a cluster is to make use of the Kubernetes as-a-service from the IBM Cloud Kubernetes Service as outlined below.","title":"Set up your kubernetes environment"},{"location":"generatedContent/kube101/Lab0/#use-the-ibm-cloud-kubernetes-service","text":"You will need either a paid IBM Cloud account or an IBM Cloud account which is a Trial account (not a Lite account). If you have one of these accounts, use the Getting Started Guide to create your cluster.","title":"Use the IBM Cloud Kubernetes Service"},{"location":"generatedContent/kube101/Lab0/#use-a-hosted-trial-environment","text":"There are a few services that are accessible over the Internet for temporary use. As these are free services, they can sometimes experience periods of limited availablity/quality. On the other hand, they can be a quick way to get started! Kubernetes playground on Katacoda This environment starts with a master and worker node pre-configured. You can run the steps from Labs 1 and onward from the master node. Play with Kubernetes After signing in with your github or docker hub id, click on Start , then Add New Instance and follow steps shown in terminal to spin up the cluster and add workers.","title":"Use a hosted trial environment"},{"location":"generatedContent/kube101/Lab0/#set-up-on-your-own-workstation","text":"If you would like to configure kubernetes to run on your local workstation for non-production, learning use, there are several options. Minikube This solution requires the installation of a supported VM provider (KVM, VirtualBox, HyperKit, Hyper-V - depending on platform) Kubernetes in Docker (kind) Runs a kubernetes cluster on Docker containers Docker Desktop (Mac) Docker Desktop (Windows) Docker Desktop includes a kubernetes environment Microk8s Installable kubernetes packaged as an Ubuntu snap image.","title":"Set up on your own workstation"},{"location":"generatedContent/kube101/Lab0/#install-the-ibm-cloud-command-line-interface","text":"As a prerequisite for the IBM Cloud Kubernetes Service plug-in, install the IBM Cloud command-line interface . Once installed, you can access IBM Cloud from your command-line with the prefix bx . Log in to the IBM Cloud CLI: ibmcloud login . Enter your IBM Cloud credentials when prompted. Note: If you have a federated ID, use ibmcloud login --sso to log in to the IBM Cloud CLI. Enter your user name, and use the provided URL in your CLI output to retrieve your one-time passcode. You know you have a federated ID when the login fails without the --sso and succeeds with the --sso option.","title":"Install the IBM Cloud command-line interface"},{"location":"generatedContent/kube101/Lab0/#install-the-ibm-cloud-kubernetes-service-plug-in","text":"To create Kubernetes clusters and manage worker nodes, install the IBM Cloud Kubernetes Service plug-in: ibmcloud plugin install container-service -r Bluemix Note: The prefix for running commands by using the IBM Cloud Kubernetes Service plug-in is bx cs . To verify that the plug-in is installed properly, run the following command: ibmcloud plugin list The IBM Cloud Kubernetes Service plug-in is displayed in the results as container-service .","title":"Install the IBM Cloud Kubernetes Service plug-in"},{"location":"generatedContent/kube101/Lab0/#download-the-kubernetes-cli","text":"To view a local version of the Kubernetes dashboard and to deploy apps into your clusters, you will need to install the Kubernetes CLI that corresponds with your operating system: OS X Linux Windows For Windows users: Install the Kubernetes CLI in the same directory as the IBM Cloud CLI. This setup saves you some filepath changes when you run commands later. For OS X and Linux users: Move the executable file to the /usr/local/bin directory using the command mv /<path_to_file>/kubectl /usr/local/bin/kubectl . Make sure that /usr/local/bin is listed in your PATH system variable. $ echo $PATH /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin Convert the binary file to an executable: chmod +x /usr/local/bin/kubectl","title":"Download the Kubernetes CLI"},{"location":"generatedContent/kube101/Lab0/#configure-kubectl-to-point-to-ibm-cloud-kubernetes-service","text":"List the clusters in your account: ibmcloud ks clusters Set an environment variable that will be used in subsequent commands in this lab. export CLUSTER_NAME = <your_cluster_name> Configure kubectl to point to your cluster ibmcloud ks cluster config --cluster $CLUSTER_NAME Validate proper configuration kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m","title":"Configure Kubectl to point to IBM Cloud Kubernetes Service"},{"location":"generatedContent/kube101/Lab0/#download-the-workshop-source-code","text":"Repo guestbook has the application that we'll be deploying. While we're not going to build it we will use the deployment configuration files from that repo. Guestbook application has two versions v1 and v2 which we will use to demonstrate some rollout functionality later. All the configuration files we use are under the directory guestbook/v1. Repo kube101 contains the step by step instructions to run the workshop. git clone https://github.com/IBM/guestbook.git git clone https://github.com/IBM/kube101.git","title":"Download the Workshop Source Code"},{"location":"generatedContent/kube101/Lab1/","text":"Lab 1. Deploy your first application \u00b6 Learn how to deploy an application to a Kubernetes cluster hosted within the IBM Container Service. 0. Prerequisites \u00b6 Make sure you satisfy the prerequisites as outlined in Lab 0 $ ibmcloud cs cluster-create --name <name-of-cluster> If the above command doesn't work, please try the command below\uff1a $ ibmcloud cs cluster create classic --name <name-of-cluseter> Once the cluster is provisioned, the kubernetes client CLI kubectl needs to be configured to talk to the provisioned cluster. Run $ ibmcloud cs cluster-config <name-of-cluster> , and set the KUBECONFIG environment variable based on the output of the command. This will make your kubectl client point to your new Kubernetes cluster. Once your client is configured, you are ready to deploy your first application, guestbook . 1. Deploy the guestbook application \u00b6 In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . Start by running guestbook : kubectl create deployment guestbook --image = ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use $ kubectl get pods . You should see output similar to the following: kubectl get pods Eventually, the status should show up as Running . $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1 /1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a service so we can access it through the IP of the worker nodes. The guestbook application listens on port 3000. Run: kubectl expose deployment guestbook --type = \"NodePort\" --port = 3000 To find the port used on that worker node, examine your new service: $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook NodePort 10 .10.10.253 <none> 3000 :31208/TCP 1m We can see that our <nodeport> is 31208 . We can see in the output the port mapping from 3000 inside the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Get the workers for your cluster and note one (any one) of the public IPs listed on the <public-IP> line. Replace $CLUSTER_NAME with your cluster name unless you have this environment variable set. $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .185.199.3 Ready master,worker 63d v1.16.2+283af84 10 .185.199.3 169 .59.228.215 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 10 .185.199.6 Ready master,worker 63d v1.16.2+283af84 10 .185.199.6 169 .47.78.51 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 We can see that our <public-IP> is 173.193.99.136 . Now that you have both the address and the port, you can now access the application in the web browser at <public-IP>:<nodeport> . In the example case this is 173.193.99.136:31208 . Congratulations, you've now deployed an application to Kubernetes! When you're all done, continue to the next lab of this course .","title":"Lab 1. Deploy your first application"},{"location":"generatedContent/kube101/Lab1/#lab-1-deploy-your-first-application","text":"Learn how to deploy an application to a Kubernetes cluster hosted within the IBM Container Service.","title":"Lab 1. Deploy your first application"},{"location":"generatedContent/kube101/Lab1/#0-prerequisites","text":"Make sure you satisfy the prerequisites as outlined in Lab 0 $ ibmcloud cs cluster-create --name <name-of-cluster> If the above command doesn't work, please try the command below\uff1a $ ibmcloud cs cluster create classic --name <name-of-cluseter> Once the cluster is provisioned, the kubernetes client CLI kubectl needs to be configured to talk to the provisioned cluster. Run $ ibmcloud cs cluster-config <name-of-cluster> , and set the KUBECONFIG environment variable based on the output of the command. This will make your kubectl client point to your new Kubernetes cluster. Once your client is configured, you are ready to deploy your first application, guestbook .","title":"0. Prerequisites"},{"location":"generatedContent/kube101/Lab1/#1-deploy-the-guestbook-application","text":"In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . Start by running guestbook : kubectl create deployment guestbook --image = ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use $ kubectl get pods . You should see output similar to the following: kubectl get pods Eventually, the status should show up as Running . $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1 /1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a service so we can access it through the IP of the worker nodes. The guestbook application listens on port 3000. Run: kubectl expose deployment guestbook --type = \"NodePort\" --port = 3000 To find the port used on that worker node, examine your new service: $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook NodePort 10 .10.10.253 <none> 3000 :31208/TCP 1m We can see that our <nodeport> is 31208 . We can see in the output the port mapping from 3000 inside the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Get the workers for your cluster and note one (any one) of the public IPs listed on the <public-IP> line. Replace $CLUSTER_NAME with your cluster name unless you have this environment variable set. $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .185.199.3 Ready master,worker 63d v1.16.2+283af84 10 .185.199.3 169 .59.228.215 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 10 .185.199.6 Ready master,worker 63d v1.16.2+283af84 10 .185.199.6 169 .47.78.51 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 We can see that our <public-IP> is 173.193.99.136 . Now that you have both the address and the port, you can now access the application in the web browser at <public-IP>:<nodeport> . In the example case this is 173.193.99.136:31208 . Congratulations, you've now deployed an application to Kubernetes! When you're all done, continue to the next lab of this course .","title":"1. Deploy the guestbook application"},{"location":"generatedContent/kube101/Lab1/script/script/","text":"Pod \u00b6 In Kubernetes, a group of one or more containers is called a pod. Containers in a pod are deployed together, and are started, stopped, and replicated as a group. The simplest pod definition describes the deployment of a single container. For example, an nginx web server pod might be defined as such: apiVersion : v1 kind : Pod metadata : name : mynginx namespace : default labels : run : nginx spec : containers : - name : mynginx image : nginx:latest ports : - containerPort : 80 Labels \u00b6 In Kubernetes, labels are a system to organize objects into groups. Labels are key-value pairs that are attached to each object. Label selectors can be passed along with a request to the apiserver to retrieve a list of objects which match that label selector. To add a label to a pod, add a labels section under metadata in the pod definition: apiVersion : v1 kind : Pod metadata : labels : run : nginx ... To label a running pod kubectl label pod mynginx type = webserver pod \"mynginx\" labeled To list pods based on labels kubectl get pods -l type = webserver NAME READY STATUS RESTARTS AGE mynginx 1 /1 Running 0 21m Deployments \u00b6 A Deployment provides declarative updates for pods and replicas. You only need to describe the desired state in a Deployment object, and it will change the actual state to the desired state. The Deployment object defines the following details: The elements of a Replication Controller definition The strategy for transitioning between deployments To create a deployment for a nginx webserver, edit the nginx-deploy.yaml file as apiVersion : apps/v1beta1 kind : Deployment metadata : generation : 1 labels : run : nginx name : nginx namespace : default spec : replicas : 3 selector : matchLabels : run : nginx strategy : rollingUpdate : maxSurge : 1 maxUnavailable : 1 type : RollingUpdate template : metadata : labels : run : nginx spec : containers : - image : nginx:latest imagePullPolicy : Always name : nginx ports : - containerPort : 80 protocol : TCP dnsPolicy : ClusterFirst restartPolicy : Always securityContext : {} terminationGracePeriodSeconds : 30 and create the deployment kubectl create -f nginx-deploy.yaml deployment \"nginx\" created The deployment creates the following objects kubectl get all -l run = nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/nginx 3 3 3 3 4m NAME DESIRED CURRENT READY AGE rs/nginx-664452237 3 3 3 4m NAME READY STATUS RESTARTS AGE po/nginx-664452237-h8dh0 1 /1 Running 0 4m po/nginx-664452237-ncsh1 1 /1 Running 0 4m po/nginx-664452237-vts63 1 /1 Running 0 4m services \u00b6 Services Kubernetes pods, as containers, are ephemeral. Replication Controllers create and destroy pods dynamically, e.g. when scaling up or down or when doing rolling updates. While each pod gets its own IP address, even those IP addresses cannot be relied upon to be stable over time. This leads to a problem: if some set of pods provides functionality to other pods inside the Kubernetes cluster, how do those pods find out and keep track of which other? A Kubernetes Service is an abstraction which defines a logical set of pods and a policy by which to access them. The set of pods targeted by a Service is usually determined by a label selector. Kubernetes offers a simple Endpoints API that is updated whenever the set of pods in a service changes. To create a service for our nginx webserver, edit the nginx-service.yaml file apiVersion : v1 kind : Service metadata : name : nginx labels : run : nginx spec : selector : run : nginx ports : - protocol : TCP port : 8000 targetPort : 80 type : ClusterIP Create the service kubectl create -f nginx-service.yaml service \"nginx\" created kubectl get service -l run = nginx NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx 10 .254.60.24 <none> 8000 /TCP 38s Describe the service: kubectl describe service nginx Name: nginx Namespace: default Labels: run = nginx Selector: run = nginx Type: ClusterIP IP: 10 .254.60.24 Port: <unset> 8000 /TCP Endpoints: 172 .30.21.3:80,172.30.4.4:80,172.30.53.4:80 Session Affinity: None No events. The above service is associated to our previous nginx pods. Pay attention to the service selector run=nginx field. It tells Kubernetes that all pods with the label run=nginx are associated to this service, and should have traffic distributed amongst them. In other words, the service provides an abstraction layer, and it is the input point to reach all of the associated pods.","title":"Pod"},{"location":"generatedContent/kube101/Lab1/script/script/#pod","text":"In Kubernetes, a group of one or more containers is called a pod. Containers in a pod are deployed together, and are started, stopped, and replicated as a group. The simplest pod definition describes the deployment of a single container. For example, an nginx web server pod might be defined as such: apiVersion : v1 kind : Pod metadata : name : mynginx namespace : default labels : run : nginx spec : containers : - name : mynginx image : nginx:latest ports : - containerPort : 80","title":"Pod"},{"location":"generatedContent/kube101/Lab1/script/script/#labels","text":"In Kubernetes, labels are a system to organize objects into groups. Labels are key-value pairs that are attached to each object. Label selectors can be passed along with a request to the apiserver to retrieve a list of objects which match that label selector. To add a label to a pod, add a labels section under metadata in the pod definition: apiVersion : v1 kind : Pod metadata : labels : run : nginx ... To label a running pod kubectl label pod mynginx type = webserver pod \"mynginx\" labeled To list pods based on labels kubectl get pods -l type = webserver NAME READY STATUS RESTARTS AGE mynginx 1 /1 Running 0 21m","title":"Labels"},{"location":"generatedContent/kube101/Lab1/script/script/#deployments","text":"A Deployment provides declarative updates for pods and replicas. You only need to describe the desired state in a Deployment object, and it will change the actual state to the desired state. The Deployment object defines the following details: The elements of a Replication Controller definition The strategy for transitioning between deployments To create a deployment for a nginx webserver, edit the nginx-deploy.yaml file as apiVersion : apps/v1beta1 kind : Deployment metadata : generation : 1 labels : run : nginx name : nginx namespace : default spec : replicas : 3 selector : matchLabels : run : nginx strategy : rollingUpdate : maxSurge : 1 maxUnavailable : 1 type : RollingUpdate template : metadata : labels : run : nginx spec : containers : - image : nginx:latest imagePullPolicy : Always name : nginx ports : - containerPort : 80 protocol : TCP dnsPolicy : ClusterFirst restartPolicy : Always securityContext : {} terminationGracePeriodSeconds : 30 and create the deployment kubectl create -f nginx-deploy.yaml deployment \"nginx\" created The deployment creates the following objects kubectl get all -l run = nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/nginx 3 3 3 3 4m NAME DESIRED CURRENT READY AGE rs/nginx-664452237 3 3 3 4m NAME READY STATUS RESTARTS AGE po/nginx-664452237-h8dh0 1 /1 Running 0 4m po/nginx-664452237-ncsh1 1 /1 Running 0 4m po/nginx-664452237-vts63 1 /1 Running 0 4m","title":"Deployments"},{"location":"generatedContent/kube101/Lab1/script/script/#services","text":"Services Kubernetes pods, as containers, are ephemeral. Replication Controllers create and destroy pods dynamically, e.g. when scaling up or down or when doing rolling updates. While each pod gets its own IP address, even those IP addresses cannot be relied upon to be stable over time. This leads to a problem: if some set of pods provides functionality to other pods inside the Kubernetes cluster, how do those pods find out and keep track of which other? A Kubernetes Service is an abstraction which defines a logical set of pods and a policy by which to access them. The set of pods targeted by a Service is usually determined by a label selector. Kubernetes offers a simple Endpoints API that is updated whenever the set of pods in a service changes. To create a service for our nginx webserver, edit the nginx-service.yaml file apiVersion : v1 kind : Service metadata : name : nginx labels : run : nginx spec : selector : run : nginx ports : - protocol : TCP port : 8000 targetPort : 80 type : ClusterIP Create the service kubectl create -f nginx-service.yaml service \"nginx\" created kubectl get service -l run = nginx NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx 10 .254.60.24 <none> 8000 /TCP 38s Describe the service: kubectl describe service nginx Name: nginx Namespace: default Labels: run = nginx Selector: run = nginx Type: ClusterIP IP: 10 .254.60.24 Port: <unset> 8000 /TCP Endpoints: 172 .30.21.3:80,172.30.4.4:80,172.30.53.4:80 Session Affinity: None No events. The above service is associated to our previous nginx pods. Pay attention to the service selector run=nginx field. It tells Kubernetes that all pods with the label run=nginx are associated to this service, and should have traffic distributed amongst them. In other words, the service provides an abstraction layer, and it is the input point to reach all of the associated pods.","title":"services"},{"location":"generatedContent/kube101/Lab2/","text":"Lab 2: Scale and Update Deployments \u00b6 In this lab, you'll learn how to update the number of instances a deployment has and how to safely roll out an update of your application on Kubernetes. For this lab, you need a running deployment of the guestbook application from the previous lab. If you need to create it, run: kubectl create deployment guestbook --image = ibmcom/guestbook:v1 1. Scale apps with replicas \u00b6 A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances: kubectl scale --replicas = 10 deployment guestbook Kubernetes will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: kubectl rollout status deployment guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: kubectl get pods You should see output listing 10 replicas of your deployment: $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1 /1 Running 0 1d guestbook-562211614-1zqn4 1 /1 Running 0 2m guestbook-562211614-5htdz 1 /1 Running 0 2m guestbook-562211614-6h04h 1 /1 Running 0 2m guestbook-562211614-ds9hb 1 /1 Running 0 2m guestbook-562211614-nb5qp 1 /1 Running 0 2m guestbook-562211614-vtfp2 1 /1 Running 0 2m guestbook-562211614-vz5qw 1 /1 Running 0 2m guestbook-562211614-zksw3 1 /1 Running 0 2m guestbook-562211614-zsp0j 1 /1 Running 0 2m Tip: Another way to improve availability is to add clusters and regions to your deployment, as shown in the following diagram: 2. Update and roll back apps \u00b6 Kubernetes allows you to do rolling upgrade of your application to a new container image. This allows you to easily update the running image and also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade we'll use the image with the v2 tag. To update and roll back: Using kubectl , you can now update your deployment to use the v2 image. kubectl allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. kubectl set image deployment/guestbook guestbook = ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) To check the status of the rollout, run: kubectl rollout status deployment/guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"public-ip\" use the following commands. Replace $CLUSTER_NAME with the name of your cluster if the environment variable is not set.: kubectl describe service guestbook and kubectl get nodes -o wide To verify that you're running \"v2\" of guestbook, look at the title of the page, it should now be Guestbook - v2 . If you are using a browser, make sure you force refresh (invalidating your cache). If you want to undo your latest rollout, use: kubectl rollout undo deployment guestbook You can then use this command to see the status: kubectl rollout status deployment/guestbook When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: $ kubectl get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results: To remove the deployment, use kubectl delete deployment guestbook To remove the service, use: kubectl delete service guestbook Congratulations! You deployed the second version of the app. Lab 2 is now complete. Continue to the next lab of this course .","title":"Lab 2. Scale and update deployments"},{"location":"generatedContent/kube101/Lab2/#lab-2-scale-and-update-deployments","text":"In this lab, you'll learn how to update the number of instances a deployment has and how to safely roll out an update of your application on Kubernetes. For this lab, you need a running deployment of the guestbook application from the previous lab. If you need to create it, run: kubectl create deployment guestbook --image = ibmcom/guestbook:v1","title":"Lab 2: Scale and Update Deployments"},{"location":"generatedContent/kube101/Lab2/#1-scale-apps-with-replicas","text":"A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances: kubectl scale --replicas = 10 deployment guestbook Kubernetes will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: kubectl rollout status deployment guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: kubectl get pods You should see output listing 10 replicas of your deployment: $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1 /1 Running 0 1d guestbook-562211614-1zqn4 1 /1 Running 0 2m guestbook-562211614-5htdz 1 /1 Running 0 2m guestbook-562211614-6h04h 1 /1 Running 0 2m guestbook-562211614-ds9hb 1 /1 Running 0 2m guestbook-562211614-nb5qp 1 /1 Running 0 2m guestbook-562211614-vtfp2 1 /1 Running 0 2m guestbook-562211614-vz5qw 1 /1 Running 0 2m guestbook-562211614-zksw3 1 /1 Running 0 2m guestbook-562211614-zsp0j 1 /1 Running 0 2m Tip: Another way to improve availability is to add clusters and regions to your deployment, as shown in the following diagram:","title":"1. Scale apps with replicas"},{"location":"generatedContent/kube101/Lab2/#2-update-and-roll-back-apps","text":"Kubernetes allows you to do rolling upgrade of your application to a new container image. This allows you to easily update the running image and also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade we'll use the image with the v2 tag. To update and roll back: Using kubectl , you can now update your deployment to use the v2 image. kubectl allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. kubectl set image deployment/guestbook guestbook = ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) To check the status of the rollout, run: kubectl rollout status deployment/guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"public-ip\" use the following commands. Replace $CLUSTER_NAME with the name of your cluster if the environment variable is not set.: kubectl describe service guestbook and kubectl get nodes -o wide To verify that you're running \"v2\" of guestbook, look at the title of the page, it should now be Guestbook - v2 . If you are using a browser, make sure you force refresh (invalidating your cache). If you want to undo your latest rollout, use: kubectl rollout undo deployment guestbook You can then use this command to see the status: kubectl rollout status deployment/guestbook When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: $ kubectl get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results: To remove the deployment, use kubectl delete deployment guestbook To remove the service, use: kubectl delete service guestbook Congratulations! You deployed the second version of the app. Lab 2 is now complete. Continue to the next lab of this course .","title":"2. Update and roll back apps"},{"location":"generatedContent/kube101/Lab3/","text":"Lab 3: Scale and update apps natively, building multi-tier applications \u00b6 In this lab you'll learn how to deploy the same guestbook application we deployed in the previous labs, however, instead of using the kubectl command line helper functions we'll be deploying the application using configuration files. The configuration file mechanism allows you to have more fine-grained control over all of resources being created within the Kubernetes cluster. Before we work with the application we need to clone a github repo: git clone https://github.com/IBM/guestbook.git This repo contains multiple versions of the guestbook application as well as the configuration files we'll use to deploy the pieces of the application. Change directory by running the command cd guestbook/v1 You will find all the configurations files for this exercise in this directory. 1. Scale apps natively \u00b6 Kubernetes can deploy an individual pod to run an application but when you need to scale it to handle a large number of requests a Deployment is the resource you want to use. A Deployment manages a collection of similar pods. When you ask for a specific number of replicas the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times. Every Kubernetes object we create should provide two nested object fields that govern the object\u2019s configuration: the object spec and the object status . Object spec defines the desired state, and object status contains Kubernetes system provided information about the actual state of the resource. As described before, Kubernetes will attempt to reconcile your desired state with the actual state of the system. For Object that we create we need to provide the apiVersion you are using to create the object, kind of the object we are creating and the metadata about the object such as a name , set of labels and optionally namespace that this object should belong. Consider the following deployment configuration for guestbook application guestbook-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook version : \"1.0\" spec : replicas : 3 selector : matchLabels : app : guestbook template : metadata : labels : app : guestbook version : \"1.0\" spec : containers : - name : guestbook image : ibmcom/guestbook:v1 ports : - name : http-server containerPort : 3000 The above configuration file create a deployment object named 'guestbook' with a pod containing a single container running the image ibmcom/guestbook:v1 . Also the configuration specifies replicas set to 3 and Kubernetes tries to make sure that at least three active pods are running at all times. Create guestbook deployment To create a Deployment using this configuration file we use the following command: kubectl create -f guestbook-deployment.yaml List the pod with label app=guestbook We can then list the pods it created by listing all pods that have a label of \"app\" with a value of \"guestbook\". This matches the labels defined above in the yaml file in the spec.template.metadata.labels section. kubectl get pods -l app = guestbook When you change the number of replicas in the configuration, Kubernetes will try to add, or remove, pods from the system to match your request. To can make these modifications by using the following command: kubectl edit deployment guestbook-v1 This will retrieve the latest configuration for the Deployment from the Kubernetes server and then load it into an editor for you. You'll notice that there are a lot more fields in this version than the original yaml file we used. This is because it contains all of the properties about the Deployment that Kubernetes knows about, not just the ones we chose to specify when we create it. Also notice that it now contains the status section mentioned previously. To exit the vi editor, type :q! , of if you made changes that you want to see reflected, save them using :wq . You can also edit the deployment file we used to create the Deployment to make changes. You should use the following command to make the change effective when you edit the deployment locally. kubectl apply -f guestbook-deployment.yaml This will ask Kubernetes to \"diff\" our yaml file with the current state of the Deployment and apply just those changes. We can now define a Service object to expose the deployment to external clients. guestbook-service.yaml apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : ports : - port : 3000 targetPort : http-server selector : app : guestbook type : LoadBalancer The above configuration creates a Service resource named guestbook. A Service can be used to create a network path for incoming traffic to your running application. In this case, we are setting up a route from port 3000 on the cluster to the \"http-server\" port on our app, which is port 3000 per the Deployment container spec. Let us now create the guestbook service using the same type of command we used when we created the Deployment: kubectl create -f guestbook-service.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> Remember, to get the nodeport and public-ip use the following commands, replacing $CLUSTER_NAME with the name of your cluster if the environment variable is not already set. kubectl describe service guestbook and kubectl get nodes -o wide 2. Connect to a back-end service \u00b6 If you look at the guestbook source code, under the guestbook/v1/guestbook directory, you'll notice that it is written to support a variety of data stores. By default it will keep the log of guestbook entries in memory. That's ok for testing purposes, but as you get into a more \"real\" environment where you scale your application that model will not work because based on which instance of the application the user is routed to they'll see very different results. To solve this we need to have all instances of our app share the same data store - in this case we're going to use a redis database that we deploy to our cluster. This instance of redis will be defined in a similar manner to the guestbook. redis-master-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-master labels : app : redis role : master spec : replicas : 1 selector : matchLabels : app : redis role : master template : metadata : labels : app : redis role : master spec : containers : - name : redis-master image : redis:3.2.9 ports : - name : redis-server containerPort : 6379 This yaml creates a redis database in a Deployment named 'redis-master'. It will create a single instance, with replicas set to 1, and the guestbook app instances will connect to it to persist data, as well as read the persisted data back. The image running in the container is 'redis:3.2.9' and exposes the standard redis port 6379. Create a redis Deployment, like we did for guestbook: kubectl create -f redis-master-deployment.yaml Check to see that redis server pod is running: $ kubectl get pods -lapp = redis,role = master NAME READY STATUS RESTARTS AGE redis-master-q9zg7 1 /1 Running 0 2d Let us test the redis standalone. Replace the pod name redis-master-q9zg7 with the name of your pod. kubectl exec -it redis-master-q9zg7 redis-cli The kubectl exec command will start a secondary process in the specified container. In this case we're asking for the \"redis-cli\" command to be executed in the container named \"redis-master-q9zg7\". When this process ends the \"kubectl exec\" command will also exit but the other processes in the container will not be impacted. Once in the container we can use the \"redis-cli\" command to make sure the redis database is running properly, or to configure it if needed. redis-cli> ping PONG redis-cli> exit Now we need to expose the redis-master Deployment as a Service so that the guestbook application can connect to it through DNS lookup. redis-master-service.yaml apiVersion : v1 kind : Service metadata : name : redis-master labels : app : redis role : master spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : master This creates a Service object named 'redis-master' and configures it to target port 6379 on the pods selected by the selectors \"app=redis\" and \"role=master\". Create the service to access redis master: kubectl create -f redis-master-service.yaml Restart guestbook so that it will find the redis service to use database: kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you already have the app open in another window. You can see now that if you open up multiple browsers and refresh the page to access the different copies of guestbook that they all have a consistent state. All instances write to the same backing persistent storage, and all instances read from that storage to display the guestbook entries that have been stored. We have our simple 3-tier application running but we need to scale the application if traffic increases. Our main bottleneck is that we only have one database server to process each request coming though guestbook. One simple solution is to separate the reads and write such that they go to different databases that are replicated properly to achieve data consistency. Create a deployment named 'redis-slave' that can talk to redis database to manage data reads. In order to scale the database we use the pattern where we can scale the reads using redis slave deployment which can run several instances to read. Redis slave deployments is configured to run two replicas. redis-slave-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-slave labels : app : redis role : slave spec : replicas : 2 selector : matchLabels : app : redis role : slave template : metadata : labels : app : redis role : slave spec : containers : - name : redis-slave image : ibmcom/guestbook-redis-slave:v2 ports : - name : redis-server containerPort : 6379 Create the pod running redis slave deployment. kubectl create -f redis-slave-deployment.yaml Check if all the slave replicas are running $ kubectl get pods -lapp = redis,role = slave NAME READY STATUS RESTARTS AGE redis-slave-kd7vx 1 /1 Running 0 2d redis-slave-wwcxw 1 /1 Running 0 2d And then go into one of those pods and look at the database to see that everything looks right. Replace the pod name redis-slave-kd7vx with your own pod name. If you get the back (empty list or set) when you print the keys, go to the guestbook application and add an entry! ```shell $ kubectl exec -it redis-slave-kd7vx redis-cli 127.0.0.1:6379> keys * 1) \"guestbook\" 127.0.0.1:6379> lrange guestbook 0 10 1) \"hello world\" 2) \"welcome to the Kube workshop\" 127.0.0.1:6379> exit Deploy redis slave service so we can access it by DNS name. Once redeployed, the application will send \"read\" operations to the `redis-slave` pods while \"write\" operations will go to the `redis-master` pods. > **redis-slave-service.yaml** ```yaml apiVersion: v1 kind: Service metadata: name: redis-slave labels: app: redis role: slave spec: ports: - port: 6379 targetPort: redis-server selector: app: redis role: slave Create the service to access redis slaves. kubectl create -f redis-slave-service.yaml Restart guestbook so that it will find the slave service to read from. kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you have the app open in another window. That's the end of the lab. Now let's clean-up our environment: kubectl delete -f guestbook-deployment.yaml kubectl delete -f guestbook-service.yaml kubectl delete -f redis-slave-service.yaml kubectl delete -f redis-slave-deployment.yaml kubectl delete -f redis-master-service.yaml kubectl delete -f redis-master-deployment.yaml","title":"Lab 3. Build multi-tier applications"},{"location":"generatedContent/kube101/Lab3/#lab-3-scale-and-update-apps-natively-building-multi-tier-applications","text":"In this lab you'll learn how to deploy the same guestbook application we deployed in the previous labs, however, instead of using the kubectl command line helper functions we'll be deploying the application using configuration files. The configuration file mechanism allows you to have more fine-grained control over all of resources being created within the Kubernetes cluster. Before we work with the application we need to clone a github repo: git clone https://github.com/IBM/guestbook.git This repo contains multiple versions of the guestbook application as well as the configuration files we'll use to deploy the pieces of the application. Change directory by running the command cd guestbook/v1 You will find all the configurations files for this exercise in this directory.","title":"Lab 3: Scale and update apps natively, building multi-tier applications"},{"location":"generatedContent/kube101/Lab3/#1-scale-apps-natively","text":"Kubernetes can deploy an individual pod to run an application but when you need to scale it to handle a large number of requests a Deployment is the resource you want to use. A Deployment manages a collection of similar pods. When you ask for a specific number of replicas the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times. Every Kubernetes object we create should provide two nested object fields that govern the object\u2019s configuration: the object spec and the object status . Object spec defines the desired state, and object status contains Kubernetes system provided information about the actual state of the resource. As described before, Kubernetes will attempt to reconcile your desired state with the actual state of the system. For Object that we create we need to provide the apiVersion you are using to create the object, kind of the object we are creating and the metadata about the object such as a name , set of labels and optionally namespace that this object should belong. Consider the following deployment configuration for guestbook application guestbook-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook version : \"1.0\" spec : replicas : 3 selector : matchLabels : app : guestbook template : metadata : labels : app : guestbook version : \"1.0\" spec : containers : - name : guestbook image : ibmcom/guestbook:v1 ports : - name : http-server containerPort : 3000 The above configuration file create a deployment object named 'guestbook' with a pod containing a single container running the image ibmcom/guestbook:v1 . Also the configuration specifies replicas set to 3 and Kubernetes tries to make sure that at least three active pods are running at all times. Create guestbook deployment To create a Deployment using this configuration file we use the following command: kubectl create -f guestbook-deployment.yaml List the pod with label app=guestbook We can then list the pods it created by listing all pods that have a label of \"app\" with a value of \"guestbook\". This matches the labels defined above in the yaml file in the spec.template.metadata.labels section. kubectl get pods -l app = guestbook When you change the number of replicas in the configuration, Kubernetes will try to add, or remove, pods from the system to match your request. To can make these modifications by using the following command: kubectl edit deployment guestbook-v1 This will retrieve the latest configuration for the Deployment from the Kubernetes server and then load it into an editor for you. You'll notice that there are a lot more fields in this version than the original yaml file we used. This is because it contains all of the properties about the Deployment that Kubernetes knows about, not just the ones we chose to specify when we create it. Also notice that it now contains the status section mentioned previously. To exit the vi editor, type :q! , of if you made changes that you want to see reflected, save them using :wq . You can also edit the deployment file we used to create the Deployment to make changes. You should use the following command to make the change effective when you edit the deployment locally. kubectl apply -f guestbook-deployment.yaml This will ask Kubernetes to \"diff\" our yaml file with the current state of the Deployment and apply just those changes. We can now define a Service object to expose the deployment to external clients. guestbook-service.yaml apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : ports : - port : 3000 targetPort : http-server selector : app : guestbook type : LoadBalancer The above configuration creates a Service resource named guestbook. A Service can be used to create a network path for incoming traffic to your running application. In this case, we are setting up a route from port 3000 on the cluster to the \"http-server\" port on our app, which is port 3000 per the Deployment container spec. Let us now create the guestbook service using the same type of command we used when we created the Deployment: kubectl create -f guestbook-service.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> Remember, to get the nodeport and public-ip use the following commands, replacing $CLUSTER_NAME with the name of your cluster if the environment variable is not already set. kubectl describe service guestbook and kubectl get nodes -o wide","title":"1. Scale apps natively"},{"location":"generatedContent/kube101/Lab3/#2-connect-to-a-back-end-service","text":"If you look at the guestbook source code, under the guestbook/v1/guestbook directory, you'll notice that it is written to support a variety of data stores. By default it will keep the log of guestbook entries in memory. That's ok for testing purposes, but as you get into a more \"real\" environment where you scale your application that model will not work because based on which instance of the application the user is routed to they'll see very different results. To solve this we need to have all instances of our app share the same data store - in this case we're going to use a redis database that we deploy to our cluster. This instance of redis will be defined in a similar manner to the guestbook. redis-master-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-master labels : app : redis role : master spec : replicas : 1 selector : matchLabels : app : redis role : master template : metadata : labels : app : redis role : master spec : containers : - name : redis-master image : redis:3.2.9 ports : - name : redis-server containerPort : 6379 This yaml creates a redis database in a Deployment named 'redis-master'. It will create a single instance, with replicas set to 1, and the guestbook app instances will connect to it to persist data, as well as read the persisted data back. The image running in the container is 'redis:3.2.9' and exposes the standard redis port 6379. Create a redis Deployment, like we did for guestbook: kubectl create -f redis-master-deployment.yaml Check to see that redis server pod is running: $ kubectl get pods -lapp = redis,role = master NAME READY STATUS RESTARTS AGE redis-master-q9zg7 1 /1 Running 0 2d Let us test the redis standalone. Replace the pod name redis-master-q9zg7 with the name of your pod. kubectl exec -it redis-master-q9zg7 redis-cli The kubectl exec command will start a secondary process in the specified container. In this case we're asking for the \"redis-cli\" command to be executed in the container named \"redis-master-q9zg7\". When this process ends the \"kubectl exec\" command will also exit but the other processes in the container will not be impacted. Once in the container we can use the \"redis-cli\" command to make sure the redis database is running properly, or to configure it if needed. redis-cli> ping PONG redis-cli> exit Now we need to expose the redis-master Deployment as a Service so that the guestbook application can connect to it through DNS lookup. redis-master-service.yaml apiVersion : v1 kind : Service metadata : name : redis-master labels : app : redis role : master spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : master This creates a Service object named 'redis-master' and configures it to target port 6379 on the pods selected by the selectors \"app=redis\" and \"role=master\". Create the service to access redis master: kubectl create -f redis-master-service.yaml Restart guestbook so that it will find the redis service to use database: kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you already have the app open in another window. You can see now that if you open up multiple browsers and refresh the page to access the different copies of guestbook that they all have a consistent state. All instances write to the same backing persistent storage, and all instances read from that storage to display the guestbook entries that have been stored. We have our simple 3-tier application running but we need to scale the application if traffic increases. Our main bottleneck is that we only have one database server to process each request coming though guestbook. One simple solution is to separate the reads and write such that they go to different databases that are replicated properly to achieve data consistency. Create a deployment named 'redis-slave' that can talk to redis database to manage data reads. In order to scale the database we use the pattern where we can scale the reads using redis slave deployment which can run several instances to read. Redis slave deployments is configured to run two replicas. redis-slave-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-slave labels : app : redis role : slave spec : replicas : 2 selector : matchLabels : app : redis role : slave template : metadata : labels : app : redis role : slave spec : containers : - name : redis-slave image : ibmcom/guestbook-redis-slave:v2 ports : - name : redis-server containerPort : 6379 Create the pod running redis slave deployment. kubectl create -f redis-slave-deployment.yaml Check if all the slave replicas are running $ kubectl get pods -lapp = redis,role = slave NAME READY STATUS RESTARTS AGE redis-slave-kd7vx 1 /1 Running 0 2d redis-slave-wwcxw 1 /1 Running 0 2d And then go into one of those pods and look at the database to see that everything looks right. Replace the pod name redis-slave-kd7vx with your own pod name. If you get the back (empty list or set) when you print the keys, go to the guestbook application and add an entry! ```shell $ kubectl exec -it redis-slave-kd7vx redis-cli 127.0.0.1:6379> keys * 1) \"guestbook\" 127.0.0.1:6379> lrange guestbook 0 10 1) \"hello world\" 2) \"welcome to the Kube workshop\" 127.0.0.1:6379> exit Deploy redis slave service so we can access it by DNS name. Once redeployed, the application will send \"read\" operations to the `redis-slave` pods while \"write\" operations will go to the `redis-master` pods. > **redis-slave-service.yaml** ```yaml apiVersion: v1 kind: Service metadata: name: redis-slave labels: app: redis role: slave spec: ports: - port: 6379 targetPort: redis-server selector: app: redis role: slave Create the service to access redis slaves. kubectl create -f redis-slave-service.yaml Restart guestbook so that it will find the slave service to read from. kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you have the app open in another window. That's the end of the lab. Now let's clean-up our environment: kubectl delete -f guestbook-deployment.yaml kubectl delete -f guestbook-service.yaml kubectl delete -f redis-slave-service.yaml kubectl delete -f redis-slave-deployment.yaml kubectl delete -f redis-master-service.yaml kubectl delete -f redis-master-deployment.yaml","title":"2. Connect to a back-end service"},{"location":"generatedContent/kube101/Lab4/","text":"UNDER CONSTRUCTION \u00b6 1. Check the health of apps \u00b6 Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers. In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the /healthz returns a 200 response and will fail afterward. Kubernetes will automatically restart the service. Open the healthcheck.yml file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods: Update the details for the image in your private registry namespace: image : \"ibmcom/guestbook:v2\" Note the HTTP liveness probe that checks the health of the container every five seconds. livenessProbe : httpGet : path : /healthz port : 3000 initialDelaySeconds : 5 periodSeconds : 5 In the Service section, note the NodePort . Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072. Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see: kubectl apply -f healthcheck.yml Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower. Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node: ibmcloud cs workers <cluster-name> In a browser, you'll see a success message. If you do not see this text, don't worry. This app is designed to go up and down. For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app. Launch your Kubernetes dashboard: Get your credentials for Kubernetes. kubectl config view -o jsonpath = '{.users[0].user.auth-provider.config.id-token}' Copy the id-token value that is shown in the output. Set the proxy with the default port number. kubectl proxy Output: Starting to serve on 127 .0.0.1:8001 Sign in to the dashboard. Open the following URL in a web browser. http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ In the sign-on page, select the Token authentication method. Then, paste the id-token value that you previously copied into the Token field and click SIGN IN . In the Workloads tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the Pods section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod. Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created. kubectl delete -f healthcheck.yml When you are done exploring the Kubernetes dashboard, in your CLI, enter CTRL+C to exit the proxy command.","title":"***UNDER CONSTRUCTION***"},{"location":"generatedContent/kube101/Lab4/#under-construction","text":"","title":"UNDER CONSTRUCTION"},{"location":"generatedContent/kube101/Lab4/#1-check-the-health-of-apps","text":"Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers. In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the /healthz returns a 200 response and will fail afterward. Kubernetes will automatically restart the service. Open the healthcheck.yml file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods: Update the details for the image in your private registry namespace: image : \"ibmcom/guestbook:v2\" Note the HTTP liveness probe that checks the health of the container every five seconds. livenessProbe : httpGet : path : /healthz port : 3000 initialDelaySeconds : 5 periodSeconds : 5 In the Service section, note the NodePort . Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072. Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see: kubectl apply -f healthcheck.yml Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower. Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node: ibmcloud cs workers <cluster-name> In a browser, you'll see a success message. If you do not see this text, don't worry. This app is designed to go up and down. For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app. Launch your Kubernetes dashboard: Get your credentials for Kubernetes. kubectl config view -o jsonpath = '{.users[0].user.auth-provider.config.id-token}' Copy the id-token value that is shown in the output. Set the proxy with the default port number. kubectl proxy Output: Starting to serve on 127 .0.0.1:8001 Sign in to the dashboard. Open the following URL in a web browser. http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ In the sign-on page, select the Token authentication method. Then, paste the id-token value that you previously copied into the Token field and click SIGN IN . In the Workloads tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the Pods section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod. Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created. kubectl delete -f healthcheck.yml When you are done exploring the Kubernetes dashboard, in your CLI, enter CTRL+C to exit the proxy command.","title":"1. Check the health of apps"},{"location":"generatedContent/kube101/LabD/","text":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes \u00b6 Advanced debugging techniques to reach your pods. Pod Logs \u00b6 You can look at the logs of any of the pods running under your deployments as follows kubectl logs <podname> Remember that if you have multiple containers running in your pod, you have to specify the specific container you want to see logs from. kubectl logs <pod-name> <container-name> This subcommand operates like tail . Including the -f flag will continue to stream the logs live once the current time is reached. kubectl edit and vi \u00b6 By default, on many Linux and macOS systems, you will be dropped into the editor vi . export EDITOR = nano On Windows, a copy of notepad.exe will be opened with the contents of the file. busybox pod \u00b6 For debugging live, this command frequently helps me: kubectl create deployment bb --image busybox --restart = Never -it --rm In the busybox image is a basic shell that contains useful utilities. Utils I often use are nslookup and wget . nslookup is useful for testing DNS resolution in a pod. wget is useful for trying to do network requests. Service Endpoints \u00b6 Endpoint resource can be used to see all the service endpoints. kubectl get endpoints <service> ImagePullPolicy \u00b6 By default Kubernetes will only pull the image on first use. This can be confusing during development when you expect changes to show up. You should be aware of the three ImagePullPolicy s: IfNotPresent - the default, only request the image if not present. Always - always request the image. Never More details on image management may be found here .","title":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes"},{"location":"generatedContent/kube101/LabD/#optional-debugging-lab-tips-and-tricks-for-debugging-applications-in-kubernetes","text":"Advanced debugging techniques to reach your pods.","title":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes"},{"location":"generatedContent/kube101/LabD/#pod-logs","text":"You can look at the logs of any of the pods running under your deployments as follows kubectl logs <podname> Remember that if you have multiple containers running in your pod, you have to specify the specific container you want to see logs from. kubectl logs <pod-name> <container-name> This subcommand operates like tail . Including the -f flag will continue to stream the logs live once the current time is reached.","title":"Pod Logs"},{"location":"generatedContent/kube101/LabD/#kubectl-edit-and-vi","text":"By default, on many Linux and macOS systems, you will be dropped into the editor vi . export EDITOR = nano On Windows, a copy of notepad.exe will be opened with the contents of the file.","title":"kubectl edit and vi"},{"location":"generatedContent/kube101/LabD/#busybox-pod","text":"For debugging live, this command frequently helps me: kubectl create deployment bb --image busybox --restart = Never -it --rm In the busybox image is a basic shell that contains useful utilities. Utils I often use are nslookup and wget . nslookup is useful for testing DNS resolution in a pod. wget is useful for trying to do network requests.","title":"busybox pod"},{"location":"generatedContent/kube101/LabD/#service-endpoints","text":"Endpoint resource can be used to see all the service endpoints. kubectl get endpoints <service>","title":"Service Endpoints"},{"location":"generatedContent/kube101/LabD/#imagepullpolicy","text":"By default Kubernetes will only pull the image on first use. This can be confusing during development when you expect changes to show up. You should be aware of the three ImagePullPolicy s: IfNotPresent - the default, only request the image if not present. Always - always request the image. Never More details on image management may be found here .","title":"ImagePullPolicy"},{"location":"generatedContent/kubernetes-extensions.git/","text":"Kubernetes Extensions \u00b6 Access the web-terminal Login Create a Custom Resource Operators Ready Made Operators Create a Custom Resource and Operator using the Operator SDK Install sdk-operator Create the Operator Cleanup Application CRD Access the web-terminal \u00b6 When running the lab for Kubernetes Extensions, you can make use of a web-terminal. The Dockerfile to use is located in https://github.com/IBMAppModernization/web-terminal, and named Dockerfile-s2i-oc-tekton-operator . To run on localhost as a Docker container, git clone https://github.com/IBMAppModernization/web-terminal.git cd web-terminal docker build --no-cache -t web-terminal:latest -f Dockerfile-s2i-oc-tekton-operator . docker run -d --restart always --name terminal -p 7681:7681 -v $HOME/dev/tmp:/root/dev web-terminal docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 85edc0b0ec27 web-terminal \"ttyd -p 7681 bash\" 17 minutes ago Up 17 minutes 0.0.0.0:7681->7681/tcp terminal The volume mapping will write all files under the working directory to the host directory $HOME/dev/tmp . So suppose my host's user home directory is /Users/remkohdev@us.ibm.com/ . If I open the terminal in the browser, the working directory for the user is /root . Any file that is created under /root is created on the host's directory $HOME/dev/tmp . Similarly if I create a file in $HOME/dev/tmp it is available in the container's /root directory. Open the web-terminal in a browser and go to http://0.0.0.0:7681. If Go, Operator SDK Login \u00b6 export CLUSTERNAME=remkohdev-roks-labs-3n-cluster ibmcloud login Go to the OpenShift web console Copy Login command oc login --token=_12AbcD345kIPDIRg2jYpCuZ-g5SM5Im9irY2tol4Q8 --server=https://c100-e.us-south.containers.cloud.ibm.com:30712 Create a Custom Resource (CR) \u00b6 https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ Custom Resource Definitions (CRD) were added in Kubernetes v1.7 in June 2017. A CRD defines Custom Resources (CR). A CR is an extension of the Kubernetes API that allows you to store your own API Objects and lets the API Server handle the lifecycle of a CR. On their own, CRs simply let you store and retrieve structured data. For instance, our Guestbook application consists of an object Guestbook with attributes GuestbookTitle and GuestbookSubtitle , and a Guestbook handles objectes of type GuestbookMessage with attributes Message , Sender . You have to ask yourself if it makes sense if your objects are added as a Custom Resource to Kubernetes or not. If your API is a Declarative API you can consider adding a CR. Your API has a small number of small objects (resources). The objects define configuration of applications or infrastructure. The objects are updated relatively infrequently. Users often need to read and write the objects. main operations on the objects are CRUD (create, read, update and delete). Transactions between objects are not required. It doesn't immediately make sense to store messages by Guestbook users in Kubernetes, but it might make sense to store meta-data about a Guestbook deployment, for instance the title and subtitle of a Guestbook deployment, assigned resources or replicas. Another benefit of adding a Custom Resource is to view your types in the Kubernetes Dashboard. If you want to deploy a Guestbook instance as a Kubernetes API object and let the Kubernetes API Server handle the lifecycle events of the Guestbook deployment, you can create a Custom Resource Definition (CRD) for the Guestbook object as follows. That way you can deploy multiple Guestbooks with different titles and let each be managed by Kubernetes. cat <<EOF >>guestbook-crd.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.apps.ibm.com spec: group: apps.ibm.com versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: guestbookTitle: type: string guestbookSubtitle: type: string scope: Namespaced names: plural: guestbooks singular: guestbook kind: Guestbook shortNames: - gb EOF You can see that the apiVersion is part of the apiextensions.k8s.io/v1 API Group in Kubernetes, which is the API that enables extensions, and the kind is set to CustomResourceDefinition . The served flag can disable and enable a version. Only 1 version can be flagged as the storage version. The spec.names.kind is used by your resource manifests and should be CamelCased. Create the Custom Resource for the Guestbook witht he command, oc create -f guestbook-crd.yaml When run in the terminal, $ oc create -f guestbook-crd.yaml customresourcedefinition.apiextensions.k8s.io/guestbooks.apps.ibm.com created You have now added a CR to the Kubernetes API, but you have not yet created a deployment of type Guestbook yet. Create a resource specification of type Guestbook named my-guestbook , cat <<EOF >>my-guestbook.yaml apiVersion: \"apps.ibm.com/v1\" kind: Guestbook metadata: name: my-guestbook spec: guestbookTitle: \"The Chemical Wedding of Remko\" guestbookSubtitle: \"First Day of Many\" EOF And to create the my-guestbook resource, run the command oc create -f my-guestbook.yaml When run in the terminal, $ oc create -f my-guestbook.yaml guestbook.apps.ibm.com/my-guestbook created If you list all Kubernetes resources, only the default Kubernetes service is listed. To list your Custom Resources, add the extended type to your command. $ oc get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 5d14h service/openshift ExternalName <none> kubernetes.default.svc.cluster.local <none> 5d14h service/openshift-apiserver ClusterIP 172.21.6.8 <none> =443/TCP 5d14h $ oc get guestbook NAME AGE my-guestbook 8m32s To read the details for the my-guestbook of type Guestbook , describe the instance, $ oc describe guestbook my-guestbook Name: my-guestbook Namespace: default Labels: <none> Annotations: <none> API Version: apps.ibm.com/v1 Kind: Guestbook Metadata: Creation Timestamp: 2020-06-30T20:31:36Z Generation: 1 Resource Version: 1081471 Self Link: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook UID: dcbdcafc-999d-4051-9244-0315093357e7 Spec: Guestbook Subtitle: First Day of Many Guestbook Title: The Chemical Wedding of Remko Events: <none> Or retrieve the resource information by specifying the type, $ oc get Guestbook -o yaml apiVersion: v1 items: - apiVersion: apps.ibm.com/v1 kind: Guestbook metadata: creationTimestamp: \"2020-07-02T04:41:57Z\" generation: 1 name: my-guestbook namespace: default resourceVersion: \"1903244\" selfLink: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook uid: 3f774899-3070-4e00-b74c-a6a14654faeb spec: guestbookSubtitle: First Day of Many guestbookTitle: The Chemical Wedding of Remko kind: List metadata: resourceVersion: \"\" selfLink: \"\" In the OpenShift web console, you can browse to Administration > Custom Resource Definitions and find the Guestbook CRD at /k8s/cluster/customresourcedefinitions/guestbooks.apps.ibm.com . You have now created a new type or Custom Resource (CR) and created an instance of your new type. But just having a new type and a new instance of the type, does not add as much control over the instances yet, we can basically only create and delete a static type with some descriptive meta-data. With a custom controller or Operator you can over-write the methods that are triggered at certain lifecycle events. Operators \u00b6 https://kubernetes.io/docs/concepts/extend-kubernetes/operator/ Operators are clients of the Kubernetes API that act as controllers for a Custom Resource. To write applications that use the Kubernetes REST API, you can use one of the following supported client libraries: - Go , - Python , - Java , - CSharp dotnet , - JavaScript , - Haskell . In addition, there are many community-maintained client libraries . Ready made operators \u00b6 At the OperatorHub.io , you find ready to use operators written by the community. Create a Custom Resource and Operator using the Operator SDK \u00b6 To write your own operator you can use existing tools: - KUDO (Kubernetes Universal Declarative Operator), - kubebuilder , - Metacontroller using custom WebHooks, - the Operator Framework . The Operator SDK provides the following workflow to develop a new Operator: The following workflow is for a new Go operator: Create a new operator project using the SDK Command Line Interface(CLI) Define new resource APIs by adding Custom Resource Definitions(CRD) Define Controllers to watch and reconcile resources Write the reconciling logic for your Controller using the SDK and controller-runtime APIs Use the SDK CLI to build and generate the operator deployment manifests Install sdk-operator \u00b6 For detailed installation instructions go here . To install the Operator SDK in Ubuntu, you need to install the Go tools and the Operator SDK. $ curl -LO https://golang.org/dl/go1.14.4.linux-amd64.tar.gz $ tar -C /usr/local -xzf go1.14.4.linux-amd64.tar.gz $ export PATH=$PATH:/usr/local/go/bin $ curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v0.18.2/operator-sdk-v0.18.2-x86_64-linux-gnu $ chmod +x operator-sdk-v0.18.2-x86_64-linux-gnu $ sudo mkdir -p /usr/local/bin/ $ sudo cp operator-sdk-v0.18.2-x86_64-linux-gnu /usr/local/bin/operator-sdk $ rm operator-sdk-v0.18.2-x86_64-linux-gnu $ go version $ operator-sdk version Create the Operator \u00b6 1. Create a New Project \u00b6 Create a new Operator project, $ export DOCKER_USERNAME=<your-docker-username> $ export OPERATOR_NAME=guestbook-operator $ export OPERATOR_PROJECT=guestbook-project $ export OPERATOR_GROUP=guestbook.remkoh.dev $ export OPERATOR_VERSION=v1 $ export CRD_KIND=Guestbook $ go version $ operator-sdk version $ operator-sdk new $OPERATOR_PROJECT --type go --repo github.com/$DOCKER_USERNAME/$OPERATOR_NAME $ cd $OPERATOR_PROJECT The scaffolding of a new project will create an operator, an api and a controller. 2. Create a new API \u00b6 Add a new API definition for a new Custom Resource under pkg/apis and generate the Custom Resource Definition (CRD) and Custom Resource (CR) files under deploy/crds . $ operator-sdk add api --api-version=$OPERATOR_GROUP/$OPERATOR_VERSION --kind=$CRD_KIND The command will create a new API, a Custom Resource (CR), a Custom Resource Definition (CRD). One file is created in pkg/apis called addtoscheme_guestbook_v1.go that registers the new schema. One new file is created in pkg/apis/guestbook called group.go that defines the package. Four new files are created in pkg/apis/guestbook/v1 : - doc.go, - guestbook_types.go, - register.go, - zz_generated.deepcopy.go. The guestbook_types.go file, package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // GuestbookSpec defines the desired state of Guestbook type GuestbookSpec struct { } // GuestbookStatus defines the observed state of Guestbook type GuestbookStatus struct { } type Guestbook struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec GuestbookSpec `json:\"spec,omitempty\"` Status GuestbookStatus `json:\"status,omitempty\"` } // GuestbookList contains a list of Guestbook type GuestbookList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Items []Guestbook `json:\"items\"` } func init() { SchemeBuilder.Register(&Guestbook{}, &GuestbookList{}) } The Custom Resource (CR) in file deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr , apiVersion: guestbook.remkoh.dev/v1 kind: Guestbook metadata: name: example-guestbook spec: # Add fields here size: 3 The Custom Resource Definition (CRD) in file deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml , apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.guestbook.remkoh.dev spec: group: guestbook.remkoh.dev names: kind: Guestbook listKind: GuestbookList plural: guestbooks singular: guestbook scope: Namespaced versions: - name: v1 schema: openAPIV3Schema: description: Guestbook is the Schema for the guestbooks API properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: GuestbookSpec defines the desired state of Guestbook type: object status: description: GuestbookStatus defines the observed state of Guestbook type: object type: object served: true storage: true subresources: status: {} 3. Create a new Controller \u00b6 Add a new controller under pkg/controller/<kind> . $ operator-sdk add controller --api-version=$OPERATOR_GROUP/$OPERATOR_VERSION --kind=$CRD_KIND This command creates two files in pkg/controller : - add_guestbook.go , which registers the new controller, and - guestbook/guestbook_controller.go , which is the actual custom controller logic. The file guestbook/guestbook_controller.go defines the Reconcile function, // Reconcile reads state of the cluster for a Guestbook object and makes changes based on the state read and what is in the Guestbook.Spec // TODO(user): User must modify this Reconcile function to implement their own Controller logic. This example creates a Pod as an example func (r *ReconcileGuestbook) Reconcile(request reconcile.Request) (reconcile.Result, error) { ... // Fetch the Guestbook instance instance := &guestbookv1.Guestbook{} ... // Define a new Pod object pod := newPodForCR(instance) ... } 4. Compile and Build the Code \u00b6 The operator-sdk build command compiles the code and builds the executables. fter you built the image, push it to your image registry, e.g. Docker hub. $ operator-sdk build docker.io/$DOCKER_USERNAME/$OPERATOR_NAME $ docker login docker.io -u $DOCKER_USERNAME $ docker push docker.io/$DOCKER_USERNAME/$OPERATOR_NAME 5. Deploy the Operator \u00b6 First replace the image attribute in the operator resource with the built image, $ sed -i \"s|REPLACE_IMAGE|docker.io/$DOCKER_USERNAME/$OPERATOR_NAME|g\" deploy/operator.yaml Make sure you are connected to the OpenShift cluster (see above how to connect), and deploy the operator with the following template code. $ oc create sa $OPERATOR_PROJECT $ oc create -f deploy/role.yaml $ oc create -f deploy/role_binding.yaml $ oc create -f deploy/crds/${OPERATOR_GROUP}_${CRD_KIND,,}s_crd.yaml $ oc create -f deploy/operator.yaml $ oc create -f deploy/crds/${OPERATOR_GROUP}_${OPERATOR_VERSION}_${CRD_KIND,,}_cr.yaml $ oc get deployment $OPERATOR_PROJECT $ oc get pod -l app=example-${CRD_KIND,,} $ oc describe ${CRD_KIND,,}s.${OPERATOR_GROUP} example-${CRD_KIND,,} For our example Guestbook project the above templates should resolve as follows, $ oc create sa guestbook-project $ oc create -f deploy/role.yaml $ oc create -f deploy/role_binding.yaml $ oc create -f deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml $ oc create -f deploy/operator.yaml $ oc create -f deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr.yaml $ oc get deployment guestbook-project $ oc get pod -l app=example-guestbook $ oc describe guestbooks.guestbook.remkoh.dev example-guestbook Cleanup \u00b6 $ oc delete sa $OPERATOR_PROJECT $ oc delete role $OPERATOR_PROJECT $ oc delete rolebinding $OPERATOR_PROJECT $ oc delete customresourcedefinition ${CRD_KIND,,}s.${OPERATOR_GROUP} $ oc delete deployment $OPERATOR_PROJECT Application CRD \u00b6 The Application CRD (Custom Resource Definition) and Controller provide the following: - Describe an applications metadata, - A point to connect the infrastructure, such as Deployments, to as a root object. - - Application level health checks. This could be used by: - Application operators, - Tools, such as Helm, and - Dashboards. apiVersion: app.k8s.io/v1beta1 kind: Application metadata: name: \"guestbook\" labels: app.kubernetes.io/name: \"guestbook\" spec: selector: matchLabels: app.kubernetes.io/name: \"guestbook\" componentKinds: - group: v1 kind: Deployment - group: v1 kind: Service descriptor: type: \"guestbook\" keywords: - \"gb\" - \"guestbook\" links: - description: Github url: \"https://github.com/IBM/guestbook\" version: \"0.1.0\" description: \"The Guestbook application is an example app to demonstrate key Kubernetes functionality.\" maintainers: - name: IBM Developer email: developer@ibm.com owners: - name: IBM Developer email: developer@ibm.com","title":"Kubernetes Extensions"},{"location":"generatedContent/kubernetes-extensions.git/#kubernetes-extensions","text":"Access the web-terminal Login Create a Custom Resource Operators Ready Made Operators Create a Custom Resource and Operator using the Operator SDK Install sdk-operator Create the Operator Cleanup Application CRD","title":"Kubernetes Extensions"},{"location":"generatedContent/kubernetes-extensions.git/#access-the-web-terminal","text":"When running the lab for Kubernetes Extensions, you can make use of a web-terminal. The Dockerfile to use is located in https://github.com/IBMAppModernization/web-terminal, and named Dockerfile-s2i-oc-tekton-operator . To run on localhost as a Docker container, git clone https://github.com/IBMAppModernization/web-terminal.git cd web-terminal docker build --no-cache -t web-terminal:latest -f Dockerfile-s2i-oc-tekton-operator . docker run -d --restart always --name terminal -p 7681:7681 -v $HOME/dev/tmp:/root/dev web-terminal docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 85edc0b0ec27 web-terminal \"ttyd -p 7681 bash\" 17 minutes ago Up 17 minutes 0.0.0.0:7681->7681/tcp terminal The volume mapping will write all files under the working directory to the host directory $HOME/dev/tmp . So suppose my host's user home directory is /Users/remkohdev@us.ibm.com/ . If I open the terminal in the browser, the working directory for the user is /root . Any file that is created under /root is created on the host's directory $HOME/dev/tmp . Similarly if I create a file in $HOME/dev/tmp it is available in the container's /root directory. Open the web-terminal in a browser and go to http://0.0.0.0:7681. If Go, Operator SDK","title":"Access the web-terminal"},{"location":"generatedContent/kubernetes-extensions.git/#login","text":"export CLUSTERNAME=remkohdev-roks-labs-3n-cluster ibmcloud login Go to the OpenShift web console Copy Login command oc login --token=_12AbcD345kIPDIRg2jYpCuZ-g5SM5Im9irY2tol4Q8 --server=https://c100-e.us-south.containers.cloud.ibm.com:30712","title":"Login"},{"location":"generatedContent/kubernetes-extensions.git/#create-a-custom-resource-cr","text":"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ Custom Resource Definitions (CRD) were added in Kubernetes v1.7 in June 2017. A CRD defines Custom Resources (CR). A CR is an extension of the Kubernetes API that allows you to store your own API Objects and lets the API Server handle the lifecycle of a CR. On their own, CRs simply let you store and retrieve structured data. For instance, our Guestbook application consists of an object Guestbook with attributes GuestbookTitle and GuestbookSubtitle , and a Guestbook handles objectes of type GuestbookMessage with attributes Message , Sender . You have to ask yourself if it makes sense if your objects are added as a Custom Resource to Kubernetes or not. If your API is a Declarative API you can consider adding a CR. Your API has a small number of small objects (resources). The objects define configuration of applications or infrastructure. The objects are updated relatively infrequently. Users often need to read and write the objects. main operations on the objects are CRUD (create, read, update and delete). Transactions between objects are not required. It doesn't immediately make sense to store messages by Guestbook users in Kubernetes, but it might make sense to store meta-data about a Guestbook deployment, for instance the title and subtitle of a Guestbook deployment, assigned resources or replicas. Another benefit of adding a Custom Resource is to view your types in the Kubernetes Dashboard. If you want to deploy a Guestbook instance as a Kubernetes API object and let the Kubernetes API Server handle the lifecycle events of the Guestbook deployment, you can create a Custom Resource Definition (CRD) for the Guestbook object as follows. That way you can deploy multiple Guestbooks with different titles and let each be managed by Kubernetes. cat <<EOF >>guestbook-crd.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.apps.ibm.com spec: group: apps.ibm.com versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: guestbookTitle: type: string guestbookSubtitle: type: string scope: Namespaced names: plural: guestbooks singular: guestbook kind: Guestbook shortNames: - gb EOF You can see that the apiVersion is part of the apiextensions.k8s.io/v1 API Group in Kubernetes, which is the API that enables extensions, and the kind is set to CustomResourceDefinition . The served flag can disable and enable a version. Only 1 version can be flagged as the storage version. The spec.names.kind is used by your resource manifests and should be CamelCased. Create the Custom Resource for the Guestbook witht he command, oc create -f guestbook-crd.yaml When run in the terminal, $ oc create -f guestbook-crd.yaml customresourcedefinition.apiextensions.k8s.io/guestbooks.apps.ibm.com created You have now added a CR to the Kubernetes API, but you have not yet created a deployment of type Guestbook yet. Create a resource specification of type Guestbook named my-guestbook , cat <<EOF >>my-guestbook.yaml apiVersion: \"apps.ibm.com/v1\" kind: Guestbook metadata: name: my-guestbook spec: guestbookTitle: \"The Chemical Wedding of Remko\" guestbookSubtitle: \"First Day of Many\" EOF And to create the my-guestbook resource, run the command oc create -f my-guestbook.yaml When run in the terminal, $ oc create -f my-guestbook.yaml guestbook.apps.ibm.com/my-guestbook created If you list all Kubernetes resources, only the default Kubernetes service is listed. To list your Custom Resources, add the extended type to your command. $ oc get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 5d14h service/openshift ExternalName <none> kubernetes.default.svc.cluster.local <none> 5d14h service/openshift-apiserver ClusterIP 172.21.6.8 <none> =443/TCP 5d14h $ oc get guestbook NAME AGE my-guestbook 8m32s To read the details for the my-guestbook of type Guestbook , describe the instance, $ oc describe guestbook my-guestbook Name: my-guestbook Namespace: default Labels: <none> Annotations: <none> API Version: apps.ibm.com/v1 Kind: Guestbook Metadata: Creation Timestamp: 2020-06-30T20:31:36Z Generation: 1 Resource Version: 1081471 Self Link: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook UID: dcbdcafc-999d-4051-9244-0315093357e7 Spec: Guestbook Subtitle: First Day of Many Guestbook Title: The Chemical Wedding of Remko Events: <none> Or retrieve the resource information by specifying the type, $ oc get Guestbook -o yaml apiVersion: v1 items: - apiVersion: apps.ibm.com/v1 kind: Guestbook metadata: creationTimestamp: \"2020-07-02T04:41:57Z\" generation: 1 name: my-guestbook namespace: default resourceVersion: \"1903244\" selfLink: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook uid: 3f774899-3070-4e00-b74c-a6a14654faeb spec: guestbookSubtitle: First Day of Many guestbookTitle: The Chemical Wedding of Remko kind: List metadata: resourceVersion: \"\" selfLink: \"\" In the OpenShift web console, you can browse to Administration > Custom Resource Definitions and find the Guestbook CRD at /k8s/cluster/customresourcedefinitions/guestbooks.apps.ibm.com . You have now created a new type or Custom Resource (CR) and created an instance of your new type. But just having a new type and a new instance of the type, does not add as much control over the instances yet, we can basically only create and delete a static type with some descriptive meta-data. With a custom controller or Operator you can over-write the methods that are triggered at certain lifecycle events.","title":"Create a Custom Resource (CR)"},{"location":"generatedContent/kubernetes-extensions.git/#operators","text":"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/ Operators are clients of the Kubernetes API that act as controllers for a Custom Resource. To write applications that use the Kubernetes REST API, you can use one of the following supported client libraries: - Go , - Python , - Java , - CSharp dotnet , - JavaScript , - Haskell . In addition, there are many community-maintained client libraries .","title":"Operators"},{"location":"generatedContent/kubernetes-extensions.git/#ready-made-operators","text":"At the OperatorHub.io , you find ready to use operators written by the community.","title":"Ready made operators"},{"location":"generatedContent/kubernetes-extensions.git/#create-a-custom-resource-and-operator-using-the-operator-sdk","text":"To write your own operator you can use existing tools: - KUDO (Kubernetes Universal Declarative Operator), - kubebuilder , - Metacontroller using custom WebHooks, - the Operator Framework . The Operator SDK provides the following workflow to develop a new Operator: The following workflow is for a new Go operator: Create a new operator project using the SDK Command Line Interface(CLI) Define new resource APIs by adding Custom Resource Definitions(CRD) Define Controllers to watch and reconcile resources Write the reconciling logic for your Controller using the SDK and controller-runtime APIs Use the SDK CLI to build and generate the operator deployment manifests","title":"Create a Custom Resource and Operator using the Operator SDK"},{"location":"generatedContent/kubernetes-extensions.git/#install-sdk-operator","text":"For detailed installation instructions go here . To install the Operator SDK in Ubuntu, you need to install the Go tools and the Operator SDK. $ curl -LO https://golang.org/dl/go1.14.4.linux-amd64.tar.gz $ tar -C /usr/local -xzf go1.14.4.linux-amd64.tar.gz $ export PATH=$PATH:/usr/local/go/bin $ curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v0.18.2/operator-sdk-v0.18.2-x86_64-linux-gnu $ chmod +x operator-sdk-v0.18.2-x86_64-linux-gnu $ sudo mkdir -p /usr/local/bin/ $ sudo cp operator-sdk-v0.18.2-x86_64-linux-gnu /usr/local/bin/operator-sdk $ rm operator-sdk-v0.18.2-x86_64-linux-gnu $ go version $ operator-sdk version","title":"Install sdk-operator"},{"location":"generatedContent/kubernetes-extensions.git/#create-the-operator","text":"","title":"Create the Operator"},{"location":"generatedContent/kubernetes-extensions.git/#1-create-a-new-project","text":"Create a new Operator project, $ export DOCKER_USERNAME=<your-docker-username> $ export OPERATOR_NAME=guestbook-operator $ export OPERATOR_PROJECT=guestbook-project $ export OPERATOR_GROUP=guestbook.remkoh.dev $ export OPERATOR_VERSION=v1 $ export CRD_KIND=Guestbook $ go version $ operator-sdk version $ operator-sdk new $OPERATOR_PROJECT --type go --repo github.com/$DOCKER_USERNAME/$OPERATOR_NAME $ cd $OPERATOR_PROJECT The scaffolding of a new project will create an operator, an api and a controller.","title":"1. Create a New Project"},{"location":"generatedContent/kubernetes-extensions.git/#2-create-a-new-api","text":"Add a new API definition for a new Custom Resource under pkg/apis and generate the Custom Resource Definition (CRD) and Custom Resource (CR) files under deploy/crds . $ operator-sdk add api --api-version=$OPERATOR_GROUP/$OPERATOR_VERSION --kind=$CRD_KIND The command will create a new API, a Custom Resource (CR), a Custom Resource Definition (CRD). One file is created in pkg/apis called addtoscheme_guestbook_v1.go that registers the new schema. One new file is created in pkg/apis/guestbook called group.go that defines the package. Four new files are created in pkg/apis/guestbook/v1 : - doc.go, - guestbook_types.go, - register.go, - zz_generated.deepcopy.go. The guestbook_types.go file, package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // GuestbookSpec defines the desired state of Guestbook type GuestbookSpec struct { } // GuestbookStatus defines the observed state of Guestbook type GuestbookStatus struct { } type Guestbook struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec GuestbookSpec `json:\"spec,omitempty\"` Status GuestbookStatus `json:\"status,omitempty\"` } // GuestbookList contains a list of Guestbook type GuestbookList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Items []Guestbook `json:\"items\"` } func init() { SchemeBuilder.Register(&Guestbook{}, &GuestbookList{}) } The Custom Resource (CR) in file deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr , apiVersion: guestbook.remkoh.dev/v1 kind: Guestbook metadata: name: example-guestbook spec: # Add fields here size: 3 The Custom Resource Definition (CRD) in file deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml , apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.guestbook.remkoh.dev spec: group: guestbook.remkoh.dev names: kind: Guestbook listKind: GuestbookList plural: guestbooks singular: guestbook scope: Namespaced versions: - name: v1 schema: openAPIV3Schema: description: Guestbook is the Schema for the guestbooks API properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string metadata: type: object spec: description: GuestbookSpec defines the desired state of Guestbook type: object status: description: GuestbookStatus defines the observed state of Guestbook type: object type: object served: true storage: true subresources: status: {}","title":"2. Create a new API"},{"location":"generatedContent/kubernetes-extensions.git/#3-create-a-new-controller","text":"Add a new controller under pkg/controller/<kind> . $ operator-sdk add controller --api-version=$OPERATOR_GROUP/$OPERATOR_VERSION --kind=$CRD_KIND This command creates two files in pkg/controller : - add_guestbook.go , which registers the new controller, and - guestbook/guestbook_controller.go , which is the actual custom controller logic. The file guestbook/guestbook_controller.go defines the Reconcile function, // Reconcile reads state of the cluster for a Guestbook object and makes changes based on the state read and what is in the Guestbook.Spec // TODO(user): User must modify this Reconcile function to implement their own Controller logic. This example creates a Pod as an example func (r *ReconcileGuestbook) Reconcile(request reconcile.Request) (reconcile.Result, error) { ... // Fetch the Guestbook instance instance := &guestbookv1.Guestbook{} ... // Define a new Pod object pod := newPodForCR(instance) ... }","title":"3. Create a new Controller"},{"location":"generatedContent/kubernetes-extensions.git/#4-compile-and-build-the-code","text":"The operator-sdk build command compiles the code and builds the executables. fter you built the image, push it to your image registry, e.g. Docker hub. $ operator-sdk build docker.io/$DOCKER_USERNAME/$OPERATOR_NAME $ docker login docker.io -u $DOCKER_USERNAME $ docker push docker.io/$DOCKER_USERNAME/$OPERATOR_NAME","title":"4. Compile and Build the Code"},{"location":"generatedContent/kubernetes-extensions.git/#5-deploy-the-operator","text":"First replace the image attribute in the operator resource with the built image, $ sed -i \"s|REPLACE_IMAGE|docker.io/$DOCKER_USERNAME/$OPERATOR_NAME|g\" deploy/operator.yaml Make sure you are connected to the OpenShift cluster (see above how to connect), and deploy the operator with the following template code. $ oc create sa $OPERATOR_PROJECT $ oc create -f deploy/role.yaml $ oc create -f deploy/role_binding.yaml $ oc create -f deploy/crds/${OPERATOR_GROUP}_${CRD_KIND,,}s_crd.yaml $ oc create -f deploy/operator.yaml $ oc create -f deploy/crds/${OPERATOR_GROUP}_${OPERATOR_VERSION}_${CRD_KIND,,}_cr.yaml $ oc get deployment $OPERATOR_PROJECT $ oc get pod -l app=example-${CRD_KIND,,} $ oc describe ${CRD_KIND,,}s.${OPERATOR_GROUP} example-${CRD_KIND,,} For our example Guestbook project the above templates should resolve as follows, $ oc create sa guestbook-project $ oc create -f deploy/role.yaml $ oc create -f deploy/role_binding.yaml $ oc create -f deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml $ oc create -f deploy/operator.yaml $ oc create -f deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr.yaml $ oc get deployment guestbook-project $ oc get pod -l app=example-guestbook $ oc describe guestbooks.guestbook.remkoh.dev example-guestbook","title":"5. Deploy the Operator"},{"location":"generatedContent/kubernetes-extensions.git/#cleanup","text":"$ oc delete sa $OPERATOR_PROJECT $ oc delete role $OPERATOR_PROJECT $ oc delete rolebinding $OPERATOR_PROJECT $ oc delete customresourcedefinition ${CRD_KIND,,}s.${OPERATOR_GROUP} $ oc delete deployment $OPERATOR_PROJECT","title":"Cleanup"},{"location":"generatedContent/kubernetes-extensions.git/#application-crd","text":"The Application CRD (Custom Resource Definition) and Controller provide the following: - Describe an applications metadata, - A point to connect the infrastructure, such as Deployments, to as a root object. - - Application level health checks. This could be used by: - Application operators, - Tools, such as Helm, and - Dashboards. apiVersion: app.k8s.io/v1beta1 kind: Application metadata: name: \"guestbook\" labels: app.kubernetes.io/name: \"guestbook\" spec: selector: matchLabels: app.kubernetes.io/name: \"guestbook\" componentKinds: - group: v1 kind: Deployment - group: v1 kind: Service descriptor: type: \"guestbook\" keywords: - \"gb\" - \"guestbook\" links: - description: Github url: \"https://github.com/IBM/guestbook\" version: \"0.1.0\" description: \"The Guestbook application is an example app to demonstrate key Kubernetes functionality.\" maintainers: - name: IBM Developer email: developer@ibm.com owners: - name: IBM Developer email: developer@ibm.com","title":"Application CRD"},{"location":"generatedContent/kubernetes-operators/","text":"Kubernetes Operators \u00b6 Kubernetes Operators is a series of labs about deploying applications to Kubernetes using the Extensions API to create Custom Resources (CR) and customize controllers using the Operator Pattern. This workshop uses the Operator Framework to create operators. Pre-requirements \u00b6 a Free IBM Cloud account, to create a new IBM Cloud account, follow the instructions here . a Red Hat OpenShift Kubernetes Service (ROKS) v4.5 using a cluster with admin rights, CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, follow the instructions here . Labs \u00b6 Setup , Lab 1 Create a Custom Resource (CR) , Lab2 Create an Operator of Type Go using the Operator SDK , Lab3 Create an Operator using an Existing Helm Chart , Tools Technologies \u00b6 Red Hat OpenShift Kubernetes Service (ROKS) v4.5 Operator Framework Contributors \u00b6 Rojan Jose, rojanjose Remko de Knikker, remkohdev","title":"Kubernetes Operators"},{"location":"generatedContent/kubernetes-operators/#kubernetes-operators","text":"Kubernetes Operators is a series of labs about deploying applications to Kubernetes using the Extensions API to create Custom Resources (CR) and customize controllers using the Operator Pattern. This workshop uses the Operator Framework to create operators.","title":"Kubernetes Operators"},{"location":"generatedContent/kubernetes-operators/#pre-requirements","text":"a Free IBM Cloud account, to create a new IBM Cloud account, follow the instructions here . a Red Hat OpenShift Kubernetes Service (ROKS) v4.5 using a cluster with admin rights, CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, follow the instructions here .","title":"Pre-requirements"},{"location":"generatedContent/kubernetes-operators/#labs","text":"Setup , Lab 1 Create a Custom Resource (CR) , Lab2 Create an Operator of Type Go using the Operator SDK , Lab3 Create an Operator using an Existing Helm Chart , Tools","title":"Labs"},{"location":"generatedContent/kubernetes-operators/#technologies","text":"Red Hat OpenShift Kubernetes Service (ROKS) v4.5 Operator Framework","title":"Technologies"},{"location":"generatedContent/kubernetes-operators/#contributors","text":"Rojan Jose, rojanjose Remko de Knikker, remkohdev","title":"Contributors"},{"location":"generatedContent/kubernetes-operators/lab1/","text":"Create a Custom Resource \u00b6 Create a Custom Resource Operators Ready Made Operators Create a Custom Resource and Operator using the Operator SDK Install sdk-operator Create the Operator Cleanup Application CRD Create a Custom Resource (CR) \u00b6 https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ Custom Resource Definitions (CRD) were added in Kubernetes v1.7 in June 2017. A CRD defines Custom Resources (CR). A CR is an extension of the Kubernetes API that allows you to store your own API Objects and lets the API Server handle the lifecycle of a CR. On their own, CRs simply let you store and retrieve structured data. For instance, our Guestbook application consists of an object Guestbook with attributes GuestbookTitle and GuestbookSubtitle , and a Guestbook handles objectes of type GuestbookMessage with attributes Message , Sender . You have to ask yourself if it makes sense if your objects are added as a Custom Resource to Kubernetes or not. If your API is a Declarative API you can consider adding a CR. Your API has a small number of small objects (resources). The objects define configuration of applications or infrastructure. The objects are updated relatively infrequently. Users often need to read and write the objects. main operations on the objects are CRUD (create, read, update and delete). Transactions between objects are not required. It doesn't immediately make sense to store messages by Guestbook users in Kubernetes, but it might make sense to store meta-data about a Guestbook deployment, for instance the title and subtitle of a Guestbook deployment, assigned resources or replicas. Another benefit of adding a Custom Resource is to view your types in the Kubernetes Dashboard. If you want to deploy a Guestbook instance as a Kubernetes API object and let the Kubernetes API Server handle the lifecycle events of the Guestbook deployment, you can create a Custom Resource Definition (CRD) for the Guestbook object as follows. That way you can deploy multiple Guestbooks with different titles and let each be managed by Kubernetes. cat <<EOF >>guestbook-crd.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.apps.ibm.com spec: group: apps.ibm.com versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: guestbookTitle: type: string guestbookSubtitle: type: string scope: Namespaced names: plural: guestbooks singular: guestbook kind: Guestbook shortNames: - gb EOF You can see that the apiVersion is part of the apiextensions.k8s.io/v1 API Group in Kubernetes, which is the API that enables extensions, and the kind is set to CustomResourceDefinition . The served flag can disable and enable a version. Only 1 version can be flagged as the storage version. The spec.names.kind is used by your resource manifests and should be CamelCased. Create the Custom Resource for the Guestbook witht he command, oc create -f guestbook-crd.yaml When run in the terminal, $ oc create -f guestbook-crd.yaml customresourcedefinition.apiextensions.k8s.io/guestbooks.apps.ibm.com created You have now added a CR to the Kubernetes API, but you have not yet created a deployment of type Guestbook yet. Create a resource specification of type Guestbook named my-guestbook , cat <<EOF >>my-guestbook.yaml apiVersion: \"apps.ibm.com/v1\" kind: Guestbook metadata: name: my-guestbook spec: guestbookTitle: \"The Chemical Wedding of Remko\" guestbookSubtitle: \"First Day of Many\" EOF And to create the my-guestbook resource, run the command oc create -f my-guestbook.yaml When run in the terminal, $ oc create -f my-guestbook.yaml guestbook.apps.ibm.com/my-guestbook created If you list all Kubernetes resources, only the default Kubernetes service is listed. To list your Custom Resources, add the extended type to your command. $ oc get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 5d14h service/openshift ExternalName <none> kubernetes.default.svc.cluster.local <none> 5d14h service/openshift-apiserver ClusterIP 172 .21.6.8 <none> = 443 /TCP 5d14h $ oc get guestbook NAME AGE my-guestbook 8m32s To read the details for the my-guestbook of type Guestbook , describe the instance, $ oc describe guestbook my-guestbook Name: my-guestbook Namespace: default Labels: <none> Annotations: <none> API Version: apps.ibm.com/v1 Kind: Guestbook Metadata: Creation Timestamp: 2020 -06-30T20:31:36Z Generation: 1 Resource Version: 1081471 Self Link: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook UID: dcbdcafc-999d-4051-9244-0315093357e7 Spec: Guestbook Subtitle: First Day of Many Guestbook Title: The Chemical Wedding of Remko Events: <none> Or retrieve the resource information by specifying the type, $ oc get Guestbook -o yaml apiVersion: v1 items: - apiVersion: apps.ibm.com/v1 kind: Guestbook metadata: creationTimestamp: \"2020-07-02T04:41:57Z\" generation: 1 name: my-guestbook namespace: default resourceVersion: \"1903244\" selfLink: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook uid: 3f774899-3070-4e00-b74c-a6a14654faeb spec: guestbookSubtitle: First Day of Many guestbookTitle: The Chemical Wedding of Remko kind: List metadata: resourceVersion: \"\" selfLink: \"\" In the OpenShift web console, you can browse to Administration > Custom Resource Definitions and find the Guestbook CRD at /k8s/cluster/customresourcedefinitions/guestbooks.apps.ibm.com . You have now created a new type or Custom Resource (CR) and created an instance of your new type. But just having a new type and a new instance of the type, does not add as much control over the instances yet, we can basically only create and delete a static type with some descriptive meta-data. With a custom controller or Operator you can over-write the methods that are triggered at certain lifecycle events.","title":"Create a Custom Resource"},{"location":"generatedContent/kubernetes-operators/lab1/#create-a-custom-resource","text":"Create a Custom Resource Operators Ready Made Operators Create a Custom Resource and Operator using the Operator SDK Install sdk-operator Create the Operator Cleanup Application CRD","title":"Create a Custom Resource"},{"location":"generatedContent/kubernetes-operators/lab1/#create-a-custom-resource-cr","text":"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ Custom Resource Definitions (CRD) were added in Kubernetes v1.7 in June 2017. A CRD defines Custom Resources (CR). A CR is an extension of the Kubernetes API that allows you to store your own API Objects and lets the API Server handle the lifecycle of a CR. On their own, CRs simply let you store and retrieve structured data. For instance, our Guestbook application consists of an object Guestbook with attributes GuestbookTitle and GuestbookSubtitle , and a Guestbook handles objectes of type GuestbookMessage with attributes Message , Sender . You have to ask yourself if it makes sense if your objects are added as a Custom Resource to Kubernetes or not. If your API is a Declarative API you can consider adding a CR. Your API has a small number of small objects (resources). The objects define configuration of applications or infrastructure. The objects are updated relatively infrequently. Users often need to read and write the objects. main operations on the objects are CRUD (create, read, update and delete). Transactions between objects are not required. It doesn't immediately make sense to store messages by Guestbook users in Kubernetes, but it might make sense to store meta-data about a Guestbook deployment, for instance the title and subtitle of a Guestbook deployment, assigned resources or replicas. Another benefit of adding a Custom Resource is to view your types in the Kubernetes Dashboard. If you want to deploy a Guestbook instance as a Kubernetes API object and let the Kubernetes API Server handle the lifecycle events of the Guestbook deployment, you can create a Custom Resource Definition (CRD) for the Guestbook object as follows. That way you can deploy multiple Guestbooks with different titles and let each be managed by Kubernetes. cat <<EOF >>guestbook-crd.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.apps.ibm.com spec: group: apps.ibm.com versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: guestbookTitle: type: string guestbookSubtitle: type: string scope: Namespaced names: plural: guestbooks singular: guestbook kind: Guestbook shortNames: - gb EOF You can see that the apiVersion is part of the apiextensions.k8s.io/v1 API Group in Kubernetes, which is the API that enables extensions, and the kind is set to CustomResourceDefinition . The served flag can disable and enable a version. Only 1 version can be flagged as the storage version. The spec.names.kind is used by your resource manifests and should be CamelCased. Create the Custom Resource for the Guestbook witht he command, oc create -f guestbook-crd.yaml When run in the terminal, $ oc create -f guestbook-crd.yaml customresourcedefinition.apiextensions.k8s.io/guestbooks.apps.ibm.com created You have now added a CR to the Kubernetes API, but you have not yet created a deployment of type Guestbook yet. Create a resource specification of type Guestbook named my-guestbook , cat <<EOF >>my-guestbook.yaml apiVersion: \"apps.ibm.com/v1\" kind: Guestbook metadata: name: my-guestbook spec: guestbookTitle: \"The Chemical Wedding of Remko\" guestbookSubtitle: \"First Day of Many\" EOF And to create the my-guestbook resource, run the command oc create -f my-guestbook.yaml When run in the terminal, $ oc create -f my-guestbook.yaml guestbook.apps.ibm.com/my-guestbook created If you list all Kubernetes resources, only the default Kubernetes service is listed. To list your Custom Resources, add the extended type to your command. $ oc get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 5d14h service/openshift ExternalName <none> kubernetes.default.svc.cluster.local <none> 5d14h service/openshift-apiserver ClusterIP 172 .21.6.8 <none> = 443 /TCP 5d14h $ oc get guestbook NAME AGE my-guestbook 8m32s To read the details for the my-guestbook of type Guestbook , describe the instance, $ oc describe guestbook my-guestbook Name: my-guestbook Namespace: default Labels: <none> Annotations: <none> API Version: apps.ibm.com/v1 Kind: Guestbook Metadata: Creation Timestamp: 2020 -06-30T20:31:36Z Generation: 1 Resource Version: 1081471 Self Link: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook UID: dcbdcafc-999d-4051-9244-0315093357e7 Spec: Guestbook Subtitle: First Day of Many Guestbook Title: The Chemical Wedding of Remko Events: <none> Or retrieve the resource information by specifying the type, $ oc get Guestbook -o yaml apiVersion: v1 items: - apiVersion: apps.ibm.com/v1 kind: Guestbook metadata: creationTimestamp: \"2020-07-02T04:41:57Z\" generation: 1 name: my-guestbook namespace: default resourceVersion: \"1903244\" selfLink: /apis/apps.ibm.com/v1/namespaces/default/guestbooks/my-guestbook uid: 3f774899-3070-4e00-b74c-a6a14654faeb spec: guestbookSubtitle: First Day of Many guestbookTitle: The Chemical Wedding of Remko kind: List metadata: resourceVersion: \"\" selfLink: \"\" In the OpenShift web console, you can browse to Administration > Custom Resource Definitions and find the Guestbook CRD at /k8s/cluster/customresourcedefinitions/guestbooks.apps.ibm.com . You have now created a new type or Custom Resource (CR) and created an instance of your new type. But just having a new type and a new instance of the type, does not add as much control over the instances yet, we can basically only create and delete a static type with some descriptive meta-data. With a custom controller or Operator you can over-write the methods that are triggered at certain lifecycle events.","title":"Create a Custom Resource (CR)"},{"location":"generatedContent/kubernetes-operators/lab2/","text":"Create an Operator of Type Go using the Operator SDK \u00b6 About Operators \u00b6 See https://kubernetes.io/docs/concepts/extend-kubernetes/operator/ . Operators are clients of the Kubernetes API that act as controllers for a Custom Resource. Operators are extensions to Kubernetes that use custom resources to manage applications and their components. They follow the Kubernetes principle of the control loop. About the Operator Framework \u00b6 The Operator Framework is an open source toolkit to manage Operators. The Operator SDK provides the following workflow to develop a new Operator: The following workflow is for a new Go operator: Create a new operator project using the SDK Command Line Interface(CLI) Define new resource APIs by adding Custom Resource Definitions(CRD) Define Controllers to watch and reconcile resources Write the reconciling logic for your Controller using the SDK and controller-runtime APIs Use the SDK CLI to build and generate the operator deployment manifests Install sdk-operator \u00b6 For detailed installation instructions go here . To install the Operator SDK in Ubuntu, you need to install the Go tools and the Operator SDK. curl -LO https://golang.org/dl/go1.14.4.linux-amd64.tar.gz tar -C /usr/local -xzf go1.14.4.linux-amd64.tar.gz export PATH = $PATH :/usr/local/go/bin curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v0.18.2/operator-sdk-v0.18.2-x86_64-linux-gnu chmod +x operator-sdk-v0.18.2-x86_64-linux-gnu sudo mkdir -p /usr/local/bin/ sudo cp operator-sdk-v0.18.2-x86_64-linux-gnu /usr/local/bin/operator-sdk rm operator-sdk-v0.18.2-x86_64-linux-gnu go version operator-sdk version 1. Create a New Project \u00b6 Create a new Operator project, export DOCKER_USERNAME = <your-docker-username> export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-project export OPERATOR_GROUP = guestbook.remkoh.dev export OPERATOR_VERSION = v1 export CRD_KIND = Guestbook go version operator-sdk version operator-sdk new $OPERATOR_PROJECT --type go --repo github.com/ $DOCKER_USERNAME / $OPERATOR_NAME cd $OPERATOR_PROJECT The scaffolding of a new project will create an operator, an api and a controller. 2. Create a new API \u00b6 Add a new API definition for a new Custom Resource under pkg/apis and generate the Custom Resource Definition (CRD) and Custom Resource (CR) files under deploy/crds . operator-sdk add api --api-version = $OPERATOR_GROUP / $OPERATOR_VERSION --kind = $CRD_KIND The command will create a new API, a Custom Resource (CR), a Custom Resource Definition (CRD). One file is created in pkg/apis called addtoscheme_guestbook_v1.go that registers the new schema. One new file is created in pkg/apis/guestbook called group.go that defines the package. Four new files are created in pkg/apis/guestbook/v1 : doc.go, guestbook_types.go, register.go, zz_generated.deepcopy.go. The guestbook_types.go file, package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // GuestbookSpec defines the desired state of Guestbook type GuestbookSpec struct { } // GuestbookStatus defines the observed state of Guestbook type GuestbookStatus struct { } type Guestbook struct { metav1 . TypeMeta `json:\",inline\"` metav1 . ObjectMeta `json:\"metadata,omitempty\"` Spec GuestbookSpec `json:\"spec,omitempty\"` Status GuestbookStatus `json:\"status,omitempty\"` } // GuestbookList contains a list of Guestbook type GuestbookList struct { metav1 . TypeMeta `json:\",inline\"` metav1 . ListMeta `json:\"metadata,omitempty\"` Items [] Guestbook `json:\"items\"` } func init () { SchemeBuilder . Register ( & Guestbook {}, & GuestbookList {}) } The Custom Resource (CR) in file deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr , apiVersion : guestbook.remkoh.dev/v1 kind : Guestbook metadata : name : example-guestbook spec : # Add fields here size : 3 The Custom Resource Definition (CRD) in file deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml , apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : guestbooks.guestbook.remkoh.dev spec : group : guestbook.remkoh.dev names : kind : Guestbook listKind : GuestbookList plural : guestbooks singular : guestbook scope : Namespaced versions : - name : v1 schema : openAPIV3Schema : description : Guestbook is the Schema for the guestbooks API properties : apiVersion : description : 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type : string kind : description : 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type : string metadata : type : object spec : description : GuestbookSpec defines the desired state of Guestbook type : object status : description : GuestbookStatus defines the observed state of Guestbook type : object type : object served : true storage : true subresources : status : {} 3. Create a new Controller \u00b6 Add a new controller under pkg/controller/<kind> . operator-sdk add controller --api-version = $OPERATOR_GROUP / $OPERATOR_VERSION --kind = $CRD_KIND This command creates two files in pkg/controller : add_guestbook.go , which registers the new controller, and guestbook/guestbook_controller.go , which is the actual custom controller logic. The file guestbook/guestbook_controller.go defines the Reconcile function, // Reconcile reads state of the cluster for a Guestbook object and makes changes based on the state read and what is in the Guestbook.Spec // TODO(user): User must modify this Reconcile function to implement their own Controller logic. This example creates a Pod as an example func ( r * ReconcileGuestbook ) Reconcile ( request reconcile . Request ) ( reconcile . Result , error ) { ... // Fetch the Guestbook instance instance := & guestbookv1 . Guestbook {} ... // Define a new Pod object pod := newPodForCR ( instance ) ... } 4. Compile and Build the Code \u00b6 The operator-sdk build command compiles the code and builds the executables. fter you built the image, push it to your image registry, e.g. Docker hub. operator-sdk build docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME docker login docker.io -u $DOCKER_USERNAME docker push docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME 5. Deploy the Operator \u00b6 First replace the image attribute in the operator resource with the built image, sed -i \"s|REPLACE_IMAGE|docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME |g\" deploy/operator.yaml Make sure you are connected to the OpenShift cluster (see above how to connect), and deploy the operator with the following template code. oc create sa $OPERATOR_PROJECT oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml oc create -f deploy/crds/ ${ OPERATOR_GROUP } _ ${ CRD_KIND ,, } s_crd.yaml oc create -f deploy/operator.yaml oc create -f deploy/crds/ ${ OPERATOR_GROUP } _ ${ OPERATOR_VERSION } _ ${ CRD_KIND ,, } _cr.yaml oc get deployment $OPERATOR_PROJECT oc get pod -l app = example- ${ CRD_KIND ,, } oc describe ${ CRD_KIND ,, } s. ${ OPERATOR_GROUP } example- ${ CRD_KIND ,, } For our example Guestbook project the above templates should resolve as follows, oc create sa guestbook-project oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml oc create -f deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml oc create -f deploy/operator.yaml oc create -f deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr.yaml oc get deployment guestbook-project oc get pod -l app = example-guestbook oc describe guestbooks.guestbook.remkoh.dev example-guestbook Cleanup \u00b6 oc delete sa $OPERATOR_PROJECT oc delete role $OPERATOR_PROJECT oc delete rolebinding $OPERATOR_PROJECT oc delete customresourcedefinition ${ CRD_KIND ,, } s. ${ OPERATOR_GROUP } oc delete deployment $OPERATOR_PROJECT","title":"Create an Operator of Type Go using the Operator SDK"},{"location":"generatedContent/kubernetes-operators/lab2/#create-an-operator-of-type-go-using-the-operator-sdk","text":"","title":"Create an Operator of Type Go using the Operator SDK"},{"location":"generatedContent/kubernetes-operators/lab2/#about-operators","text":"See https://kubernetes.io/docs/concepts/extend-kubernetes/operator/ . Operators are clients of the Kubernetes API that act as controllers for a Custom Resource. Operators are extensions to Kubernetes that use custom resources to manage applications and their components. They follow the Kubernetes principle of the control loop.","title":"About Operators"},{"location":"generatedContent/kubernetes-operators/lab2/#about-the-operator-framework","text":"The Operator Framework is an open source toolkit to manage Operators. The Operator SDK provides the following workflow to develop a new Operator: The following workflow is for a new Go operator: Create a new operator project using the SDK Command Line Interface(CLI) Define new resource APIs by adding Custom Resource Definitions(CRD) Define Controllers to watch and reconcile resources Write the reconciling logic for your Controller using the SDK and controller-runtime APIs Use the SDK CLI to build and generate the operator deployment manifests","title":"About the Operator Framework"},{"location":"generatedContent/kubernetes-operators/lab2/#install-sdk-operator","text":"For detailed installation instructions go here . To install the Operator SDK in Ubuntu, you need to install the Go tools and the Operator SDK. curl -LO https://golang.org/dl/go1.14.4.linux-amd64.tar.gz tar -C /usr/local -xzf go1.14.4.linux-amd64.tar.gz export PATH = $PATH :/usr/local/go/bin curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v0.18.2/operator-sdk-v0.18.2-x86_64-linux-gnu chmod +x operator-sdk-v0.18.2-x86_64-linux-gnu sudo mkdir -p /usr/local/bin/ sudo cp operator-sdk-v0.18.2-x86_64-linux-gnu /usr/local/bin/operator-sdk rm operator-sdk-v0.18.2-x86_64-linux-gnu go version operator-sdk version","title":"Install sdk-operator"},{"location":"generatedContent/kubernetes-operators/lab2/#1-create-a-new-project","text":"Create a new Operator project, export DOCKER_USERNAME = <your-docker-username> export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-project export OPERATOR_GROUP = guestbook.remkoh.dev export OPERATOR_VERSION = v1 export CRD_KIND = Guestbook go version operator-sdk version operator-sdk new $OPERATOR_PROJECT --type go --repo github.com/ $DOCKER_USERNAME / $OPERATOR_NAME cd $OPERATOR_PROJECT The scaffolding of a new project will create an operator, an api and a controller.","title":"1. Create a New Project"},{"location":"generatedContent/kubernetes-operators/lab2/#2-create-a-new-api","text":"Add a new API definition for a new Custom Resource under pkg/apis and generate the Custom Resource Definition (CRD) and Custom Resource (CR) files under deploy/crds . operator-sdk add api --api-version = $OPERATOR_GROUP / $OPERATOR_VERSION --kind = $CRD_KIND The command will create a new API, a Custom Resource (CR), a Custom Resource Definition (CRD). One file is created in pkg/apis called addtoscheme_guestbook_v1.go that registers the new schema. One new file is created in pkg/apis/guestbook called group.go that defines the package. Four new files are created in pkg/apis/guestbook/v1 : doc.go, guestbook_types.go, register.go, zz_generated.deepcopy.go. The guestbook_types.go file, package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // GuestbookSpec defines the desired state of Guestbook type GuestbookSpec struct { } // GuestbookStatus defines the observed state of Guestbook type GuestbookStatus struct { } type Guestbook struct { metav1 . TypeMeta `json:\",inline\"` metav1 . ObjectMeta `json:\"metadata,omitempty\"` Spec GuestbookSpec `json:\"spec,omitempty\"` Status GuestbookStatus `json:\"status,omitempty\"` } // GuestbookList contains a list of Guestbook type GuestbookList struct { metav1 . TypeMeta `json:\",inline\"` metav1 . ListMeta `json:\"metadata,omitempty\"` Items [] Guestbook `json:\"items\"` } func init () { SchemeBuilder . Register ( & Guestbook {}, & GuestbookList {}) } The Custom Resource (CR) in file deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr , apiVersion : guestbook.remkoh.dev/v1 kind : Guestbook metadata : name : example-guestbook spec : # Add fields here size : 3 The Custom Resource Definition (CRD) in file deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml , apiVersion : apiextensions.k8s.io/v1 kind : CustomResourceDefinition metadata : name : guestbooks.guestbook.remkoh.dev spec : group : guestbook.remkoh.dev names : kind : Guestbook listKind : GuestbookList plural : guestbooks singular : guestbook scope : Namespaced versions : - name : v1 schema : openAPIV3Schema : description : Guestbook is the Schema for the guestbooks API properties : apiVersion : description : 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type : string kind : description : 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type : string metadata : type : object spec : description : GuestbookSpec defines the desired state of Guestbook type : object status : description : GuestbookStatus defines the observed state of Guestbook type : object type : object served : true storage : true subresources : status : {}","title":"2. Create a new API"},{"location":"generatedContent/kubernetes-operators/lab2/#3-create-a-new-controller","text":"Add a new controller under pkg/controller/<kind> . operator-sdk add controller --api-version = $OPERATOR_GROUP / $OPERATOR_VERSION --kind = $CRD_KIND This command creates two files in pkg/controller : add_guestbook.go , which registers the new controller, and guestbook/guestbook_controller.go , which is the actual custom controller logic. The file guestbook/guestbook_controller.go defines the Reconcile function, // Reconcile reads state of the cluster for a Guestbook object and makes changes based on the state read and what is in the Guestbook.Spec // TODO(user): User must modify this Reconcile function to implement their own Controller logic. This example creates a Pod as an example func ( r * ReconcileGuestbook ) Reconcile ( request reconcile . Request ) ( reconcile . Result , error ) { ... // Fetch the Guestbook instance instance := & guestbookv1 . Guestbook {} ... // Define a new Pod object pod := newPodForCR ( instance ) ... }","title":"3. Create a new Controller"},{"location":"generatedContent/kubernetes-operators/lab2/#4-compile-and-build-the-code","text":"The operator-sdk build command compiles the code and builds the executables. fter you built the image, push it to your image registry, e.g. Docker hub. operator-sdk build docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME docker login docker.io -u $DOCKER_USERNAME docker push docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME","title":"4. Compile and Build the Code"},{"location":"generatedContent/kubernetes-operators/lab2/#5-deploy-the-operator","text":"First replace the image attribute in the operator resource with the built image, sed -i \"s|REPLACE_IMAGE|docker.io/ $DOCKER_USERNAME / $OPERATOR_NAME |g\" deploy/operator.yaml Make sure you are connected to the OpenShift cluster (see above how to connect), and deploy the operator with the following template code. oc create sa $OPERATOR_PROJECT oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml oc create -f deploy/crds/ ${ OPERATOR_GROUP } _ ${ CRD_KIND ,, } s_crd.yaml oc create -f deploy/operator.yaml oc create -f deploy/crds/ ${ OPERATOR_GROUP } _ ${ OPERATOR_VERSION } _ ${ CRD_KIND ,, } _cr.yaml oc get deployment $OPERATOR_PROJECT oc get pod -l app = example- ${ CRD_KIND ,, } oc describe ${ CRD_KIND ,, } s. ${ OPERATOR_GROUP } example- ${ CRD_KIND ,, } For our example Guestbook project the above templates should resolve as follows, oc create sa guestbook-project oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml oc create -f deploy/crds/guestbook.remkoh.dev_guestbooks_crd.yaml oc create -f deploy/operator.yaml oc create -f deploy/crds/guestbook.remkoh.dev_v1_guestbook_cr.yaml oc get deployment guestbook-project oc get pod -l app = example-guestbook oc describe guestbooks.guestbook.remkoh.dev example-guestbook","title":"5. Deploy the Operator"},{"location":"generatedContent/kubernetes-operators/lab2/#cleanup","text":"oc delete sa $OPERATOR_PROJECT oc delete role $OPERATOR_PROJECT oc delete rolebinding $OPERATOR_PROJECT oc delete customresourcedefinition ${ CRD_KIND ,, } s. ${ OPERATOR_GROUP } oc delete deployment $OPERATOR_PROJECT","title":"Cleanup"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/","text":"Create an Operator using an Existing Helm Chart \u00b6 The Operator Framework is an open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator. The Operator SDK provides the tools to build, test and package Operators. The following workflow is for a Helm operator using existing chart : Create a new operator project using the SDK Command Line Interface(CLI) Create a new (or add your existing) Helm chart for use by the operator\u2019s reconciling logic Use the SDK CLI to build and generate the operator deployment manifests Optionally add additional CRD\u2019s using the SDK CLI and repeat steps 2 and 3 Use the SDK bundle feature to package the operator for OLM deployment. Deploy, test and publish. In this lab, we will use the IBM Guestbook helm chart available here as the base to scaffold a new operator. Information of creating a new operator can be found here Setup \u00b6 The lab requires you to have the operator-sdk installed. Login into the client CLI following these instructions . Run the command shown below to install the prerequisites: source < ( curl -s https://raw.githubusercontent.com/rojanjose/guestbook-helm-operator/master/scripts/operatorInstall.sh ) Check the command output to ensure the SDK version is correct. ... Checking prereqs version ... Go version: go version go1.14.4 linux/amd64 ----------------------------- Helm version: version.BuildInfo { Version: \"v3.0.3\" , GitCommit: \"ac925eb7279f4a6955df663a0128044a8a6b7593\" , GitTreeState: \"clean\" , GoVersion: \"go1.13.6\" } ----------------------------- Operator-sdk version: operator-sdk version: \"v0.19.2\" , commit: \"4282ce9acdef6d7a1e9f90832db4dc5a212ae850\" , kubernetes version: \"v1.18.2\" , go version: \"go1.13.10 linux/amd64\" Log into your OpenShift cluster . oc login --token = YQ2-mTJIWlz1gsWeI2tsO4CzHBbRSCQbH-IdA3tEFrM --server = https://c100-e.us-east.containers.cloud.ibm.com:32055 Create the operator \u00b6 1. Create a new project \u00b6 Export these environment variables prior to starting the project. export DOCKER_USERNAME = <your-docker-username> export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-operator-project export OPERATOR_VERSION = v1.0.0 export IMG = docker.io/ ${ DOCKER_USERNAME } / ${ OPERATOR_NAME } : ${ OPERATOR_VERSION } Create a new project called guestbook-operator using the existing guestbook helm chart. The guestbook chart is available at the repo https://ibm.github.io/helm101/ . operator-sdk new $OPERATOR_PROJECT --type = helm --helm-chart = guestbook --helm-chart-repo = https://ibm.github.io/helm101/ cd $OPERATOR_PROJECT Output: INFO [ 0000 ] Creating new Helm operator 'guestbook-operator-project' . INFO [ 0000 ] Created helm-charts/guestbook INFO [ 0000 ] Generating RBAC rules I0813 23 :07:00.995286 11211 request.go:621 ] Throttling request took 1 .031369076s, request: GET:https://c100-e.us-east.containers.cloud.ibm.com:31941/apis/scheduling.k8s.io/v1?timeout = 32s WARN [ 0002 ] The RBAC rules generated in deploy/role.yaml are based on the chart 's default manifest. Some rules may be missing for resources that are only enabled with custom values, and some existing rules may be overly broad. Double check the rules generated in deploy/role.yaml to ensure they meet the operator' s permission requirements. INFO [ 0002 ] Created build/Dockerfile INFO [ 0002 ] Created deploy/service_account.yaml INFO [ 0002 ] Created deploy/role.yaml INFO [ 0002 ] Created deploy/role_binding.yaml INFO [ 0002 ] Created deploy/operator.yaml INFO [ 0002 ] Created deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml INFO [ 0002 ] Generated CustomResourceDefinition manifests. INFO [ 0002 ] Project creation complete. Review the code and customize the operator logic as required to obtain the desired results. By default, the Guestbook operator installs the configured helm chart watches the events shown in the watches.yaml . - group : helm.operator-sdk version : v1alpha1 kind : Guestbook chart : helm-charts/guestbook The custom resource (CR) file defines the properties used by operator while it creates an instance of the Guestbook application. These properties are derived from the values.yaml file in the Helm chart. apiVersion : helm.operator-sdk/v1alpha1 kind : Guestbook metadata : name : example-guestbook spec : # Default values copied from <project_dir>/helm-charts/guestbook/values.yaml image : pullPolicy : Always repository : ibmcom/guestbook tag : v1 redis : port : 6379 slaveEnabled : true replicaCount : 2 service : port : 3000 type : LoadBalancer 2. Deploy the CRD \u00b6 Let Kubernetes know about the new custom resource definition (CRD) the operator will be watching. oc create -f deploy/crds/helm.operator-sdk_guestbooks_crd.yaml Verify the CRD install in OpenShift console: Alternatively, query using the following CLI commands: oc get crd guestbooks.helm.operator-sdk oc describe crd guestbooks.helm.operator-sdk 3. Build the code \u00b6 Use the generated Dockerfile under build directory for image build. FROM quay.io/operator-framework/helm-operator:v0.19.2 COPY watches.yaml ${ HOME } /watches.yaml COPY helm-charts/ ${ HOME } /helm-charts/ Run the operator sdk build command to build the image for the helm operator. operator-sdk build ${ IMG } INFO [ 0000 ] Building OCI image docker.io/rojanjose/guestbook-operator:v1.0.0 Sending build context to Docker daemon 41 .98kB Step 1 /3 : FROM quay.io/operator-framework/helm-operator:v0.19.2 v0.19.2: Pulling from operator-framework/helm-operator 41ae95b593e0: Pull complete f20f68829d13: Pull complete 05c2e7d4212e: Pull complete 66213365a0c9: Pull complete 09e5a7e28c6f: Pull complete Digest: sha256:0f1e104719267f687280d8640a6958c61510fae27a6937369c419b0dd2b91564 .... Verify the built image: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE rojanjose/guestbook-operator v1.0.0 d05f5e2c441e 7 seconds ago 200MB quay.io/operator-framework/helm-operator v0.19.2 11862329f28c 2 weeks ago 200MB Log into the docker registry and push image: docker login docker.io -u $DOCKER_USERNAME docker push ${ IMG } Replace the image name string in the operator.yaml file: sed -i 's|REPLACE_IMAGE|' ${ IMG } '|g' deploy/operator.yaml ( MacOS: ) sed -i \"\" 's|REPLACE_IMAGE|' ${ IMG } '|g' deploy/operator.yaml At this stage, the operator can be deployed with the available manifest files, however, we will explore the operator deloy with OLM features. 4. Deploy the Operator with the Operator Lifecycle Manager (OLM) \u00b6 Ensure OLM is enabled on the cluster by running this command: operator-sdk olm status --olm-namespace openshift-operator-lifecycle-manager Expected result: operator-sdk olm status --olm-namespace openshift-operator-lifecycle-manager I0813 23 :36:41.881438 14844 request.go:621 ] Throttling request took 1 .020925705s, request: GET:https://c100-e.us-east.containers.cloud.ibm.com:31941/apis/rbac.authorization.k8s.io/v1beta1?timeout = 32s INFO [ 0002 ] Fetching CRDs for version \"0.13.0\" INFO [ 0002 ] Fetching resources for version \"0.13.0\" INFO [ 0003 ] Successfully got OLM status for version \"0.13.0\" NAME NAMESPACE KIND STATUS installplans.operators.coreos.com CustomResourceDefinition Installed clusterserviceversions.operators.coreos.com CustomResourceDefinition Installed aggregate-olm-view ClusterRole Installed operatorgroups.operators.coreos.com CustomResourceDefinition Installed catalogsources.operators.coreos.com CustomResourceDefinition Installed subscriptions.operators.coreos.com CustomResourceDefinition Installed system:controller:operator-lifecycle-manager ClusterRole Installed aggregate-olm-edit ClusterRole Installed olm-operator-binding-olm ClusterRoleBinding clusterrolebindings.rbac.authorization.k8s.io \"olm-operator-binding-olm\" not found olm-operator-serviceaccount olm ServiceAccount serviceaccounts \"olm-operator-serviceaccount\" not found olm-operator olm Deployment deployments.apps \"olm-operator\" not found catalog-operator olm Deployment deployments.apps \"catalog-operator\" not found operators Namespace namespaces \"operators\" not found olm Namespace namespaces \"olm\" not found global-operators operators OperatorGroup operatorgroups.operators.coreos.com \"global-operators\" not found olm-operators olm OperatorGroup operatorgroups.operators.coreos.com \"olm-operators\" not found packageserver olm ClusterServiceVersion clusterserviceversions.operators.coreos.com \"packageserver\" not found operatorhubio-catalog olm CatalogSource catalogsources.operators.coreos.com \"operatorhubio-catalog\" not found [Note: OLM is partially enabled which is sufficient to complete this lab.] Create a bundle: operator-sdk generate bundle --version 1 .0.0 Output of the command: INFO [ 0000 ] Generating bundle manifests version 1 .0.0 Display name for the operator ( required ) : > Guestbook Operator Description for the operator ( required ) : > Demo helm operator for Guestbook Provider 's name for the operator (required): > IBM Any relevant URL for the provider name (optional): > https://github.com/rojanjose/guestbook-helm-operator Comma-separated list of keywords for your operator (required): > helm,operator,kubernetes,openshift Comma-separated list of maintainers and their emails (e.g. ' name1:email1, name2:email2 ' ) ( required ) : > Rojan:rojanjose@gmail.com INFO [ 0164 ] Bundle manifests generated successfully in deploy/olm-catalog/guestbook-operator-project INFO [ 0164 ] Building annotations.yaml INFO [ 0164 ] Writing annotations.yaml in /Users/operator/guestbook-operator-project/deploy/olm-catalog/guestbook-operator-project/metadata INFO [ 0164 ] Building Dockerfile INFO [ 0164 ] Writing bundle.Dockerfile in /Users/operator/guestbook-operator-project A bundle manifests directory deploy/olm-catalog/guestbook-operator-project/manifests containing a CSV and all CRDs in deploy/crds and a bundle metadata directory deploy/olm-catalog/guestbook-operator-project/metadata are generated. Create Project where operator OLM should be installed: oc new-project guest-operator-ns Output: Now using project \"guest-operator-ns\" on server \"https://c100-e.us-east.containers.cloud.ibm.com:31941\" . You can add applications to this project with the 'new-app' command. For example, try: oc new-app django-psql-example ... Create an OperatorGroup yaml definition: cat <<EOF >>deploy/operator_group.yaml apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: guestbook-og namespace: guest-operator-ns spec: targetNamespaces: - guest-operator-ns EOF Replace placeholder string with project guest-operator-ns in guestbook-operator.clusterserviceversion.yaml sed -i 's#namespace: placeholder#namespace: guest-operator-ns#' deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml or on Mac, sed -i \"\" 's#namespace: placeholder#namespace: guest-operator-ns#' deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml 5. Install the operator \u00b6 Create the Operator group: oc create -f deploy/operator_group.yaml Apply the Operator\u2019s CSV manifest to the specified namespace in the cluster: oc create -f deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml Create the role, role binding, and service account to grant resource permissions to the Operator to create the Guestbook type that the Operator manages: oc create -f deploy/service_account.yaml oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml Wait for few minutes for the Guestbook operator to complete the installation. oc get ClusterServiceVersion ClusterServiceVersion should show a PHASE value of Succeeded , $ oc get ClusterServiceVersion NAME DISPLAY VERSION REPLACES PHASE guestbook-operator-project.v1.0.0 Guestbook Operator 1 .0.0 Succeeded Check the list of Installed Operators under the project guest-operator-ns . Open the operator and validate that install succeeded. Now, create an instance of Guestbook helm chart. Click on Create instance icon. Goto Workloads > Pods to view the pods. You should see 2 frontend pods, 1 Redis master, 2 Redis slave and pod supporting the Guestbook operator OLM. 6. Update the Guestbook application instance \u00b6 Open the deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml and change the value of replicaCount to 4. Save the file and run the oc apply command: oc apply -f deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml Run oc get pods to validate the guesbook pod count: oc get podsNAME READY STATUS RESTARTS AGE example-guestbook-6fdb6776b-hdq64 1 /1 Running 0 41m example-guestbook-6fdb6776b-pktr8 1 /1 Running 0 8s example-guestbook-6fdb6776b-x2nqw 1 /1 Running 0 41m example-guestbook-6fdb6776b-xz8lp 1 /1 Running 0 77s guestbook-operator-project-767cc5686c-ksmxq 1 /1 Running 0 52m redis-master-68857cd57c-pwctp 1 /1 Running 0 41m redis-slave-bbd8d8545-6jk8m 1 /1 Running 0 41m redis-slave-bbd8d8545-k65wz 1 /1 Running 0 41m 7. Clean up \u00b6 Run the oc delete commands to remove the operator. oc delete -f deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml oc delete -f deploy/service_account.yaml oc delete -f deploy/role.yaml oc delete -f deploy/role_binding.yaml oc delete -f deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml oc delete -f deploy/operator_group.yaml oc delete -f deploy/crds/helm.operator-sdk_guestbooks_crd.yaml","title":"Create an Operator using an Existing Helm Chart"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#create-an-operator-using-an-existing-helm-chart","text":"The Operator Framework is an open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator. The Operator SDK provides the tools to build, test and package Operators. The following workflow is for a Helm operator using existing chart : Create a new operator project using the SDK Command Line Interface(CLI) Create a new (or add your existing) Helm chart for use by the operator\u2019s reconciling logic Use the SDK CLI to build and generate the operator deployment manifests Optionally add additional CRD\u2019s using the SDK CLI and repeat steps 2 and 3 Use the SDK bundle feature to package the operator for OLM deployment. Deploy, test and publish. In this lab, we will use the IBM Guestbook helm chart available here as the base to scaffold a new operator. Information of creating a new operator can be found here","title":"Create an Operator using an Existing Helm Chart"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#setup","text":"The lab requires you to have the operator-sdk installed. Login into the client CLI following these instructions . Run the command shown below to install the prerequisites: source < ( curl -s https://raw.githubusercontent.com/rojanjose/guestbook-helm-operator/master/scripts/operatorInstall.sh ) Check the command output to ensure the SDK version is correct. ... Checking prereqs version ... Go version: go version go1.14.4 linux/amd64 ----------------------------- Helm version: version.BuildInfo { Version: \"v3.0.3\" , GitCommit: \"ac925eb7279f4a6955df663a0128044a8a6b7593\" , GitTreeState: \"clean\" , GoVersion: \"go1.13.6\" } ----------------------------- Operator-sdk version: operator-sdk version: \"v0.19.2\" , commit: \"4282ce9acdef6d7a1e9f90832db4dc5a212ae850\" , kubernetes version: \"v1.18.2\" , go version: \"go1.13.10 linux/amd64\" Log into your OpenShift cluster . oc login --token = YQ2-mTJIWlz1gsWeI2tsO4CzHBbRSCQbH-IdA3tEFrM --server = https://c100-e.us-east.containers.cloud.ibm.com:32055","title":"Setup"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#create-the-operator","text":"","title":"Create the operator"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#1-create-a-new-project","text":"Export these environment variables prior to starting the project. export DOCKER_USERNAME = <your-docker-username> export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-operator-project export OPERATOR_VERSION = v1.0.0 export IMG = docker.io/ ${ DOCKER_USERNAME } / ${ OPERATOR_NAME } : ${ OPERATOR_VERSION } Create a new project called guestbook-operator using the existing guestbook helm chart. The guestbook chart is available at the repo https://ibm.github.io/helm101/ . operator-sdk new $OPERATOR_PROJECT --type = helm --helm-chart = guestbook --helm-chart-repo = https://ibm.github.io/helm101/ cd $OPERATOR_PROJECT Output: INFO [ 0000 ] Creating new Helm operator 'guestbook-operator-project' . INFO [ 0000 ] Created helm-charts/guestbook INFO [ 0000 ] Generating RBAC rules I0813 23 :07:00.995286 11211 request.go:621 ] Throttling request took 1 .031369076s, request: GET:https://c100-e.us-east.containers.cloud.ibm.com:31941/apis/scheduling.k8s.io/v1?timeout = 32s WARN [ 0002 ] The RBAC rules generated in deploy/role.yaml are based on the chart 's default manifest. Some rules may be missing for resources that are only enabled with custom values, and some existing rules may be overly broad. Double check the rules generated in deploy/role.yaml to ensure they meet the operator' s permission requirements. INFO [ 0002 ] Created build/Dockerfile INFO [ 0002 ] Created deploy/service_account.yaml INFO [ 0002 ] Created deploy/role.yaml INFO [ 0002 ] Created deploy/role_binding.yaml INFO [ 0002 ] Created deploy/operator.yaml INFO [ 0002 ] Created deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml INFO [ 0002 ] Generated CustomResourceDefinition manifests. INFO [ 0002 ] Project creation complete. Review the code and customize the operator logic as required to obtain the desired results. By default, the Guestbook operator installs the configured helm chart watches the events shown in the watches.yaml . - group : helm.operator-sdk version : v1alpha1 kind : Guestbook chart : helm-charts/guestbook The custom resource (CR) file defines the properties used by operator while it creates an instance of the Guestbook application. These properties are derived from the values.yaml file in the Helm chart. apiVersion : helm.operator-sdk/v1alpha1 kind : Guestbook metadata : name : example-guestbook spec : # Default values copied from <project_dir>/helm-charts/guestbook/values.yaml image : pullPolicy : Always repository : ibmcom/guestbook tag : v1 redis : port : 6379 slaveEnabled : true replicaCount : 2 service : port : 3000 type : LoadBalancer","title":"1. Create a new project"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#2-deploy-the-crd","text":"Let Kubernetes know about the new custom resource definition (CRD) the operator will be watching. oc create -f deploy/crds/helm.operator-sdk_guestbooks_crd.yaml Verify the CRD install in OpenShift console: Alternatively, query using the following CLI commands: oc get crd guestbooks.helm.operator-sdk oc describe crd guestbooks.helm.operator-sdk","title":"2. Deploy the CRD"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#3-build-the-code","text":"Use the generated Dockerfile under build directory for image build. FROM quay.io/operator-framework/helm-operator:v0.19.2 COPY watches.yaml ${ HOME } /watches.yaml COPY helm-charts/ ${ HOME } /helm-charts/ Run the operator sdk build command to build the image for the helm operator. operator-sdk build ${ IMG } INFO [ 0000 ] Building OCI image docker.io/rojanjose/guestbook-operator:v1.0.0 Sending build context to Docker daemon 41 .98kB Step 1 /3 : FROM quay.io/operator-framework/helm-operator:v0.19.2 v0.19.2: Pulling from operator-framework/helm-operator 41ae95b593e0: Pull complete f20f68829d13: Pull complete 05c2e7d4212e: Pull complete 66213365a0c9: Pull complete 09e5a7e28c6f: Pull complete Digest: sha256:0f1e104719267f687280d8640a6958c61510fae27a6937369c419b0dd2b91564 .... Verify the built image: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE rojanjose/guestbook-operator v1.0.0 d05f5e2c441e 7 seconds ago 200MB quay.io/operator-framework/helm-operator v0.19.2 11862329f28c 2 weeks ago 200MB Log into the docker registry and push image: docker login docker.io -u $DOCKER_USERNAME docker push ${ IMG } Replace the image name string in the operator.yaml file: sed -i 's|REPLACE_IMAGE|' ${ IMG } '|g' deploy/operator.yaml ( MacOS: ) sed -i \"\" 's|REPLACE_IMAGE|' ${ IMG } '|g' deploy/operator.yaml At this stage, the operator can be deployed with the available manifest files, however, we will explore the operator deloy with OLM features.","title":"3. Build the code"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#4-deploy-the-operator-with-the-operator-lifecycle-manager-olm","text":"Ensure OLM is enabled on the cluster by running this command: operator-sdk olm status --olm-namespace openshift-operator-lifecycle-manager Expected result: operator-sdk olm status --olm-namespace openshift-operator-lifecycle-manager I0813 23 :36:41.881438 14844 request.go:621 ] Throttling request took 1 .020925705s, request: GET:https://c100-e.us-east.containers.cloud.ibm.com:31941/apis/rbac.authorization.k8s.io/v1beta1?timeout = 32s INFO [ 0002 ] Fetching CRDs for version \"0.13.0\" INFO [ 0002 ] Fetching resources for version \"0.13.0\" INFO [ 0003 ] Successfully got OLM status for version \"0.13.0\" NAME NAMESPACE KIND STATUS installplans.operators.coreos.com CustomResourceDefinition Installed clusterserviceversions.operators.coreos.com CustomResourceDefinition Installed aggregate-olm-view ClusterRole Installed operatorgroups.operators.coreos.com CustomResourceDefinition Installed catalogsources.operators.coreos.com CustomResourceDefinition Installed subscriptions.operators.coreos.com CustomResourceDefinition Installed system:controller:operator-lifecycle-manager ClusterRole Installed aggregate-olm-edit ClusterRole Installed olm-operator-binding-olm ClusterRoleBinding clusterrolebindings.rbac.authorization.k8s.io \"olm-operator-binding-olm\" not found olm-operator-serviceaccount olm ServiceAccount serviceaccounts \"olm-operator-serviceaccount\" not found olm-operator olm Deployment deployments.apps \"olm-operator\" not found catalog-operator olm Deployment deployments.apps \"catalog-operator\" not found operators Namespace namespaces \"operators\" not found olm Namespace namespaces \"olm\" not found global-operators operators OperatorGroup operatorgroups.operators.coreos.com \"global-operators\" not found olm-operators olm OperatorGroup operatorgroups.operators.coreos.com \"olm-operators\" not found packageserver olm ClusterServiceVersion clusterserviceversions.operators.coreos.com \"packageserver\" not found operatorhubio-catalog olm CatalogSource catalogsources.operators.coreos.com \"operatorhubio-catalog\" not found [Note: OLM is partially enabled which is sufficient to complete this lab.] Create a bundle: operator-sdk generate bundle --version 1 .0.0 Output of the command: INFO [ 0000 ] Generating bundle manifests version 1 .0.0 Display name for the operator ( required ) : > Guestbook Operator Description for the operator ( required ) : > Demo helm operator for Guestbook Provider 's name for the operator (required): > IBM Any relevant URL for the provider name (optional): > https://github.com/rojanjose/guestbook-helm-operator Comma-separated list of keywords for your operator (required): > helm,operator,kubernetes,openshift Comma-separated list of maintainers and their emails (e.g. ' name1:email1, name2:email2 ' ) ( required ) : > Rojan:rojanjose@gmail.com INFO [ 0164 ] Bundle manifests generated successfully in deploy/olm-catalog/guestbook-operator-project INFO [ 0164 ] Building annotations.yaml INFO [ 0164 ] Writing annotations.yaml in /Users/operator/guestbook-operator-project/deploy/olm-catalog/guestbook-operator-project/metadata INFO [ 0164 ] Building Dockerfile INFO [ 0164 ] Writing bundle.Dockerfile in /Users/operator/guestbook-operator-project A bundle manifests directory deploy/olm-catalog/guestbook-operator-project/manifests containing a CSV and all CRDs in deploy/crds and a bundle metadata directory deploy/olm-catalog/guestbook-operator-project/metadata are generated. Create Project where operator OLM should be installed: oc new-project guest-operator-ns Output: Now using project \"guest-operator-ns\" on server \"https://c100-e.us-east.containers.cloud.ibm.com:31941\" . You can add applications to this project with the 'new-app' command. For example, try: oc new-app django-psql-example ... Create an OperatorGroup yaml definition: cat <<EOF >>deploy/operator_group.yaml apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: guestbook-og namespace: guest-operator-ns spec: targetNamespaces: - guest-operator-ns EOF Replace placeholder string with project guest-operator-ns in guestbook-operator.clusterserviceversion.yaml sed -i 's#namespace: placeholder#namespace: guest-operator-ns#' deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml or on Mac, sed -i \"\" 's#namespace: placeholder#namespace: guest-operator-ns#' deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml","title":"4. Deploy the Operator with the Operator Lifecycle Manager (OLM)"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#5-install-the-operator","text":"Create the Operator group: oc create -f deploy/operator_group.yaml Apply the Operator\u2019s CSV manifest to the specified namespace in the cluster: oc create -f deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml Create the role, role binding, and service account to grant resource permissions to the Operator to create the Guestbook type that the Operator manages: oc create -f deploy/service_account.yaml oc create -f deploy/role.yaml oc create -f deploy/role_binding.yaml Wait for few minutes for the Guestbook operator to complete the installation. oc get ClusterServiceVersion ClusterServiceVersion should show a PHASE value of Succeeded , $ oc get ClusterServiceVersion NAME DISPLAY VERSION REPLACES PHASE guestbook-operator-project.v1.0.0 Guestbook Operator 1 .0.0 Succeeded Check the list of Installed Operators under the project guest-operator-ns . Open the operator and validate that install succeeded. Now, create an instance of Guestbook helm chart. Click on Create instance icon. Goto Workloads > Pods to view the pods. You should see 2 frontend pods, 1 Redis master, 2 Redis slave and pod supporting the Guestbook operator OLM.","title":"5. Install the operator"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#6-update-the-guestbook-application-instance","text":"Open the deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml and change the value of replicaCount to 4. Save the file and run the oc apply command: oc apply -f deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml Run oc get pods to validate the guesbook pod count: oc get podsNAME READY STATUS RESTARTS AGE example-guestbook-6fdb6776b-hdq64 1 /1 Running 0 41m example-guestbook-6fdb6776b-pktr8 1 /1 Running 0 8s example-guestbook-6fdb6776b-x2nqw 1 /1 Running 0 41m example-guestbook-6fdb6776b-xz8lp 1 /1 Running 0 77s guestbook-operator-project-767cc5686c-ksmxq 1 /1 Running 0 52m redis-master-68857cd57c-pwctp 1 /1 Running 0 41m redis-slave-bbd8d8545-6jk8m 1 /1 Running 0 41m redis-slave-bbd8d8545-k65wz 1 /1 Running 0 41m","title":"6. Update the Guestbook application instance"},{"location":"generatedContent/kubernetes-operators/lab3-v0.19/#7-clean-up","text":"Run the oc delete commands to remove the operator. oc delete -f deploy/crds/helm.operator-sdk_v1alpha1_guestbook_cr.yaml oc delete -f deploy/service_account.yaml oc delete -f deploy/role.yaml oc delete -f deploy/role_binding.yaml oc delete -f deploy/olm-catalog/guestbook-operator-project/manifests/guestbook-operator-project.clusterserviceversion.yaml oc delete -f deploy/operator_group.yaml oc delete -f deploy/crds/helm.operator-sdk_guestbooks_crd.yaml","title":"7. Clean up"},{"location":"generatedContent/kubernetes-operators/lab3/","text":"Create an Operator using an Existing Helm Chart \u00b6 The Operator Framework is an open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator. The Operator SDK provides the tools to build, test and package Operators. The following workflow is to build an operator using an existing Helm chart : Create a new operator project and initialize it using the SDK Command Line Interface(CLI) Create the API to generate the CRD files for the chart. Build the Operator container image and push it to a registry. Apply the CRD in the cluster and deploy the operator image. Deploy the operand by applying the custom resource (CR) into the cluster. Cleanup the deployment. In this lab, we will use the IBM Guestbook helm chart available here as the base to scaffold a new operator. Information on creating a new operator can be found here Operator SDK made several technology and architecture changes with the release of v1.0 which as listed here . Setup \u00b6 The following must be done before you can get started on the lab: Create your lab environment by following the steps found here The lab requires a newer version of the operator-sdk installed. In the lab terminal, run the commands shown below to install the prerequisites: source < ( curl -s https://raw.githubusercontent.com/ibm/kubernetes-operators/master/src/scripts/operatorInstall.sh ) export PATH = \" ${ HOME } /bin: ${ PATH } \" $ source <(curl -s https://raw.githubusercontent.com/ibm/kubernetes-operators/master/src/scripts/operatorInstall.sh) Downloading operaror-sdk-v1.3.0-linux_amd64 ... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 632 100 632 0 0 1876 0 --:--:-- --:--:-- --:--:-- 1875 100 64.8M 100 64.8M 0 0 61.9M 0 0:00:01 0:00:01 --:--:-- 61.9M operaror-sdk-v1.3.0-linux_amd64 downloaded. ...... Run a version check after the operator-sdk installation is complete: operator-sdk version $ operator-sdk version operator-sdk version: \"v1.3.0\", commit: \"1abf57985b43bf6a59dcd18147b3c574fa57d3f6\", kubernetes version: \"1.19.4\", go version: \"go1.15.5\", GOOS: \"linux\", GOARCH: \"amd64\" Log into the OpenShift cluster: Scroll down on the Quick Links and Common commands page until you see a terminal command block with green text and a description above it that says Log in to your OpenShift cluster. Click on the command and it will automatically paste into your terminal and execute. This lab uses docker registry to container image storage. Create a new docker hub id, if you do not have one. Create the operator \u00b6 1. Create a new project & initialize it using SDK \u00b6 Certain parameters will be used repetitively. Export these parameters as environment variables prior to starting the project. Replace <your-docker-username> with your docker hub id. export DOCKER_USERNAME = <your-docker-username> Set names for the operator, project and operator version. The operator container images is built using these values. export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-operator-project export OPERATOR_VERSION = v1.0.0 export IMAGE = docker.io/ ${ DOCKER_USERNAME } / ${ OPERATOR_NAME } : ${ OPERATOR_VERSION } Create the project directory for the operator. mkdir -p ${ OPERATOR_PROJECT } cd ${ OPERATOR_PROJECT } Use the operator SDK to initialize the project. Specify the plugin and API group as the parameters for this command. operator-sdk init --plugins = helm --domain guestbook.ibm.com $ operator-sdk init --plugins=helm --domain guestbook.ibm.com Next: define a resource with: $ operator-sdk create api The initialization step create a scaffolding with the operator boiler plate code. At high level, this creates the config directory, watches.yaml and the place holder for the helm chart. Use the command tree . to view the complete directory structure as shown in the block below: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 PROJECT \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 default \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager_auth_proxy_patch.yaml \u2502 \u251c\u2500\u2500 manager \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager.yaml \u2502 \u251c\u2500\u2500 prometheus \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 monitor.yaml \u2502 \u251c\u2500\u2500 rbac \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_service.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 role.yaml \u2502 \u2502 \u2514\u2500\u2500 role_binding.yaml \u2502 \u2514\u2500\u2500 scorecard \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2514\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 patches \u2502 \u251c\u2500\u2500 basic.config.yaml \u2502 \u2514\u2500\u2500 olm.config.yaml \u251c\u2500\u2500 helm-charts \u2514\u2500\u2500 watches.yaml Operator SDK uses the kubernetes Kustomize tool for managing the deployment of yaml files, hence you see the kustomization.yaml in all the directories. config/default and confg/manager contains the specification to inject the controller manager container into the operator pod as a side car. The confg/rbac folder contains a set of default access control rules. Review the Makefile to understand the operator-sdk , kustomize and docker commands executed for various tasks. 2. Create the API to generate the CRD files for the chart. \u00b6 Next step, create the API artifacts. Provide the name and the location of the helm chart as input parameters to this command. This command will create the crd folder with the custom resource definition for the Guestbook operator. The command picks the latest version of the helm chart, if the helm version parameter is ignored. operator-sdk create api --helm-chart=guestbook --helm-chart-repo=https://raw.githubusercontent.com/IBM/helm101/master/ operator-sdk create api --helm-chart=guestbook --helm-chart-repo=https://raw.githubusercontent.com/IBM/helm101/master/ Created helm-charts/guestbook Generating RBAC rules I0202 15:46:05.545032 48799 request.go:645] Throttling request took 1.005544854s, request: GET:https://c107-e.us-south.containers.cloud.ibm.com:30606/apis/extensions/v1beta1?timeout=32s WARN[0003] The RBAC rules generated in config/rbac/role.yaml are based on the chart's default manifest. Some rules may be missing for resources that are only enabled with custom values, and some existing rules may be overly broad. Double check the rules generated in config/rbac/role.yaml to ensure they meet the operator's permission requirements. Check the new additions to the scaffolding using the tree . command: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 PROJECT \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 crd \u2502 \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2502 \u2514\u2500\u2500 charts.guestbook.ibm.com_guestbooks.yaml \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 default \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager_auth_proxy_patch.yaml \u2502 \u251c\u2500\u2500 manager \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager.yaml \u2502 \u251c\u2500\u2500 prometheus \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 monitor.yaml \u2502 \u251c\u2500\u2500 rbac \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_service.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook_editor_role.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook_viewer_role.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 role.yaml \u2502 \u2502 \u2514\u2500\u2500 role_binding.yaml \u2502 \u251c\u2500\u2500 samples \u2502 \u2502 \u2514\u2500\u2500 charts_v1alpha1_guestbook.yaml \u2502 \u2514\u2500\u2500 scorecard \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2514\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 patches \u2502 \u251c\u2500\u2500 basic.config.yaml \u2502 \u2514\u2500\u2500 olm.config.yaml \u251c\u2500\u2500 helm-charts \u2502 \u2514\u2500\u2500 guestbook \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 LICENSE \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook-service.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-master-service.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \u2502 \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \u2502 \u2514\u2500\u2500 values.yaml \u2514\u2500\u2500 watches.yaml View the contents of the CRD. Note the values for names and schema.openAPIV3Schema.properties . more config/crd/bases/charts.guestbook.ibm.com_guestbooks.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.charts.guestbook.ibm.com spec: group: charts.guestbook.ibm.com names: kind: Guestbook listKind: GuestbookList plural: guestbooks singular: guestbook scope: Namespaced versions: - name: v1alpha1 schema: openAPIV3Schema: ... 3. Build the Operator container image and push it to registry. \u00b6 Login into the docker registry using your personal id and password. docker login docker.io -u $DOCKER_USERNAME $ docker login docker.io -u $DOCKER_USERNAME Password: WARNING! Your password will be stored unencrypted in /home/student/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Build the Guestbook operator container image and push image to the docker hub registry. make docker-build docker-push IMG = ${ IMAGE } make docker-build docker-push IMG=${IMAGE} docker build . -t docker.io/rojanjose/guestbook-operator:v1.0.0 [+] Building 4.2s (9/9) FINISHED => [internal] load .dockerignore 0.0s => => transferring context: 2B 0.0s => [internal] load build definition from Dockerfile 0.0s => => transferring dockerfile: 237B ........... ........... 753e76240780: Pushed 4a3bef90e857: Pushed d0e9a59c2057: Pushed 1d8db7e222a6: Pushed 00af10937683: Pushed 3aa55ff7bca1: Pushed v1.0.0: digest: sha256:c0724c7f31a748094621b7623a81fae107511c23819b729f25878f7e5a7377dd size: 1984 You can view the local docker images by running: docker images $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE rojanjose/guestbook-operator v1.0.0 590c0196c2b6 10 seconds ago 160MB quay.io/operator-framework/helm-operator v1.3.0 57683a970d10 6 weeks ago 160MB 4. Apply the CRD in the cluster and deploy the operator image. \u00b6 Install the Guestbook customer resource definition using the make install command: make install make install /home/student/guestbook-operator-project/bin/kustomize build config/crd | kubectl apply -f - customresourcedefinition.apiextensions.k8s.io/guestbooks.charts.guestbook.ibm.com created View the deployed CRD oc describe CustomResourceDefinition guestbooks.charts.guestbook.ibm.com Next step is to deploy the operator. Note that the operator is installed in its own namespace guestbook-operator-project-system . make deploy IMG=${IMAGE} $ make deploy IMG=${IMAGE} cd config/manager && /home/student/guestbook-operator-project/bin/kustomize edit set image controller=docker.io/rojanjose/guestbook-operator:v1.0.0 /home/student/guestbook-operator-project/bin/kustomize build config/default | kubectl apply -f - namespace/guestbook-operator-project-system created customresourcedefinition.apiextensions.k8s.io/guestbooks.charts.guestbook.ibm.com unchanged role.rbac.authorization.k8s.io/guestbook-operator-project-leader-election-role created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-manager-role created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-metrics-reader created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-proxy-role created rolebinding.rbac.authorization.k8s.io/guestbook-operator-project-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/guestbook-operator-project-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/guestbook-operator-project-proxy-rolebinding created service/guestbook-operator-project-controller-manager-metrics-service created deployment.apps/guestbook-operator-project-controller-manager created View of what got deployed: oc get all -n ${ OPERATOR_PROJECT } -system $ oc get all -n ${OPERATOR_PROJECT}-system NAME READY STATUS RESTARTS AGE pod/guestbook-operator-project-controller-manager-7bc6f986dd-2r898 2/2 Running 0 2m24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-operator-project-controller-manager-metrics-service ClusterIP 172.21.110.64 <none> 8443/TCP 2m24s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-operator-project-controller-manager 1/1 1 1 2m24s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-operator-project-controller-manager-7bc6f986dd 1 1 1 2m24s 5. Deploy the operand by applying the custom resource (CR) into the cluster. \u00b6 Create a new project called guestbook where Guestbook application will be deployed. oc new-project guestbook $ oc new-project guestbook Now using project \"guestbook\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:30606\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app ruby~https://github.com/sclorg/ruby-ex.git to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node An example custom resource yaml file was automatically generated under the config/samples directory as part of the create API step earlier. This is based on the default values.yaml from the Guestbook helm chart under helm-charts/guestbook/values.yaml . Let's use this file to create the operand. oc apply -f config/samples/charts_v1alpha1_guestbook.yaml $ oc apply -f config/samples/charts_v1alpha1_guestbook.yaml guestbook.charts.guestbook.ibm.com/guestbook-sample created Outcome of the operand deploy can be viewed by running the command: oc get all -n guestbook $ oc get all -n guestbook NAME READY STATUS RESTARTS AGE pod/guestbook-sample-8594c8dc46-bl7sm 1/1 Running 0 75s pod/guestbook-sample-8594c8dc46-qwgkv 1/1 Running 0 75s pod/redis-master-68857cd57c-bjxt5 1/1 Running 0 75s pod/redis-slave-bbd8d8545-57944 1/1 Running 0 75s pod/redis-slave-bbd8d8545-rxqc5 1/1 Running 0 75s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-sample LoadBalancer 172.21.41.67 169.45.217.90 3000:30940/TCP 75s service/redis-master ClusterIP 172.21.206.242 <none> 6379/TCP 75s service/redis-slave ClusterIP 172.21.133.74 <none> 6379/TCP 75s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-sample 2/2 2 2 75s deployment.apps/redis-master 1/1 1 1 75s deployment.apps/redis-slave 2/2 2 2 75s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-sample-8594c8dc46 2 2 2 75s replicaset.apps/redis-master-68857cd57c 1 1 1 75s replicaset.apps/redis-slave-bbd8d8545 2 2 2 75s Validate the Guestbook application is running by accessing it with the following commands: HOSTNAME = ` oc get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` oc get svc guestbook-sample -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" http://169.45.242.104:30940 Copy and paste the above URL in a new browser tab to get to the Guestbook landing page as shown below: Open the OpenShift console to view the artificats created as part of this exerice. Guestbook operator pod: Guestbook application pods: 6. Cleanup the deployment. \u00b6 Remove the Guestbook application by deleting the customer resource (CR). oc delete -f config/samples/charts_v1alpha1_guestbook.yaml $ oc delete -f config/samples/charts_v1alpha1_guestbook.yaml guestbook.charts.guestbook.ibm.com \"guestbook-sample\" deleted $ oc get all -n guestbook No resources found in guestbook namespace. $ oc delete project guestbook project.project.openshift.io \"guestbook\" deleted Finally, to delete the CRDs, run the make undeploy command: make undeploy $ make undeploy /home/student/guestbook-operator-project/bin/kustomize build config/default | kubectl delete -f - namespace \"gb-helm-operator-system\" deleted customresourcedefinition.apiextensions.k8s.io \"guestbooks.charts.guestbook.ibm.com\" deleted role.rbac.authorization.k8s.io \"gb-helm-operator-leader-election-role\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-manager-role\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-metrics-reader\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-proxy-role\" deleted rolebinding.rbac.authorization.k8s.io \"gb-helm-operator-leader-election-rolebinding\" deleted clusterrolebinding.rbac.authorization.k8s.io \"gb-helm-operator-manager-rolebinding\" deleted clusterrolebinding.rbac.authorization.k8s.io \"gb-helm-operator-proxy-rolebinding\" deleted service \"gb-helm-operator-controller-manager-metrics-service\" deleted deployment.apps \"gb-helm-operator-controller-manager\" deleted This concludes the Helm Operator lab. Go here for additional information on building operators using Helm.","title":"Create an Operator using an Existing Helm Chart"},{"location":"generatedContent/kubernetes-operators/lab3/#create-an-operator-using-an-existing-helm-chart","text":"The Operator Framework is an open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator. The Operator SDK provides the tools to build, test and package Operators. The following workflow is to build an operator using an existing Helm chart : Create a new operator project and initialize it using the SDK Command Line Interface(CLI) Create the API to generate the CRD files for the chart. Build the Operator container image and push it to a registry. Apply the CRD in the cluster and deploy the operator image. Deploy the operand by applying the custom resource (CR) into the cluster. Cleanup the deployment. In this lab, we will use the IBM Guestbook helm chart available here as the base to scaffold a new operator. Information on creating a new operator can be found here Operator SDK made several technology and architecture changes with the release of v1.0 which as listed here .","title":"Create an Operator using an Existing Helm Chart"},{"location":"generatedContent/kubernetes-operators/lab3/#setup","text":"The following must be done before you can get started on the lab: Create your lab environment by following the steps found here The lab requires a newer version of the operator-sdk installed. In the lab terminal, run the commands shown below to install the prerequisites: source < ( curl -s https://raw.githubusercontent.com/ibm/kubernetes-operators/master/src/scripts/operatorInstall.sh ) export PATH = \" ${ HOME } /bin: ${ PATH } \" $ source <(curl -s https://raw.githubusercontent.com/ibm/kubernetes-operators/master/src/scripts/operatorInstall.sh) Downloading operaror-sdk-v1.3.0-linux_amd64 ... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 632 100 632 0 0 1876 0 --:--:-- --:--:-- --:--:-- 1875 100 64.8M 100 64.8M 0 0 61.9M 0 0:00:01 0:00:01 --:--:-- 61.9M operaror-sdk-v1.3.0-linux_amd64 downloaded. ...... Run a version check after the operator-sdk installation is complete: operator-sdk version $ operator-sdk version operator-sdk version: \"v1.3.0\", commit: \"1abf57985b43bf6a59dcd18147b3c574fa57d3f6\", kubernetes version: \"1.19.4\", go version: \"go1.15.5\", GOOS: \"linux\", GOARCH: \"amd64\" Log into the OpenShift cluster: Scroll down on the Quick Links and Common commands page until you see a terminal command block with green text and a description above it that says Log in to your OpenShift cluster. Click on the command and it will automatically paste into your terminal and execute. This lab uses docker registry to container image storage. Create a new docker hub id, if you do not have one.","title":"Setup"},{"location":"generatedContent/kubernetes-operators/lab3/#create-the-operator","text":"","title":"Create the operator"},{"location":"generatedContent/kubernetes-operators/lab3/#1-create-a-new-project-initialize-it-using-sdk","text":"Certain parameters will be used repetitively. Export these parameters as environment variables prior to starting the project. Replace <your-docker-username> with your docker hub id. export DOCKER_USERNAME = <your-docker-username> Set names for the operator, project and operator version. The operator container images is built using these values. export OPERATOR_NAME = guestbook-operator export OPERATOR_PROJECT = guestbook-operator-project export OPERATOR_VERSION = v1.0.0 export IMAGE = docker.io/ ${ DOCKER_USERNAME } / ${ OPERATOR_NAME } : ${ OPERATOR_VERSION } Create the project directory for the operator. mkdir -p ${ OPERATOR_PROJECT } cd ${ OPERATOR_PROJECT } Use the operator SDK to initialize the project. Specify the plugin and API group as the parameters for this command. operator-sdk init --plugins = helm --domain guestbook.ibm.com $ operator-sdk init --plugins=helm --domain guestbook.ibm.com Next: define a resource with: $ operator-sdk create api The initialization step create a scaffolding with the operator boiler plate code. At high level, this creates the config directory, watches.yaml and the place holder for the helm chart. Use the command tree . to view the complete directory structure as shown in the block below: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 PROJECT \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 default \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager_auth_proxy_patch.yaml \u2502 \u251c\u2500\u2500 manager \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager.yaml \u2502 \u251c\u2500\u2500 prometheus \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 monitor.yaml \u2502 \u251c\u2500\u2500 rbac \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_service.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 role.yaml \u2502 \u2502 \u2514\u2500\u2500 role_binding.yaml \u2502 \u2514\u2500\u2500 scorecard \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2514\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 patches \u2502 \u251c\u2500\u2500 basic.config.yaml \u2502 \u2514\u2500\u2500 olm.config.yaml \u251c\u2500\u2500 helm-charts \u2514\u2500\u2500 watches.yaml Operator SDK uses the kubernetes Kustomize tool for managing the deployment of yaml files, hence you see the kustomization.yaml in all the directories. config/default and confg/manager contains the specification to inject the controller manager container into the operator pod as a side car. The confg/rbac folder contains a set of default access control rules. Review the Makefile to understand the operator-sdk , kustomize and docker commands executed for various tasks.","title":"1. Create a new project &amp; initialize it using SDK"},{"location":"generatedContent/kubernetes-operators/lab3/#2-create-the-api-to-generate-the-crd-files-for-the-chart","text":"Next step, create the API artifacts. Provide the name and the location of the helm chart as input parameters to this command. This command will create the crd folder with the custom resource definition for the Guestbook operator. The command picks the latest version of the helm chart, if the helm version parameter is ignored. operator-sdk create api --helm-chart=guestbook --helm-chart-repo=https://raw.githubusercontent.com/IBM/helm101/master/ operator-sdk create api --helm-chart=guestbook --helm-chart-repo=https://raw.githubusercontent.com/IBM/helm101/master/ Created helm-charts/guestbook Generating RBAC rules I0202 15:46:05.545032 48799 request.go:645] Throttling request took 1.005544854s, request: GET:https://c107-e.us-south.containers.cloud.ibm.com:30606/apis/extensions/v1beta1?timeout=32s WARN[0003] The RBAC rules generated in config/rbac/role.yaml are based on the chart's default manifest. Some rules may be missing for resources that are only enabled with custom values, and some existing rules may be overly broad. Double check the rules generated in config/rbac/role.yaml to ensure they meet the operator's permission requirements. Check the new additions to the scaffolding using the tree . command: . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 PROJECT \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 crd \u2502 \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2502 \u2514\u2500\u2500 charts.guestbook.ibm.com_guestbooks.yaml \u2502 \u2502 \u2514\u2500\u2500 kustomization.yaml \u2502 \u251c\u2500\u2500 default \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager_auth_proxy_patch.yaml \u2502 \u251c\u2500\u2500 manager \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 manager.yaml \u2502 \u251c\u2500\u2500 prometheus \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u2514\u2500\u2500 monitor.yaml \u2502 \u251c\u2500\u2500 rbac \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_client_clusterrole.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 auth_proxy_service.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook_editor_role.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook_viewer_role.yaml \u2502 \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role.yaml \u2502 \u2502 \u251c\u2500\u2500 leader_election_role_binding.yaml \u2502 \u2502 \u251c\u2500\u2500 role.yaml \u2502 \u2502 \u2514\u2500\u2500 role_binding.yaml \u2502 \u251c\u2500\u2500 samples \u2502 \u2502 \u2514\u2500\u2500 charts_v1alpha1_guestbook.yaml \u2502 \u2514\u2500\u2500 scorecard \u2502 \u251c\u2500\u2500 bases \u2502 \u2502 \u2514\u2500\u2500 config.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 patches \u2502 \u251c\u2500\u2500 basic.config.yaml \u2502 \u2514\u2500\u2500 olm.config.yaml \u251c\u2500\u2500 helm-charts \u2502 \u2514\u2500\u2500 guestbook \u2502 \u251c\u2500\u2500 Chart.yaml \u2502 \u251c\u2500\u2500 LICENSE \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 guestbook-service.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-master-service.yaml \u2502 \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \u2502 \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \u2502 \u2514\u2500\u2500 values.yaml \u2514\u2500\u2500 watches.yaml View the contents of the CRD. Note the values for names and schema.openAPIV3Schema.properties . more config/crd/bases/charts.guestbook.ibm.com_guestbooks.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: guestbooks.charts.guestbook.ibm.com spec: group: charts.guestbook.ibm.com names: kind: Guestbook listKind: GuestbookList plural: guestbooks singular: guestbook scope: Namespaced versions: - name: v1alpha1 schema: openAPIV3Schema: ...","title":"2. Create the API to generate the CRD files for the chart."},{"location":"generatedContent/kubernetes-operators/lab3/#3-build-the-operator-container-image-and-push-it-to-registry","text":"Login into the docker registry using your personal id and password. docker login docker.io -u $DOCKER_USERNAME $ docker login docker.io -u $DOCKER_USERNAME Password: WARNING! Your password will be stored unencrypted in /home/student/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Build the Guestbook operator container image and push image to the docker hub registry. make docker-build docker-push IMG = ${ IMAGE } make docker-build docker-push IMG=${IMAGE} docker build . -t docker.io/rojanjose/guestbook-operator:v1.0.0 [+] Building 4.2s (9/9) FINISHED => [internal] load .dockerignore 0.0s => => transferring context: 2B 0.0s => [internal] load build definition from Dockerfile 0.0s => => transferring dockerfile: 237B ........... ........... 753e76240780: Pushed 4a3bef90e857: Pushed d0e9a59c2057: Pushed 1d8db7e222a6: Pushed 00af10937683: Pushed 3aa55ff7bca1: Pushed v1.0.0: digest: sha256:c0724c7f31a748094621b7623a81fae107511c23819b729f25878f7e5a7377dd size: 1984 You can view the local docker images by running: docker images $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE rojanjose/guestbook-operator v1.0.0 590c0196c2b6 10 seconds ago 160MB quay.io/operator-framework/helm-operator v1.3.0 57683a970d10 6 weeks ago 160MB","title":"3. Build the Operator container image and push it to registry."},{"location":"generatedContent/kubernetes-operators/lab3/#4-apply-the-crd-in-the-cluster-and-deploy-the-operator-image","text":"Install the Guestbook customer resource definition using the make install command: make install make install /home/student/guestbook-operator-project/bin/kustomize build config/crd | kubectl apply -f - customresourcedefinition.apiextensions.k8s.io/guestbooks.charts.guestbook.ibm.com created View the deployed CRD oc describe CustomResourceDefinition guestbooks.charts.guestbook.ibm.com Next step is to deploy the operator. Note that the operator is installed in its own namespace guestbook-operator-project-system . make deploy IMG=${IMAGE} $ make deploy IMG=${IMAGE} cd config/manager && /home/student/guestbook-operator-project/bin/kustomize edit set image controller=docker.io/rojanjose/guestbook-operator:v1.0.0 /home/student/guestbook-operator-project/bin/kustomize build config/default | kubectl apply -f - namespace/guestbook-operator-project-system created customresourcedefinition.apiextensions.k8s.io/guestbooks.charts.guestbook.ibm.com unchanged role.rbac.authorization.k8s.io/guestbook-operator-project-leader-election-role created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-manager-role created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-metrics-reader created clusterrole.rbac.authorization.k8s.io/guestbook-operator-project-proxy-role created rolebinding.rbac.authorization.k8s.io/guestbook-operator-project-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/guestbook-operator-project-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/guestbook-operator-project-proxy-rolebinding created service/guestbook-operator-project-controller-manager-metrics-service created deployment.apps/guestbook-operator-project-controller-manager created View of what got deployed: oc get all -n ${ OPERATOR_PROJECT } -system $ oc get all -n ${OPERATOR_PROJECT}-system NAME READY STATUS RESTARTS AGE pod/guestbook-operator-project-controller-manager-7bc6f986dd-2r898 2/2 Running 0 2m24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-operator-project-controller-manager-metrics-service ClusterIP 172.21.110.64 <none> 8443/TCP 2m24s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-operator-project-controller-manager 1/1 1 1 2m24s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-operator-project-controller-manager-7bc6f986dd 1 1 1 2m24s","title":"4. Apply the CRD in the cluster and deploy the operator image."},{"location":"generatedContent/kubernetes-operators/lab3/#5-deploy-the-operand-by-applying-the-custom-resource-cr-into-the-cluster","text":"Create a new project called guestbook where Guestbook application will be deployed. oc new-project guestbook $ oc new-project guestbook Now using project \"guestbook\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:30606\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app ruby~https://github.com/sclorg/ruby-ex.git to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node An example custom resource yaml file was automatically generated under the config/samples directory as part of the create API step earlier. This is based on the default values.yaml from the Guestbook helm chart under helm-charts/guestbook/values.yaml . Let's use this file to create the operand. oc apply -f config/samples/charts_v1alpha1_guestbook.yaml $ oc apply -f config/samples/charts_v1alpha1_guestbook.yaml guestbook.charts.guestbook.ibm.com/guestbook-sample created Outcome of the operand deploy can be viewed by running the command: oc get all -n guestbook $ oc get all -n guestbook NAME READY STATUS RESTARTS AGE pod/guestbook-sample-8594c8dc46-bl7sm 1/1 Running 0 75s pod/guestbook-sample-8594c8dc46-qwgkv 1/1 Running 0 75s pod/redis-master-68857cd57c-bjxt5 1/1 Running 0 75s pod/redis-slave-bbd8d8545-57944 1/1 Running 0 75s pod/redis-slave-bbd8d8545-rxqc5 1/1 Running 0 75s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-sample LoadBalancer 172.21.41.67 169.45.217.90 3000:30940/TCP 75s service/redis-master ClusterIP 172.21.206.242 <none> 6379/TCP 75s service/redis-slave ClusterIP 172.21.133.74 <none> 6379/TCP 75s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-sample 2/2 2 2 75s deployment.apps/redis-master 1/1 1 1 75s deployment.apps/redis-slave 2/2 2 2 75s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-sample-8594c8dc46 2 2 2 75s replicaset.apps/redis-master-68857cd57c 1 1 1 75s replicaset.apps/redis-slave-bbd8d8545 2 2 2 75s Validate the Guestbook application is running by accessing it with the following commands: HOSTNAME = ` oc get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` oc get svc guestbook-sample -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" http://169.45.242.104:30940 Copy and paste the above URL in a new browser tab to get to the Guestbook landing page as shown below: Open the OpenShift console to view the artificats created as part of this exerice. Guestbook operator pod: Guestbook application pods:","title":"5. Deploy the operand by applying the custom resource (CR) into the cluster."},{"location":"generatedContent/kubernetes-operators/lab3/#6-cleanup-the-deployment","text":"Remove the Guestbook application by deleting the customer resource (CR). oc delete -f config/samples/charts_v1alpha1_guestbook.yaml $ oc delete -f config/samples/charts_v1alpha1_guestbook.yaml guestbook.charts.guestbook.ibm.com \"guestbook-sample\" deleted $ oc get all -n guestbook No resources found in guestbook namespace. $ oc delete project guestbook project.project.openshift.io \"guestbook\" deleted Finally, to delete the CRDs, run the make undeploy command: make undeploy $ make undeploy /home/student/guestbook-operator-project/bin/kustomize build config/default | kubectl delete -f - namespace \"gb-helm-operator-system\" deleted customresourcedefinition.apiextensions.k8s.io \"guestbooks.charts.guestbook.ibm.com\" deleted role.rbac.authorization.k8s.io \"gb-helm-operator-leader-election-role\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-manager-role\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-metrics-reader\" deleted clusterrole.rbac.authorization.k8s.io \"gb-helm-operator-proxy-role\" deleted rolebinding.rbac.authorization.k8s.io \"gb-helm-operator-leader-election-rolebinding\" deleted clusterrolebinding.rbac.authorization.k8s.io \"gb-helm-operator-manager-rolebinding\" deleted clusterrolebinding.rbac.authorization.k8s.io \"gb-helm-operator-proxy-rolebinding\" deleted service \"gb-helm-operator-controller-manager-metrics-service\" deleted deployment.apps \"gb-helm-operator-controller-manager\" deleted This concludes the Helm Operator lab. Go here for additional information on building operators using Helm.","title":"6. Cleanup the deployment."},{"location":"generatedContent/kubernetes-operators/lab4/","text":"Introduction to Operator Lifecyle Management \u00b6 COMING SOON! \u00b6","title":"Introduction to Operator Lifecyle Management"},{"location":"generatedContent/kubernetes-operators/lab4/#introduction-to-operator-lifecyle-management","text":"","title":"Introduction to Operator Lifecyle Management"},{"location":"generatedContent/kubernetes-operators/lab4/#coming-soon","text":"","title":"COMING SOON!"},{"location":"generatedContent/kubernetes-operators/lab5/","text":"Building Operator with OLM \u00b6 COMING SOON! \u00b6","title":"Building Operator with OLM"},{"location":"generatedContent/kubernetes-operators/lab5/#building-operator-with-olm","text":"","title":"Building Operator with OLM"},{"location":"generatedContent/kubernetes-operators/lab5/#coming-soon","text":"","title":"COMING SOON!"},{"location":"generatedContent/kubernetes-operators/lab6/","text":"Operator Tools \u00b6 To write applications that use the Kubernetes REST API, you can use one of the following supported client libraries: Go , Python , Java , CSharp dotnet , JavaScript , Haskell . In addition, there are many community-maintained client libraries . At the OperatorHub.io , you find ready to use operators written by the community. To write your own operator you can use existing tools: KUDO (Kubernetes Universal Declarative Operator), kubebuilder , Metacontroller using custom WebHooks, the Operator Framework .","title":"Operator Tools"},{"location":"generatedContent/kubernetes-operators/lab6/#operator-tools","text":"To write applications that use the Kubernetes REST API, you can use one of the following supported client libraries: Go , Python , Java , CSharp dotnet , JavaScript , Haskell . In addition, there are many community-maintained client libraries . At the OperatorHub.io , you find ready to use operators written by the community. To write your own operator you can use existing tools: KUDO (Kubernetes Universal Declarative Operator), kubebuilder , Metacontroller using custom WebHooks, the Operator Framework .","title":"Operator Tools"},{"location":"generatedContent/kubernetes-operators/setup/","text":"Client Setup \u00b6 Access the web-terminal \u00b6 When running the lab for Kubernetes Extensions, you can make use of a web-terminal. The Dockerfile to use is located in https://github.com/IBMAppModernization/web-terminal , and named Dockerfile-s2i-oc-tekton-operator . To run on localhost as a Docker container, git clone https://github.com/IBMAppModernization/web-terminal.git cd web-terminal docker build --no-cache -t web-terminal:latest -f Dockerfile-s2i-oc-tekton-operator . docker run -d --restart always --name terminal -p 7681 :7681 -v $HOME /dev/tmp:/root/dev web-terminal docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 85edc0b0ec27 web-terminal \"ttyd -p 7681 bash\" 17 minutes ago Up 17 minutes 0 .0.0.0:7681->7681/tcp terminal The volume mapping will write all files under the working directory to the host directory $HOME/dev/tmp . So suppose my host's user home directory is /Users/remkohdev@us.ibm.com/ . If I open the terminal in the browser, the working directory for the user is /root . Any file that is created under /root is created on the host's directory $HOME/dev/tmp . Similarly if I create a file in $HOME/dev/tmp it is available in the container's /root directory. Open the web-terminal in a browser and go to http://0.0.0.0:7681 . If Go, Operator SD export CLUSTERNAME = remkohdev-roks-labs-3n-cluster ibmcloud login Go to the OpenShift web console Copy Login command oc login --token = _12AbcD345kIPDIRg2jYpCuZ-g5SM5Im9irY2tol4Q8 --server = https://c100-e.us-south.containers.cloud.ibm.com:30712","title":"Client Setup"},{"location":"generatedContent/kubernetes-operators/setup/#client-setup","text":"","title":"Client Setup"},{"location":"generatedContent/kubernetes-operators/setup/#access-the-web-terminal","text":"When running the lab for Kubernetes Extensions, you can make use of a web-terminal. The Dockerfile to use is located in https://github.com/IBMAppModernization/web-terminal , and named Dockerfile-s2i-oc-tekton-operator . To run on localhost as a Docker container, git clone https://github.com/IBMAppModernization/web-terminal.git cd web-terminal docker build --no-cache -t web-terminal:latest -f Dockerfile-s2i-oc-tekton-operator . docker run -d --restart always --name terminal -p 7681 :7681 -v $HOME /dev/tmp:/root/dev web-terminal docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 85edc0b0ec27 web-terminal \"ttyd -p 7681 bash\" 17 minutes ago Up 17 minutes 0 .0.0.0:7681->7681/tcp terminal The volume mapping will write all files under the working directory to the host directory $HOME/dev/tmp . So suppose my host's user home directory is /Users/remkohdev@us.ibm.com/ . If I open the terminal in the browser, the working directory for the user is /root . Any file that is created under /root is created on the host's directory $HOME/dev/tmp . Similarly if I create a file in $HOME/dev/tmp it is available in the container's /root directory. Open the web-terminal in a browser and go to http://0.0.0.0:7681 . If Go, Operator SD export CLUSTERNAME = remkohdev-roks-labs-3n-cluster ibmcloud login Go to the OpenShift web console Copy Login command oc login --token = _12AbcD345kIPDIRg2jYpCuZ-g5SM5Im9irY2tol4Q8 --server = https://c100-e.us-south.containers.cloud.ibm.com:30712","title":"Access the web-terminal"},{"location":"generatedContent/kubernetes-storage/","text":"Introduction to Kubernetes Storage \u00b6 Objectives \u00b6 Overview \u00b6 The introductory page of the workshop is broken down into the following sections: Lecture: Update K8s storage lecture. Lab 1: Container storage and Kubernetes Lab 2 File storage with kubernetes Lab 3: Block storage with kubernetes Lab 4: Kubernetes StatefulSets Lab 5: Object Storage with Kubernetes Lab 6: Software Defined Storage (SDS) with Portworx, coming soon... Lab 7: Connecting to External Storage","title":"Introduction to Kubernetes Storage"},{"location":"generatedContent/kubernetes-storage/#introduction-to-kubernetes-storage","text":"","title":"Introduction to Kubernetes Storage"},{"location":"generatedContent/kubernetes-storage/#objectives","text":"","title":"Objectives"},{"location":"generatedContent/kubernetes-storage/#overview","text":"The introductory page of the workshop is broken down into the following sections: Lecture: Update K8s storage lecture. Lab 1: Container storage and Kubernetes Lab 2 File storage with kubernetes Lab 3: Block storage with kubernetes Lab 4: Kubernetes StatefulSets Lab 5: Object Storage with Kubernetes Lab 6: Software Defined Storage (SDS) with Portworx, coming soon... Lab 7: Connecting to External Storage","title":"Overview"},{"location":"generatedContent/kubernetes-storage/SUMMARY/","text":"Summary \u00b6 Workshop \u00b6 Lab 0: Prework Lab 1: Container storage and Kubernetes Lab 2 File storage with kubernetes Lab 3: Block storage with kubernetes Lab 4: Kubernetes StatefulSets Lab 5: Object Storage with Kubernetes Lab 6: Using Software Defined Storage (SDS) with Portworx Lab 7: Connecting to External Storage Resources \u00b6 IBM Developer","title":"Summary"},{"location":"generatedContent/kubernetes-storage/SUMMARY/#summary","text":"","title":"Summary"},{"location":"generatedContent/kubernetes-storage/SUMMARY/#workshop","text":"Lab 0: Prework Lab 1: Container storage and Kubernetes Lab 2 File storage with kubernetes Lab 3: Block storage with kubernetes Lab 4: Kubernetes StatefulSets Lab 5: Object Storage with Kubernetes Lab 6: Using Software Defined Storage (SDS) with Portworx Lab 7: Connecting to External Storage","title":"Workshop"},{"location":"generatedContent/kubernetes-storage/SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"generatedContent/kubernetes-storage/Lab0/","text":"Lab 0: Pre-work \u00b6 1. Setup Kubernetes environment \u00b6 Run through the instructions listed here 2. Cloud shell login \u00b6 3. Docker hub account \u00b6 Create a dockerhub user and set the environment variable. DOCKERUSER = <dockerhub useid> 4. Set the cluster name \u00b6 ibmcloud ks clusters OK Name ID State Created Workers Location Version Resource Group Name Provider user001-workshop bseqlkkd0o1gdqg4jc10 normal 3 months ago 5 Dallas 4 .3.38_1544_openshift default classic CLUSTERNAME = use001-workshop or CLUSTERNAME = ` ibmcloud ks clusters | grep Name -A 1 | awk '{print $1}' | grep -v Name ` echo $CLUSTERNAME user001-workshop","title":"Lab 0: Pre-work"},{"location":"generatedContent/kubernetes-storage/Lab0/#lab-0-pre-work","text":"","title":"Lab 0: Pre-work"},{"location":"generatedContent/kubernetes-storage/Lab0/#1-setup-kubernetes-environment","text":"Run through the instructions listed here","title":"1. Setup Kubernetes environment"},{"location":"generatedContent/kubernetes-storage/Lab0/#2-cloud-shell-login","text":"","title":"2. Cloud shell login"},{"location":"generatedContent/kubernetes-storage/Lab0/#3-docker-hub-account","text":"Create a dockerhub user and set the environment variable. DOCKERUSER = <dockerhub useid>","title":"3. Docker hub account"},{"location":"generatedContent/kubernetes-storage/Lab0/#4-set-the-cluster-name","text":"ibmcloud ks clusters OK Name ID State Created Workers Location Version Resource Group Name Provider user001-workshop bseqlkkd0o1gdqg4jc10 normal 3 months ago 5 Dallas 4 .3.38_1544_openshift default classic CLUSTERNAME = use001-workshop or CLUSTERNAME = ` ibmcloud ks clusters | grep Name -A 1 | awk '{print $1}' | grep -v Name ` echo $CLUSTERNAME user001-workshop","title":"4. Set the cluster name"},{"location":"generatedContent/kubernetes-storage/Lab1/","text":"Lab 1: Non-persistent storage with Kubernetes \u00b6 Storing data in containers or worker nodes are considered as the non-persistent forms of data storage. In this lab, we will explore storage options on the IBM Kubernetes worker nodes. Follow this lab is you are interested in learning more about container-based storage. The lab covers the following topics: Create and claim IBM Kubernetes non-persistent storage based on the primary and secondary storage available on the worker nodes. Make the volumes available in the Guestbook application. Use the volumes to store application cache and debug information. Access the data from the guestbook container using the Kubernetes CLI. Assess the impact of losing a pod on data retention. Claim back the storage resources and clean up. The primary storage maps to the volume type hostPath and the secondary storage maps to emptyDir . Learn more about Kubernetes volume types here . Reserve Persistent Volumes \u00b6 From the cloud shell prompt, run the following commands to get the guestbook application and the kubernetes configuration needed for the storage labs. cd $HOME git clone --branch fs https://github.com/IBM/guestbook-nodejs.git git clone --branch storage https://github.com/rojanjose/guestbook-config.git cd $HOME /guestbook-config/storage/lab1 Let's start with reserving the Persistent volume from the primary storage. Review the yaml file pv-hostpath.yaml . Note the values set for type , storageClassName and hostPath . apiVersion : v1 kind : PersistentVolume metadata : name : guestbook-primary-pv labels : type : local spec : storageClassName : manual capacity : storage : 10Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" Create the persistent volume as shown in the command below: kubectl create -f pv-hostpath.yaml persistentvolume/guestbook-primary-pv created kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE guestbook-primary-pv 10Gi RWO Retain Available manual 13s Next PVC yaml: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : guestbook-local-pvc spec : storageClassName : manual accessModes : - ReadWriteMany resources : requests : storage : 3Gi Create PVC: kubectl create -f pvc-hostpath.yaml persistentvolumeclaim/guestbook-local-pvc created \u276f kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-local-pvc Bound guestbook-local-pv 10Gi RWX manual 6s Guestbook application using storage \u00b6 The application is the Guestbook App , which is a simple multi-tier web application built using the loopback framework. Change to the guestbook application source directory: cd $HOME /guestbook-nodejs/src Review the source common/models/entry.js . The application uses storage allocated using hostPath to store data cache in the file data/cache.txt . The file logs/debug.txt records debug messages provisioned using the emptyDir storage type. module . exports = function ( Entry ) { Entry . greet = function ( msg , cb ) { // console.log(\"testing \" + msg); fs . appendFile ( 'logs/debug.txt' , \"Received message: \" + msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Debug stagement printed' ); }); fs . appendFile ( 'data/cache.txt' , msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Saved in cache!' ); }); ... Run the commands listed below to build the guestbook image and copy into the docker hub registry: docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab1 cat guestbook-deployment.yaml Replace the first part of image name with your docker hub user id. The section spec.volumes lists hostPath and emptyDir volumes. The section spec.containers.volumeMounts lists the mount paths that the application uses to write in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-primary-volume mountPath : /home/node/app/data - name : guestbook-secondary-volume mountPath : /home/node/app/logs volumes : - name : guestbook-primary-volume persistentVolumeClaim : claimName : guestbook-primary-pvc - name : guestbook-secondary-volume emptyDir : {} ... Deploy the Guestbook application: kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 14s kubectl create -f guestbook-service.yaml service/guestbook created Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -o wide | tail -n 1 | awk '{print $7}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Open the URL in a browser and create guest book entries. Next, inspect the data. To do this, run a bash process inside the application container using kubectl exec . Reference the pod name from the previous kubectl get pods command. Once inside the container, use the subsequent comands to inspect the data. kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls -al total 256 drwxr-xr-x 1 root root 4096 Nov 11 23 :40 . drwxr-xr-x 1 node node 4096 Nov 11 23 :20 .. -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile drwxr-xr-x 2 root root 4096 Nov 11 03 :40 client drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 2 root root 4096 Nov 11 23 :16 data drwxrwxrwx 2 root root 4096 Nov 11 23 :44 logs drwxr-xr-x 439 root root 16384 Nov 11 23 :20 node_modules -rw-r--r-- 1 root root 176643 Nov 11 23 :20 package-lock.json -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 3 .5G 90G 4 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup /dev/mapper/docker_data 98G 3 .5G 90G 4 % /etc/hosts shm 64M 0 64M 0 % /dev/shm /dev/xvda2 25G 3 .6G 20G 16 % /home/node/app/data tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware While still inside the container, create a file on the container file system. This file will not persist when we kill the container. Then run /sbin/killall5 to terminate the container. root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# touch dontdeletemeplease root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease dontdeletemeplease root@guestbook-v1-66798779d6-fqh2j:/home/node/app# /sbin/killall5 root@guestbook-v1-66798779d6-fqh2j:/home/node/app# command terminated with exit code 137 The killall5 command will kick you out of the container (which is no longer running), but the pod is still running. Verify this with kubectl get pods . Not the 0/1 status indicating the application container is no longer running. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 0 /1 CrashLoopBackOff 2 16m After a few seconds, the Pod will restart the container: kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 1 /1 Running 3 16m Run a bash process inside the container to inspect your data again: kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease ls: dontdeletemeplease: No such file or directory Notice how the storage from the primary ( hostPath ) and secondary ( emptyDir ) storage types persisted beyond the lifecycle of the container, but the dontdeletemeplease file, did not. Next, we'll kill the pod to see the impact of deleting the pod on data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 12m kubectl delete pod guestbook-v1-6f55cb54c5-jb89d pod \"guestbook-v1-6f55cb54c5-jb89d\" deleted kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s Enter new data: Log into the pod to view the state of the data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s kubectl exec -it guestbook-v1-5cbc445dc9-sx58j bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [ POD ] -- [ COMMAND ] instead. root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! Bye Kubernetes! Aloha Kubernetes! Ciao Kubernetes! Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat logs/debug.txt Received message: Bye Kubernetes! Received message: Aloha Kubernetes! Received message: Ciao Kubernetes! Received message: Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# This shows that the storage type emptyDir loose data on a pod restart whereas hostPath data lives until the worker node or cluster is deleted. Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container Normally Kubernetes clusters have multiple worker nodes in a cluster with replicas for a single application running across different worker nodes. In this case, only applications running on the same worker node will share data persisted with IKS Primary Storage (HostPath). More suitable solutions are available for cross worker nodes, cross availability-zone and cross-region storage. Clean up \u00b6 cd $HOME /guestbook-config/storage/lab1 kubectl delete -f .","title":"Lab 1: Non-persistent storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab1/#lab-1-non-persistent-storage-with-kubernetes","text":"Storing data in containers or worker nodes are considered as the non-persistent forms of data storage. In this lab, we will explore storage options on the IBM Kubernetes worker nodes. Follow this lab is you are interested in learning more about container-based storage. The lab covers the following topics: Create and claim IBM Kubernetes non-persistent storage based on the primary and secondary storage available on the worker nodes. Make the volumes available in the Guestbook application. Use the volumes to store application cache and debug information. Access the data from the guestbook container using the Kubernetes CLI. Assess the impact of losing a pod on data retention. Claim back the storage resources and clean up. The primary storage maps to the volume type hostPath and the secondary storage maps to emptyDir . Learn more about Kubernetes volume types here .","title":"Lab 1: Non-persistent storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab1/#reserve-persistent-volumes","text":"From the cloud shell prompt, run the following commands to get the guestbook application and the kubernetes configuration needed for the storage labs. cd $HOME git clone --branch fs https://github.com/IBM/guestbook-nodejs.git git clone --branch storage https://github.com/rojanjose/guestbook-config.git cd $HOME /guestbook-config/storage/lab1 Let's start with reserving the Persistent volume from the primary storage. Review the yaml file pv-hostpath.yaml . Note the values set for type , storageClassName and hostPath . apiVersion : v1 kind : PersistentVolume metadata : name : guestbook-primary-pv labels : type : local spec : storageClassName : manual capacity : storage : 10Gi accessModes : - ReadWriteOnce hostPath : path : \"/mnt/data\" Create the persistent volume as shown in the command below: kubectl create -f pv-hostpath.yaml persistentvolume/guestbook-primary-pv created kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE guestbook-primary-pv 10Gi RWO Retain Available manual 13s Next PVC yaml: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : guestbook-local-pvc spec : storageClassName : manual accessModes : - ReadWriteMany resources : requests : storage : 3Gi Create PVC: kubectl create -f pvc-hostpath.yaml persistentvolumeclaim/guestbook-local-pvc created \u276f kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-local-pvc Bound guestbook-local-pv 10Gi RWX manual 6s","title":"Reserve Persistent Volumes"},{"location":"generatedContent/kubernetes-storage/Lab1/#guestbook-application-using-storage","text":"The application is the Guestbook App , which is a simple multi-tier web application built using the loopback framework. Change to the guestbook application source directory: cd $HOME /guestbook-nodejs/src Review the source common/models/entry.js . The application uses storage allocated using hostPath to store data cache in the file data/cache.txt . The file logs/debug.txt records debug messages provisioned using the emptyDir storage type. module . exports = function ( Entry ) { Entry . greet = function ( msg , cb ) { // console.log(\"testing \" + msg); fs . appendFile ( 'logs/debug.txt' , \"Received message: \" + msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Debug stagement printed' ); }); fs . appendFile ( 'data/cache.txt' , msg + \"\\n\" , function ( err ) { if ( err ) throw err ; console . log ( 'Saved in cache!' ); }); ... Run the commands listed below to build the guestbook image and copy into the docker hub registry: docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab1 cat guestbook-deployment.yaml Replace the first part of image name with your docker hub user id. The section spec.volumes lists hostPath and emptyDir volumes. The section spec.containers.volumeMounts lists the mount paths that the application uses to write in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-primary-volume mountPath : /home/node/app/data - name : guestbook-secondary-volume mountPath : /home/node/app/logs volumes : - name : guestbook-primary-volume persistentVolumeClaim : claimName : guestbook-primary-pvc - name : guestbook-secondary-volume emptyDir : {} ... Deploy the Guestbook application: kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 14s kubectl create -f guestbook-service.yaml service/guestbook created Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -o wide | tail -n 1 | awk '{print $7}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Open the URL in a browser and create guest book entries. Next, inspect the data. To do this, run a bash process inside the application container using kubectl exec . Reference the pod name from the previous kubectl get pods command. Once inside the container, use the subsequent comands to inspect the data. kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls -al total 256 drwxr-xr-x 1 root root 4096 Nov 11 23 :40 . drwxr-xr-x 1 node node 4096 Nov 11 23 :20 .. -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile drwxr-xr-x 2 root root 4096 Nov 11 03 :40 client drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 2 root root 4096 Nov 11 23 :16 data drwxrwxrwx 2 root root 4096 Nov 11 23 :44 logs drwxr-xr-x 439 root root 16384 Nov 11 23 :20 node_modules -rw-r--r-- 1 root root 176643 Nov 11 23 :20 package-lock.json -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 3 .5G 90G 4 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup /dev/mapper/docker_data 98G 3 .5G 90G 4 % /etc/hosts shm 64M 0 64M 0 % /dev/shm /dev/xvda2 25G 3 .6G 20G 16 % /home/node/app/data tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware While still inside the container, create a file on the container file system. This file will not persist when we kill the container. Then run /sbin/killall5 to terminate the container. root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# touch dontdeletemeplease root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease dontdeletemeplease root@guestbook-v1-66798779d6-fqh2j:/home/node/app# /sbin/killall5 root@guestbook-v1-66798779d6-fqh2j:/home/node/app# command terminated with exit code 137 The killall5 command will kick you out of the container (which is no longer running), but the pod is still running. Verify this with kubectl get pods . Not the 0/1 status indicating the application container is no longer running. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 0 /1 CrashLoopBackOff 2 16m After a few seconds, the Pod will restart the container: kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-66798779d6-fqh2j 1 /1 Running 3 16m Run a bash process inside the container to inspect your data again: kubectl exec -it [ POD NAME ] -- bash root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# cat logs/debug.txt Received message: Hello Kubernetes! Received message: Hola Kubernetes! Received message: Zdravstvuyte Kubernetes! Received message: N\u01d0n h\u01ceo Kubernetes! Received message: Goedendag Kubernetes! root@guestbook-v1-6f55cb54c5-jb89d:/home/node/app# ls dontdeletemeplease ls: dontdeletemeplease: No such file or directory Notice how the storage from the primary ( hostPath ) and secondary ( emptyDir ) storage types persisted beyond the lifecycle of the container, but the dontdeletemeplease file, did not. Next, we'll kill the pod to see the impact of deleting the pod on data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-6f55cb54c5-jb89d 1 /1 Running 0 12m kubectl delete pod guestbook-v1-6f55cb54c5-jb89d pod \"guestbook-v1-6f55cb54c5-jb89d\" deleted kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s Enter new data: Log into the pod to view the state of the data. kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5cbc445dc9-sx58j 1 /1 Running 0 86s kubectl exec -it guestbook-v1-5cbc445dc9-sx58j bash kubectl exec [ POD ] [ COMMAND ] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [ POD ] -- [ COMMAND ] instead. root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat data/cache.txt Hello Kubernetes! Hola Kubernetes! Zdravstvuyte Kubernetes! N\u01d0n h\u01ceo Kubernetes! Goedendag Kubernetes! Bye Kubernetes! Aloha Kubernetes! Ciao Kubernetes! Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# cat logs/debug.txt Received message: Bye Kubernetes! Received message: Aloha Kubernetes! Received message: Ciao Kubernetes! Received message: Sayonara Kubernetes! root@guestbook-v1-5cbc445dc9-sx58j:/home/node/app# This shows that the storage type emptyDir loose data on a pod restart whereas hostPath data lives until the worker node or cluster is deleted. Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container Normally Kubernetes clusters have multiple worker nodes in a cluster with replicas for a single application running across different worker nodes. In this case, only applications running on the same worker node will share data persisted with IKS Primary Storage (HostPath). More suitable solutions are available for cross worker nodes, cross availability-zone and cross-region storage.","title":"Guestbook application using storage"},{"location":"generatedContent/kubernetes-storage/Lab1/#clean-up","text":"cd $HOME /guestbook-config/storage/lab1 kubectl delete -f .","title":"Clean up"},{"location":"generatedContent/kubernetes-storage/Lab2/","text":"Lab 2: File storage with Kubernetes \u00b6 This lab demonstrates the use of cloud based file storage with Kubernetes. It uses the IBM Cloud File Storage which is persistent, fast, and flexible network-attached, NFS-based File Storage capacity ranging from 25 GB to 12,000 GB capacity with up to 48,000 IOPS. The IBM Cloud File Storage provides data across all worker nodes within a single availability zone. Following topics are covered in this exercise: Claim a classic file storage volume. Make the volumes available in the Guestbook application. Copy media files such as images into the volume using the Kubernetes CLI. Use the Guestbook application to view the images. Claim back the storage resources and clean up. Prereqs \u00b6 Follow the prereqs if you haven't already. Claim file storage volume \u00b6 Review the storage classes for file storage. In addition to the standard set of storage classes, custom storage classes can be defined to meet the storage requirements. kubectl get storageclasses Expected output: $ kubectl get storageclasses default ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-custom ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold ( default ) ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-retain-bronze ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-custom ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-gold ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-silver ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-silver ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-silver-gid ibm.io/ibmc-file Delete Immediate false 27m IKS comes with storage class definitions for file storage. This lab uses the storage class ibm-file-silver . Note that the default class is ibmc-file-gold is allocated if storgage class is not expliciity definded. kubectl describe storageclass ibmc-file-silver Expected output: $ kubectl describe storageclass ibmc-file-silver Name: ibmc-file-silver IsDefaultClass: No Annotations: kubectl.kubernetes.io/last-applied-configuration ={ \"apiVersion\" : \"storage.k8s.io/v1\" , \"kind\" : \"StorageClass\" , \"metadata\" : { \"annotations\" : {} , \"labels\" : { \"kubernetes.io/cluster-service\" : \"true\" } , \"name\" : \"ibmc-file-silver\" } , \"parameters\" : { \"billingType\" : \"hourly\" , \"classVersion\" : \"2\" , \"iopsPerGB\" : \"4\" , \"sizeRange\" : \"[20-12000]Gi\" , \"type\" : \"Endurance\" } , \"provisioner\" : \"ibm.io/ibmc-file\" , \"reclaimPolicy\" : \"Delete\" } Provisioner: ibm.io/ibmc-file Parameters: billingType = hourly,classVersion = 2 ,iopsPerGB = 4 ,sizeRange =[ 20 -12000 ] Gi,type = Endurance AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> File sliver has an IOPS of 4GB and a max capacity of 12TB. Claim a file storage volume \u00b6 IBM Cloud File Storage provides fast access to your data for a cluster running in a single available zone. For higher availability, use a storage option that is designed for geographically distributed data . Review the yaml for the file storage PersistentVolumeClaim . When we create this PersistentVolumeClaim , it automatically creates it within an availability zone where the worker nodes are located. cd guestbook-config/storage/lab2 cat pvc-file-silver.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: guestbook-pvc labels: billingType: hourly spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi storageClassName: ibmc-file-silver Create the PVC kubectl apply -f pvc-file-silver.yaml Expected output: $ kubectl create -f pvc-file-silver.yaml persistentvolumeclaim/guestbook-filesilver-pvc created Verify the PVC claim is created with the status Bound . This may take a minute or two. kubectl get pvc guestbook-filesilver-pvc Expected output: $ kubectl get pvc guestbook-filesilver-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-filesilver-pvc Bound pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX ibmc-file-silver 2m Details associated with the pv . Use the pv name from the previous command output. kubectl get pv [ pv name ] Expected output: $ kubectl get pv pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX Delete Bound default/guestbook-filesilver-pvc ibmc-file-silver 90s Use the volume in the Guestbook application \u00b6 Change to the guestbook application source directory and review the html files images.html and index.html . images.html has the code to display the images stored in the file storage. cd $HOME /guestbook-nodejs/src cat client/images.html cat client/index.html Run the commands listed below to build the guestbook image and copy into the docker hub registry: (Skip this step if you have already completed lab 1.) cd $HOME /guestbook-nodejs/src docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab2 cat guestbook-deployment.yaml Replace the first part of the image name with your docker hub user id. The section spec.volumes references the file volume PVC. The section spec.containers.volumeMounts has the mount path to store images in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-file-volume mountPath : /app/public/images volumes : - name : guestbook-file-volume persistentVolumeClaim : claimName : guestbook-filesilver-pvc Deploy the Guestbook application. kubectl create -f guestbook-deployment.yaml kubectl create -f guestbook-service.yaml Verify the Guestbook application is runing. kubectl get all Expected output: $ kubectl get all NAME READY STATUS RESTARTS AGE pod/guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 13s pod/guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/guestbook LoadBalancer 172 .21.238.40 150 .238.30.150 3000 :31986/TCP 6s service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 9d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-v1 2 /2 2 2 13s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-5bd76b568f 2 2 2 13s Check the mount path inside the pod container. Get the pod listing. $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 78s guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 78s Set these variables for each of your pod names: export POD1 =[ FIRST POD NAME ] export POD2 =[ SECOND POD NAME ] Log into any one of the pod. Use one of the pod names from the previous command output. kubectl exec -it $POD1 -- bash Run the commands ls -al; ls -al images; df -ah to view the volume and files. Review the mount for the new volume. Note that the images folder is empty. $ kubectl exec -it $POD1 -- bash root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt total 252 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 client drwxr-xr-x 1 root root 4096 Nov 13 03 :15 . drwxr-xr-x 439 root root 16384 Nov 13 03 :15 node_modules -rw-r--r-- 1 root root 176643 Nov 13 03 :15 package-lock.json drwxr-xr-x 1 node node 4096 Nov 13 03 :15 .. -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt client/images total 8 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 02 :02 . root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .9G 89G 6 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .9G 89G 6 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 0 20G 0 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# exit Note the filesystem fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 is mounted on path /home/node/app/client/images . Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Verify that the images are missing by viewing the data from the Guestbook application. Click on the hyperlink labled images at the bottom of the guestbook home page. The images.html page shows images with broken links. Load the file storage with images \u00b6 Run the kubectl cp command to move the images into the mounted volume. cd $HOME /guestbook-config/storage/lab2 kubectl cp images $POD1 :/home/node/app/client/ Refresh the page images.html page in the guestbook application to view the uploaded images. Shared storage across pods \u00b6 Login into the other pod $POD2 to verify the volume mount. kubectl exec -it $POD2 -- bash root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# ls -alt /home/node/app/client/images total 160 -rw-r--r-- 1 501 staff 56191 Nov 13 03 :44 gb3.jpg drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 03 :44 . -rw-r--r-- 1 501 staff 21505 Nov 13 03 :44 gb2.jpg -rw-r--r-- 1 501 staff 58286 Nov 13 03 :44 gb1.jpg drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .2G 89G 5 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .2G 89G 5 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 128K 20G 1 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# exit Note that the volume and the data are available on all the pods running the Guestbook application. IBM Cloud File Storage is a NFS-based file storage that is available across all worker nodes within a single availability zone. If you are running a cluster with multiple nodes (within a single AZ) you can run the following commands to prove that your data is available across different nodes: kubectl get pods -o wide kubectl get nodes Expected output: $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES guestbook-v1-6fb8b86876-n9jtz 1 /1 Running 0 39h 172 .30.224.70 10 .38.216.205 <none> <none> guestbook-v1-6fb8b86876-njwcz 1 /1 Running 0 39h 172 .30.169.144 10 .38.216.238 <none> <none> $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10 .38.216.205 Ready <none> 4d5h v1.18.10+IKS 10 .38.216.238 Ready <none> 4d5h v1.18.10+IKS To extend our table from Lab 1 we now have: Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container IBM Cloud File Storage (NFS) Availabilty Zone Applications running in a single availabilty zone Data is available to all nodes within the availability zone where the file storage exists, but the accessMode parameter on the PersistentVolumeClaim determines if multiple pods are able to mount a volume specificed by a PVC. The possible values for this parameter are: ReadWriteMany : The PVC can be mounted by multiple pods. All pods can read from and write to the volume. ReadOnlyMany : The PVC can be mounted by multiple pods. All pods have read-only access. ReadWriteOnce : The PVC can be mounted by one pod only. This pod can read from and write to the volume. [Optional exercises] \u00b6 Another way to see that the data is persisted at the availability zone level, you can: Back up data. Delete pods to confirm that it does not impact the data used by the application. Delete the Kubernetes cluster. Create a new cluster and reuse the volume. Clean up \u00b6 List all the PVCs and PVs kubectl get pvc kubectl get pv Delete all the pods using the PVC. Delete the PVC kubectl delete pvc guestbook-pvc persistentvolumeclaim \"guestbook-pvc\" deleted List PV to ensure that it is removed as well. Cancel the physical storage volume from the cloud account. (Note: requires admin permissions?) ibmcloud sl file volume-list --columns id --columns notes | grep pvc-6362f614-258e-48ee-a596-62bb4629cd75 183223942 { \"plugin\" : \"ibm-file-plugin-7b9db9c79f-86x8w\" , \"region\" : \"us-south\" , \"cluster\" : \"bugql3nd088jsp8iiagg\" , \"type\" : \"Endurance\" , \"ns\" : \"default\" , \"pvc\" : \"guestbook-pvc\" , \"pv\" : \"pvc-6362f614-258e-48ee-a596-62bb4629cd75\" , \"storageclass\" : \"ibmc-file-silver\" , \"reclaim\" : \"Delete\" } ibmcloud sl file volume-cancel 183223942 This will cancel the file volume: 183223942 and cannot be undone. Continue?> yes Failed to cancel file volume: 183223942 . No billing item is found to cancel.","title":"Lab 2: File storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab2/#lab-2-file-storage-with-kubernetes","text":"This lab demonstrates the use of cloud based file storage with Kubernetes. It uses the IBM Cloud File Storage which is persistent, fast, and flexible network-attached, NFS-based File Storage capacity ranging from 25 GB to 12,000 GB capacity with up to 48,000 IOPS. The IBM Cloud File Storage provides data across all worker nodes within a single availability zone. Following topics are covered in this exercise: Claim a classic file storage volume. Make the volumes available in the Guestbook application. Copy media files such as images into the volume using the Kubernetes CLI. Use the Guestbook application to view the images. Claim back the storage resources and clean up.","title":"Lab 2: File storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab2/#prereqs","text":"Follow the prereqs if you haven't already.","title":"Prereqs"},{"location":"generatedContent/kubernetes-storage/Lab2/#claim-file-storage-volume","text":"Review the storage classes for file storage. In addition to the standard set of storage classes, custom storage classes can be defined to meet the storage requirements. kubectl get storageclasses Expected output: $ kubectl get storageclasses default ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-bronze-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-custom ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold ( default ) ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-gold-gid ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-retain-bronze ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-custom ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-gold ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-retain-silver ibm.io/ibmc-file Retain Immediate false 27m ibmc-file-silver ibm.io/ibmc-file Delete Immediate false 27m ibmc-file-silver-gid ibm.io/ibmc-file Delete Immediate false 27m IKS comes with storage class definitions for file storage. This lab uses the storage class ibm-file-silver . Note that the default class is ibmc-file-gold is allocated if storgage class is not expliciity definded. kubectl describe storageclass ibmc-file-silver Expected output: $ kubectl describe storageclass ibmc-file-silver Name: ibmc-file-silver IsDefaultClass: No Annotations: kubectl.kubernetes.io/last-applied-configuration ={ \"apiVersion\" : \"storage.k8s.io/v1\" , \"kind\" : \"StorageClass\" , \"metadata\" : { \"annotations\" : {} , \"labels\" : { \"kubernetes.io/cluster-service\" : \"true\" } , \"name\" : \"ibmc-file-silver\" } , \"parameters\" : { \"billingType\" : \"hourly\" , \"classVersion\" : \"2\" , \"iopsPerGB\" : \"4\" , \"sizeRange\" : \"[20-12000]Gi\" , \"type\" : \"Endurance\" } , \"provisioner\" : \"ibm.io/ibmc-file\" , \"reclaimPolicy\" : \"Delete\" } Provisioner: ibm.io/ibmc-file Parameters: billingType = hourly,classVersion = 2 ,iopsPerGB = 4 ,sizeRange =[ 20 -12000 ] Gi,type = Endurance AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> File sliver has an IOPS of 4GB and a max capacity of 12TB.","title":"Claim file storage volume"},{"location":"generatedContent/kubernetes-storage/Lab2/#claim-a-file-storage-volume","text":"IBM Cloud File Storage provides fast access to your data for a cluster running in a single available zone. For higher availability, use a storage option that is designed for geographically distributed data . Review the yaml for the file storage PersistentVolumeClaim . When we create this PersistentVolumeClaim , it automatically creates it within an availability zone where the worker nodes are located. cd guestbook-config/storage/lab2 cat pvc-file-silver.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: guestbook-pvc labels: billingType: hourly spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi storageClassName: ibmc-file-silver Create the PVC kubectl apply -f pvc-file-silver.yaml Expected output: $ kubectl create -f pvc-file-silver.yaml persistentvolumeclaim/guestbook-filesilver-pvc created Verify the PVC claim is created with the status Bound . This may take a minute or two. kubectl get pvc guestbook-filesilver-pvc Expected output: $ kubectl get pvc guestbook-filesilver-pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE guestbook-filesilver-pvc Bound pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX ibmc-file-silver 2m Details associated with the pv . Use the pv name from the previous command output. kubectl get pv [ pv name ] Expected output: $ kubectl get pv pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-a7cb12ed-b52b-4342-966a-eceaf24e42a9 20Gi RWX Delete Bound default/guestbook-filesilver-pvc ibmc-file-silver 90s","title":"Claim a file storage volume"},{"location":"generatedContent/kubernetes-storage/Lab2/#use-the-volume-in-the-guestbook-application","text":"Change to the guestbook application source directory and review the html files images.html and index.html . images.html has the code to display the images stored in the file storage. cd $HOME /guestbook-nodejs/src cat client/images.html cat client/index.html Run the commands listed below to build the guestbook image and copy into the docker hub registry: (Skip this step if you have already completed lab 1.) cd $HOME /guestbook-nodejs/src docker build -t $DOCKERUSER /guestbook-nodejs:storage . docker login -u $DOCKERUSER docker push $DOCKERUSER /guestbook-nodejs:storage Review the deployment yaml file guestbook-deplopyment.yaml prior to deploying the application into the cluster. cd $HOME /guestbook-config/storage/lab2 cat guestbook-deployment.yaml Replace the first part of the image name with your docker hub user id. The section spec.volumes references the file volume PVC. The section spec.containers.volumeMounts has the mount path to store images in the volumes. apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 ... spec : containers : - name : guestbook image : rojanjose/guestbook-nodejs:storage imagePullPolicy : Always ports : - name : http-server containerPort : 3000 volumeMounts : - name : guestbook-file-volume mountPath : /app/public/images volumes : - name : guestbook-file-volume persistentVolumeClaim : claimName : guestbook-filesilver-pvc Deploy the Guestbook application. kubectl create -f guestbook-deployment.yaml kubectl create -f guestbook-service.yaml Verify the Guestbook application is runing. kubectl get all Expected output: $ kubectl get all NAME READY STATUS RESTARTS AGE pod/guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 13s pod/guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/guestbook LoadBalancer 172 .21.238.40 150 .238.30.150 3000 :31986/TCP 6s service/kubernetes ClusterIP 172 .21.0.1 <none> 443 /TCP 9d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-v1 2 /2 2 2 13s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-5bd76b568f 2 2 2 13s Check the mount path inside the pod container. Get the pod listing. $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-v1-5bd76b568f-cdhr5 1 /1 Running 0 78s guestbook-v1-5bd76b568f-w6h6h 1 /1 Running 0 78s Set these variables for each of your pod names: export POD1 =[ FIRST POD NAME ] export POD2 =[ SECOND POD NAME ] Log into any one of the pod. Use one of the pod names from the previous command output. kubectl exec -it $POD1 -- bash Run the commands ls -al; ls -al images; df -ah to view the volume and files. Review the mount for the new volume. Note that the images folder is empty. $ kubectl exec -it $POD1 -- bash root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt total 252 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 client drwxr-xr-x 1 root root 4096 Nov 13 03 :15 . drwxr-xr-x 439 root root 16384 Nov 13 03 :15 node_modules -rw-r--r-- 1 root root 176643 Nov 13 03 :15 package-lock.json drwxr-xr-x 1 node node 4096 Nov 13 03 :15 .. -rw-r--r-- 1 root root 830 Nov 11 23 :20 package.json drwxr-xr-x 3 root root 4096 Nov 10 23 :04 common drwxr-xr-x 3 root root 4096 Nov 10 23 :04 server -rw-r--r-- 1 root root 12 Oct 29 21 :00 .dockerignore -rw-r--r-- 1 root root 288 Oct 29 21 :00 .editorconfig -rw-r--r-- 1 root root 8 Oct 29 21 :00 .eslintignore -rw-r--r-- 1 root root 27 Oct 29 21 :00 .eslintrc -rw-r--r-- 1 root root 151 Oct 29 21 :00 .gitignore -rw-r--r-- 1 root root 30 Oct 29 21 :00 .yo-rc.json -rw-r--r-- 1 root root 105 Oct 29 21 :00 Dockerfile root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# ls -alt client/images total 8 drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 02 :02 . root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .9G 89G 6 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .9G 89G 6 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 0 20G 0 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-cdhr5:/home/node/app# exit Note the filesystem fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 is mounted on path /home/node/app/client/images . Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" Verify that the images are missing by viewing the data from the Guestbook application. Click on the hyperlink labled images at the bottom of the guestbook home page. The images.html page shows images with broken links.","title":"Use the volume in the Guestbook application"},{"location":"generatedContent/kubernetes-storage/Lab2/#load-the-file-storage-with-images","text":"Run the kubectl cp command to move the images into the mounted volume. cd $HOME /guestbook-config/storage/lab2 kubectl cp images $POD1 :/home/node/app/client/ Refresh the page images.html page in the guestbook application to view the uploaded images.","title":"Load the file storage with images"},{"location":"generatedContent/kubernetes-storage/Lab2/#shared-storage-across-pods","text":"Login into the other pod $POD2 to verify the volume mount. kubectl exec -it $POD2 -- bash root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# ls -alt /home/node/app/client/images total 160 -rw-r--r-- 1 501 staff 56191 Nov 13 03 :44 gb3.jpg drwxr-xr-x 2 nobody 4294967294 4096 Nov 13 03 :44 . -rw-r--r-- 1 501 staff 21505 Nov 13 03 :44 gb2.jpg -rw-r--r-- 1 501 staff 58286 Nov 13 03 :44 gb1.jpg drwxr-xr-x 1 root root 4096 Nov 13 03 :17 .. root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# df -h Filesystem Size Used Avail Use% Mounted on overlay 98G 4 .2G 89G 5 % / tmpfs 64M 0 64M 0 % /dev tmpfs 7 .9G 0 7 .9G 0 % /sys/fs/cgroup shm 64M 0 64M 0 % /dev/shm /dev/mapper/docker_data 98G 4 .2G 89G 5 % /etc/hosts tmpfs 7 .9G 16K 7 .9G 1 % /run/secrets/kubernetes.io/serviceaccount fsf-dal1003d-fz.adn.networklayer.com:/IBM02SEV2058850_2177/data01 20G 128K 20G 1 % /home/node/app/client/images tmpfs 7 .9G 0 7 .9G 0 % /proc/acpi tmpfs 7 .9G 0 7 .9G 0 % /proc/scsi tmpfs 7 .9G 0 7 .9G 0 % /sys/firmware root@guestbook-v1-5bd76b568f-w6h6h:/home/node/app# exit Note that the volume and the data are available on all the pods running the Guestbook application. IBM Cloud File Storage is a NFS-based file storage that is available across all worker nodes within a single availability zone. If you are running a cluster with multiple nodes (within a single AZ) you can run the following commands to prove that your data is available across different nodes: kubectl get pods -o wide kubectl get nodes Expected output: $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES guestbook-v1-6fb8b86876-n9jtz 1 /1 Running 0 39h 172 .30.224.70 10 .38.216.205 <none> <none> guestbook-v1-6fb8b86876-njwcz 1 /1 Running 0 39h 172 .30.169.144 10 .38.216.238 <none> <none> $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10 .38.216.205 Ready <none> 4d5h v1.18.10+IKS 10 .38.216.238 Ready <none> 4d5h v1.18.10+IKS To extend our table from Lab 1 we now have: Storage Type Persisted at which level Example Uses Container local storage Container ephermal state Secondary Storage ( EmptyDir ) Pod Checkpoint a long computation process Primary Storage ( HostPath ) Node Running cAdvisor in a container IBM Cloud File Storage (NFS) Availabilty Zone Applications running in a single availabilty zone Data is available to all nodes within the availability zone where the file storage exists, but the accessMode parameter on the PersistentVolumeClaim determines if multiple pods are able to mount a volume specificed by a PVC. The possible values for this parameter are: ReadWriteMany : The PVC can be mounted by multiple pods. All pods can read from and write to the volume. ReadOnlyMany : The PVC can be mounted by multiple pods. All pods have read-only access. ReadWriteOnce : The PVC can be mounted by one pod only. This pod can read from and write to the volume.","title":"Shared storage across pods"},{"location":"generatedContent/kubernetes-storage/Lab2/#optional-exercises","text":"Another way to see that the data is persisted at the availability zone level, you can: Back up data. Delete pods to confirm that it does not impact the data used by the application. Delete the Kubernetes cluster. Create a new cluster and reuse the volume.","title":"[Optional exercises]"},{"location":"generatedContent/kubernetes-storage/Lab2/#clean-up","text":"List all the PVCs and PVs kubectl get pvc kubectl get pv Delete all the pods using the PVC. Delete the PVC kubectl delete pvc guestbook-pvc persistentvolumeclaim \"guestbook-pvc\" deleted List PV to ensure that it is removed as well. Cancel the physical storage volume from the cloud account. (Note: requires admin permissions?) ibmcloud sl file volume-list --columns id --columns notes | grep pvc-6362f614-258e-48ee-a596-62bb4629cd75 183223942 { \"plugin\" : \"ibm-file-plugin-7b9db9c79f-86x8w\" , \"region\" : \"us-south\" , \"cluster\" : \"bugql3nd088jsp8iiagg\" , \"type\" : \"Endurance\" , \"ns\" : \"default\" , \"pvc\" : \"guestbook-pvc\" , \"pv\" : \"pvc-6362f614-258e-48ee-a596-62bb4629cd75\" , \"storageclass\" : \"ibmc-file-silver\" , \"reclaim\" : \"Delete\" } ibmcloud sl file volume-cancel 183223942 This will cancel the file volume: 183223942 and cannot be undone. Continue?> yes Failed to cancel file volume: 183223942 . No billing item is found to cancel.","title":"Clean up"},{"location":"generatedContent/kubernetes-storage/Lab3/","text":"Lab 3. Using IBM Cloud Block Storage with Kubernetes \u00b6 Introduction \u00b6 When looking at what kind of storage class you would like to use in Kubernetes, there are are a few choices such as file storage, block storage, object storage, etc. If your use case requires fast and reliable data access then consider block storage. Block storage is a storage option that breaks data into \"blocks\" and stores those blocks across a Storage Area Network (SAN). These smaller blocks are faster to store and retrieve than large data objects. For this reason, block storage is primarily used as a backing storage for databases. In this lab we will deploy a Mongo database on top of block storage on Kubernetes. The basic architecture is as follows When we install MongoDB with the helm chart, a Persistent Volume Claim (PVC) is created on the cluster. This PVC is a request for storage to be used by the application. In IBM Cloud, the request goes to the IBM Cloud storage provider which then provisions a physical storage device within IBM Cloud. A Persistent Volume (PV) is then created which acts as a reference to the physical storage device created earlier. This PV is then mounted as a directory in a container's file system. The guestbook application receives requests to store guestbook entries from the user which the guestbook pod then sends to the MongoDB pod to store. The MongoDB pod receives the request to store information and persists the data to the mounted directory from the Persistent Volume. Setup \u00b6 Before we get into the lab we first need to do some setup to ensure that the lab will flow smoothly. <!-- 1. Replace <docker username> with your DockerHub username and run the following command (be sure to replace the < > too!). DOCKERUSER = <docker username> ``` --> 1 . In your terminal, navigate to where you would like to store the files used in this lab and run the following. ``` bash WORK_DIR = ` pwd ` Ensure that you have run through the prerequistes in Lab0 Using IBM Cloud Block Storage with Kubernetes \u00b6 Log into the Kubernetes cluster and create a project where we want to deploy our application. kubectl create namespace mongo Install Block Storage Plugin \u00b6 By default IBM Kubernetes Service Clusters don't have the option to deploy block storage persistent volumes. However, there is an easy process to add the block storage storageClass to your cluster through the use of an automated helm chart install. Follow the steps outlined here to install the block storage storageClass . First you need to add the iks-charts helm repo to your local helm repos. This will allow you to utilize a variety of charts to install software on the IBM Kubernetes Service. helm repo add iks-charts https://icr.io/helm/iks-charts Then, we need to update the repo to ensure that we have the latest charts: helm repo update Install the block storage plugin from the iks-charts repo: helm install block-storage-plugin iks-charts/ibmcloud-block-storage-plugin Lastly, verify that the plugin installation was successful by retrieving the list of storage classes in the cluster: kubectl get storageclasses You should notice a few options that start with ibmc-block as seen below. NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ibmc-block-bronze ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-custom ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-gold ibm.io/ibmc-block Delete Immediate true 62s Helm Repo setup \u00b6 The lab uses Bitnami's Mongodb Helm chart to show case the use of block storage. Set the Bitnami helm repo prior to installing mongodb. helm repo add bitnami https://charts.bitnami.com/bitnami Expected output: $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories Validate the repo is available in the list. helm repo list You should see a list of repos available to you as seen below: NAME URL bitnami https://charts.bitnami.com/bitnami iks-charts https://icr.io/helm/iks-charts Mongodb with block storage \u00b6 Installation Dry Run \u00b6 Before we install MongoDB, let's do a test of the installation to see what the chart will create. Since we are using Helm to install MongoDB, we can make use of the --dry-run flag in our helm install command to show us the manifest files that Helm will apply on the cluster. Dryrun: helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = testing,auth.username = guestbookAdmin,auth.database = guestbook -n mongo --dry-run > mongdb-install-dryrun.yaml There is a detailed breakdown of this command in the next section titled Installing MongoDB if you would like to understand what this helm command is doing. This command will test out our helm install command and save the output manifests in a file called mongodb-install-dryrun.yaml . You can then examine this manifest file so that you know exactly what will be installed on your cluster. Check out the file in your code editor and take a look at the PersistentVolumeClaim object. There should be a property named storageClassName in the spec and the value should be ibmc-block-gold to signify that we will be using block storage for our database. Below is what that PersistentVolumeClaim object should look like. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : mongo-mongodb namespace : mongo labels : app.kubernetes.io/name : mongodb helm.sh/chart : mongodb-10.0.4 app.kubernetes.io/instance : mongo app.kubernetes.io/managed-by : Helm app.kubernetes.io/component : mongodb spec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"8Gi\" storageClassName : ibmc-block-gold Install Mongodb \u00b6 Before we install MongoDB we need to generate a password for our database credentials. These credentials will be used in the application to authenticate with the database. For this lab, will be using the openssl tool to generate the password as this is a common open source cryptographic library. The rest of the command will strip out any characters that could cause issues with the password. USER_PASS = ` openssl rand -base64 12 | tr -d \"=+/\" ` Now we can install MongoDB and supply the password that we just generated. helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = $USER_PASS ,auth.username = guestbook-admin,auth.database = guestbook -n mongo Here's an explanation of the above command: helm install mongo bitnami/mongo : Install the MongoDB bitnami helm chart and name the release \"mongo\". --set global.storageClass=ibmc-block-gold : Set the storage class to block storage rather than the default file storage. ... auth.password=$USER_PASS : Create a custom user with this password (Which we generated earlier). ... auth.username=guestbook-admin : Create a custom user with this username. ... auth.database=guestbook : Create a database named guestbook that the custom user can authenticate to. -n mongo : Install this release in the mongo namespace. Expected output: NAME: mongo LAST DEPLOYED: Tue Nov 24 10 :41:15 2020 NAMESPACE: mongo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ... View the objects being created by the helm chart. kubectl get all -n mongo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 0 /1 0 0 17s NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-6f8f7cd789 1 0 0 17s View the list of persistence volume claims. Note that the mongo-mongodb is pending volume allocation. kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Pending ibmc-block-gold 21s After waiting for some time. The pod supporting Mongodb should have a Running status. $ kubectl get all -n mongo NAME READY STATUS RESTARTS AGE pod/mongo-mongodb-66d7bcd7cf-vqvbj 1 /1 Running 0 8m37s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 12m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 1 /1 1 1 12m NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-66d7bcd7cf 1 1 1 8m37s replicaset.apps/mongo-mongodb-6f8f7cd789 0 0 0 12m And the PVC mongo-mongodb is now bound to volume pvc-2f423668-4f87-4ae4-8edf-8c892188b645 $ kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Bound pvc-2f423668-4f87-4ae4-8edf-8c892188b645 20Gi RWO ibmc-block-gold 2m26s With MongoDB deployed now we need to deploy an application that will utilize it as a datastore. Building Guestbook \u00b6 For this lab we will be using the guestbook application which is a common sample kubernetes application. However, the version that we are using has been refactored as a loopback application. Clone the application repo and the configuration repo. In your terminal, run the following: cd $WORK_DIR git clone https://github.com/IBM/guestbook-nodejs.git git clone https://github.com/IBM/guestbook-nodejs-config/ --branch mongo Then, navigate into the guestbook-nodejs directory. cd $WORK_DIR /guestbook-nodejs/src Replace the code in the server/datasources.json file with the following: { \"in-memory\" : { \"name\" : \"in-memory\" , \"localStorage\" : \"\" , \"file\" : \"\" , \"connector\" : \"memory\" }, \"mongo\" : { \"host\" : \"${MONGO_HOST}\" , \"port\" : \"${MONGO_PORT}\" , \"url\" : \"\" , \"database\" : \"${MONGO_DB}\" , \"password\" : \"${MONGO_PASS}\" , \"name\" : \"mongo\" , \"user\" : \"${MONGO_USER}\" , \"useNewUrlParser\" : true , \"connector\" : \"mongodb\" } } This file will contain the connection information to our MongoDB instance. These variables will be passed into the environment from ConfigMaps and Secrets that we will create. Open the server/model-config.json file and change the entry.datasource value to mongo as seen below: ... \"entry\" : { \"dataSource\" : \"mongo\" , \"public\" : true } } In this file we are telling the application which datasource we should use; in-memory or MongoDB. By default the application comes with an in-memory datastore for storing information but this data does not persist after the application crashes or if the pod goes down for any reason. We are changing in-memory to mongo so that the data will persist in our MongoDB instance external to the application so that the data will remain even after the application crashes. Now we need to build our application image and push it to DockerHub. cd $WORK_DIR /guestbook-nodejs/src IMAGE_NAME = $DOCKERUSER /guestbook-nodejs:mongo docker build -t $IMAGE_NAME . docker login -u $DOCKERUSER docker push $IMAGE_NAME Deploying Guestbook \u00b6 Now that we have built our application, let's check out the manifest files needed to deploy it to Kubernetes. Navigate to the configuration repo that we cloned earlier. cd $WORK_DIR /guestbook-nodejs-config This repo contains 3 manifests that we will be deploying to our cluster today: A deployment manifest A service manifest A configMap manifest These manifests will create their respective kubernetes objects on our cluster. The deployment will deploy our application image that we built earlier while the service will expose that application to external traffic. The configMap will contain connection information for our database such as database hostname and port. Open the guestbook-deployment.yaml file and edit line 25 to point to the image that you built and pushed earlier. Do this by replacing <DockerUsername> with your docker username. (Don't forget to replace the < > too!) For example, my Docker username is odrodrig so line 25 in my guestbook-deployment.yaml file would look like this: ... image : odrodrig/guestbook-nodejs:mongo ... As part of the deployment, kubernetes will copy the database connection information from the configMap into the environment of the application. You can see where this is specified in the env section of the deployment manifest as seen below: ... env : - name : MONGO_HOST valueFrom : configMapKeyRef : name : mongo-config key : mongo_host - name : MONGO_PORT valueFrom : configMapKeyRef : name : mongo-config key : mongo_port - name : MONGO_USER valueFrom : secretKeyRef : name : mongodb key : username - name : MONGO_PASS valueFrom : secretKeyRef : name : mongodb key : password - name : MONGO_DB valueFrom : configMapKeyRef : name : mongo-config key : mongo_db_name You might also notice that we are getting our database username ( MONGO_USER ) and password ( MONGO_PASS ) from a kubernetes secret. We haven't defined that secret yet so let's do it now. kubectl create secret generic mongodb --from-literal = username = guestbook-admin --from-literal = password = $USER_PASS -n mongo Now we are ready to deploy the application. Run the following commands: cd $WORK_DIR /guestbook-nodejs-config/ kubectl apply -f . -n mongo Ensure that the application pod is running: kubectl get pods -n mongo You should see both the mongo pod and the guestbook pod running now: NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-zdhqv 1 /1 Running 0 19s mongo-mongodb-757d9777d7-j4759 1 /1 Running 0 27m Test out the application \u00b6 Now that we have deployed the application, let's test it out. Find the URL for the guestbook application by joining the worker node external IP and service node port. Run the following to get the IP and service node port of the application: HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -n mongo -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" In your browser, open up the address that was output as part of the previous command. Type in a few test entries in the text box and press enter to submit them. These entries are now saved in the Mongo database. Let's take down the application and see if the data will truly persist. Find the name of the pod that is running our application: kubectl get pods -n mongo Copy the name of the pod that starts with guestbook . For me, the pod is named guestbook-v1-9465dcbb4-f6s9h . NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-f6s9h 1 /1 Running 0 4m7s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 5m47s Then, run the following command, replacing <pod name> with pod name that you just copied. kubectl delete pod -n mongo <pod name> You should then see a message saying that your pod has been deleted. $ kubectl delete pod -n mongo guestbook-v1-9465dcbb4-f6s9h pod \"guestbook-v1-9465dcbb4-f6s9h\" deleted Now, view your pods again: kubectl get pods -n mongo You should see the guestbook pod is back now with and the age has been reset. This means that it is a brand new pod that kubernetes has deployed automatically after our previous pod was deleted. NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-8z8bt 1 /1 Running 0 87s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 9m13s Refresh your browser tab that had the guestbook application and you will see that your data has indeed persisted after our pod went down. Summary \u00b6 In this lab we used block storage to run our own database on Kubernetes. Block storage allows for fast I/O operations making it ideal for our application database. We utilized configMaps and secrets to store the database configuration making it easy to use this application with different database configurations without making code changes. Cleanup (Optional) \u00b6 This part of the lab desrcibes the steps to delete what was built in the lab. Deleting the application \u00b6 cd $WORK_DIR /guestbook-nodejs-config kubectl delete -f . -n mongo Uninstalling Mongo \u00b6 helm uninstall mongo -n mongo Remove namespace \u00b6 kubectl delete namespace mongo","title":"Lab 3. Using IBM Cloud Block Storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab3/#lab-3-using-ibm-cloud-block-storage-with-kubernetes","text":"","title":"Lab 3. Using IBM Cloud Block Storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab3/#introduction","text":"When looking at what kind of storage class you would like to use in Kubernetes, there are are a few choices such as file storage, block storage, object storage, etc. If your use case requires fast and reliable data access then consider block storage. Block storage is a storage option that breaks data into \"blocks\" and stores those blocks across a Storage Area Network (SAN). These smaller blocks are faster to store and retrieve than large data objects. For this reason, block storage is primarily used as a backing storage for databases. In this lab we will deploy a Mongo database on top of block storage on Kubernetes. The basic architecture is as follows When we install MongoDB with the helm chart, a Persistent Volume Claim (PVC) is created on the cluster. This PVC is a request for storage to be used by the application. In IBM Cloud, the request goes to the IBM Cloud storage provider which then provisions a physical storage device within IBM Cloud. A Persistent Volume (PV) is then created which acts as a reference to the physical storage device created earlier. This PV is then mounted as a directory in a container's file system. The guestbook application receives requests to store guestbook entries from the user which the guestbook pod then sends to the MongoDB pod to store. The MongoDB pod receives the request to store information and persists the data to the mounted directory from the Persistent Volume.","title":"Introduction"},{"location":"generatedContent/kubernetes-storage/Lab3/#setup","text":"Before we get into the lab we first need to do some setup to ensure that the lab will flow smoothly. <!-- 1. Replace <docker username> with your DockerHub username and run the following command (be sure to replace the < > too!). DOCKERUSER = <docker username> ``` --> 1 . In your terminal, navigate to where you would like to store the files used in this lab and run the following. ``` bash WORK_DIR = ` pwd ` Ensure that you have run through the prerequistes in Lab0","title":"Setup"},{"location":"generatedContent/kubernetes-storage/Lab3/#using-ibm-cloud-block-storage-with-kubernetes","text":"Log into the Kubernetes cluster and create a project where we want to deploy our application. kubectl create namespace mongo","title":"Using IBM Cloud Block Storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab3/#install-block-storage-plugin","text":"By default IBM Kubernetes Service Clusters don't have the option to deploy block storage persistent volumes. However, there is an easy process to add the block storage storageClass to your cluster through the use of an automated helm chart install. Follow the steps outlined here to install the block storage storageClass . First you need to add the iks-charts helm repo to your local helm repos. This will allow you to utilize a variety of charts to install software on the IBM Kubernetes Service. helm repo add iks-charts https://icr.io/helm/iks-charts Then, we need to update the repo to ensure that we have the latest charts: helm repo update Install the block storage plugin from the iks-charts repo: helm install block-storage-plugin iks-charts/ibmcloud-block-storage-plugin Lastly, verify that the plugin installation was successful by retrieving the list of storage classes in the cluster: kubectl get storageclasses You should notice a few options that start with ibmc-block as seen below. NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ibmc-block-bronze ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-custom ibm.io/ibmc-block Delete Immediate true 62s ibmc-block-gold ibm.io/ibmc-block Delete Immediate true 62s","title":"Install Block Storage Plugin"},{"location":"generatedContent/kubernetes-storage/Lab3/#helm-repo-setup","text":"The lab uses Bitnami's Mongodb Helm chart to show case the use of block storage. Set the Bitnami helm repo prior to installing mongodb. helm repo add bitnami https://charts.bitnami.com/bitnami Expected output: $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories Validate the repo is available in the list. helm repo list You should see a list of repos available to you as seen below: NAME URL bitnami https://charts.bitnami.com/bitnami iks-charts https://icr.io/helm/iks-charts","title":"Helm Repo setup"},{"location":"generatedContent/kubernetes-storage/Lab3/#mongodb-with-block-storage","text":"","title":"Mongodb with block storage"},{"location":"generatedContent/kubernetes-storage/Lab3/#installation-dry-run","text":"Before we install MongoDB, let's do a test of the installation to see what the chart will create. Since we are using Helm to install MongoDB, we can make use of the --dry-run flag in our helm install command to show us the manifest files that Helm will apply on the cluster. Dryrun: helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = testing,auth.username = guestbookAdmin,auth.database = guestbook -n mongo --dry-run > mongdb-install-dryrun.yaml There is a detailed breakdown of this command in the next section titled Installing MongoDB if you would like to understand what this helm command is doing. This command will test out our helm install command and save the output manifests in a file called mongodb-install-dryrun.yaml . You can then examine this manifest file so that you know exactly what will be installed on your cluster. Check out the file in your code editor and take a look at the PersistentVolumeClaim object. There should be a property named storageClassName in the spec and the value should be ibmc-block-gold to signify that we will be using block storage for our database. Below is what that PersistentVolumeClaim object should look like. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : mongo-mongodb namespace : mongo labels : app.kubernetes.io/name : mongodb helm.sh/chart : mongodb-10.0.4 app.kubernetes.io/instance : mongo app.kubernetes.io/managed-by : Helm app.kubernetes.io/component : mongodb spec : accessModes : - \"ReadWriteOnce\" resources : requests : storage : \"8Gi\" storageClassName : ibmc-block-gold","title":"Installation Dry Run"},{"location":"generatedContent/kubernetes-storage/Lab3/#install-mongodb","text":"Before we install MongoDB we need to generate a password for our database credentials. These credentials will be used in the application to authenticate with the database. For this lab, will be using the openssl tool to generate the password as this is a common open source cryptographic library. The rest of the command will strip out any characters that could cause issues with the password. USER_PASS = ` openssl rand -base64 12 | tr -d \"=+/\" ` Now we can install MongoDB and supply the password that we just generated. helm install mongo bitnami/mongodb --set global.storageClass = ibmc-block-gold,auth.password = $USER_PASS ,auth.username = guestbook-admin,auth.database = guestbook -n mongo Here's an explanation of the above command: helm install mongo bitnami/mongo : Install the MongoDB bitnami helm chart and name the release \"mongo\". --set global.storageClass=ibmc-block-gold : Set the storage class to block storage rather than the default file storage. ... auth.password=$USER_PASS : Create a custom user with this password (Which we generated earlier). ... auth.username=guestbook-admin : Create a custom user with this username. ... auth.database=guestbook : Create a database named guestbook that the custom user can authenticate to. -n mongo : Install this release in the mongo namespace. Expected output: NAME: mongo LAST DEPLOYED: Tue Nov 24 10 :41:15 2020 NAMESPACE: mongo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ... View the objects being created by the helm chart. kubectl get all -n mongo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 17s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 0 /1 0 0 17s NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-6f8f7cd789 1 0 0 17s View the list of persistence volume claims. Note that the mongo-mongodb is pending volume allocation. kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Pending ibmc-block-gold 21s After waiting for some time. The pod supporting Mongodb should have a Running status. $ kubectl get all -n mongo NAME READY STATUS RESTARTS AGE pod/mongo-mongodb-66d7bcd7cf-vqvbj 1 /1 Running 0 8m37s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/mongo-mongodb ClusterIP 172 .21.242.70 <none> 27017 /TCP 12m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mongo-mongodb 1 /1 1 1 12m NAME DESIRED CURRENT READY AGE replicaset.apps/mongo-mongodb-66d7bcd7cf 1 1 1 8m37s replicaset.apps/mongo-mongodb-6f8f7cd789 0 0 0 12m And the PVC mongo-mongodb is now bound to volume pvc-2f423668-4f87-4ae4-8edf-8c892188b645 $ kubectl get pvc -n mongo NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mongo-mongodb Bound pvc-2f423668-4f87-4ae4-8edf-8c892188b645 20Gi RWO ibmc-block-gold 2m26s With MongoDB deployed now we need to deploy an application that will utilize it as a datastore.","title":"Install Mongodb"},{"location":"generatedContent/kubernetes-storage/Lab3/#building-guestbook","text":"For this lab we will be using the guestbook application which is a common sample kubernetes application. However, the version that we are using has been refactored as a loopback application. Clone the application repo and the configuration repo. In your terminal, run the following: cd $WORK_DIR git clone https://github.com/IBM/guestbook-nodejs.git git clone https://github.com/IBM/guestbook-nodejs-config/ --branch mongo Then, navigate into the guestbook-nodejs directory. cd $WORK_DIR /guestbook-nodejs/src Replace the code in the server/datasources.json file with the following: { \"in-memory\" : { \"name\" : \"in-memory\" , \"localStorage\" : \"\" , \"file\" : \"\" , \"connector\" : \"memory\" }, \"mongo\" : { \"host\" : \"${MONGO_HOST}\" , \"port\" : \"${MONGO_PORT}\" , \"url\" : \"\" , \"database\" : \"${MONGO_DB}\" , \"password\" : \"${MONGO_PASS}\" , \"name\" : \"mongo\" , \"user\" : \"${MONGO_USER}\" , \"useNewUrlParser\" : true , \"connector\" : \"mongodb\" } } This file will contain the connection information to our MongoDB instance. These variables will be passed into the environment from ConfigMaps and Secrets that we will create. Open the server/model-config.json file and change the entry.datasource value to mongo as seen below: ... \"entry\" : { \"dataSource\" : \"mongo\" , \"public\" : true } } In this file we are telling the application which datasource we should use; in-memory or MongoDB. By default the application comes with an in-memory datastore for storing information but this data does not persist after the application crashes or if the pod goes down for any reason. We are changing in-memory to mongo so that the data will persist in our MongoDB instance external to the application so that the data will remain even after the application crashes. Now we need to build our application image and push it to DockerHub. cd $WORK_DIR /guestbook-nodejs/src IMAGE_NAME = $DOCKERUSER /guestbook-nodejs:mongo docker build -t $IMAGE_NAME . docker login -u $DOCKERUSER docker push $IMAGE_NAME","title":"Building Guestbook"},{"location":"generatedContent/kubernetes-storage/Lab3/#deploying-guestbook","text":"Now that we have built our application, let's check out the manifest files needed to deploy it to Kubernetes. Navigate to the configuration repo that we cloned earlier. cd $WORK_DIR /guestbook-nodejs-config This repo contains 3 manifests that we will be deploying to our cluster today: A deployment manifest A service manifest A configMap manifest These manifests will create their respective kubernetes objects on our cluster. The deployment will deploy our application image that we built earlier while the service will expose that application to external traffic. The configMap will contain connection information for our database such as database hostname and port. Open the guestbook-deployment.yaml file and edit line 25 to point to the image that you built and pushed earlier. Do this by replacing <DockerUsername> with your docker username. (Don't forget to replace the < > too!) For example, my Docker username is odrodrig so line 25 in my guestbook-deployment.yaml file would look like this: ... image : odrodrig/guestbook-nodejs:mongo ... As part of the deployment, kubernetes will copy the database connection information from the configMap into the environment of the application. You can see where this is specified in the env section of the deployment manifest as seen below: ... env : - name : MONGO_HOST valueFrom : configMapKeyRef : name : mongo-config key : mongo_host - name : MONGO_PORT valueFrom : configMapKeyRef : name : mongo-config key : mongo_port - name : MONGO_USER valueFrom : secretKeyRef : name : mongodb key : username - name : MONGO_PASS valueFrom : secretKeyRef : name : mongodb key : password - name : MONGO_DB valueFrom : configMapKeyRef : name : mongo-config key : mongo_db_name You might also notice that we are getting our database username ( MONGO_USER ) and password ( MONGO_PASS ) from a kubernetes secret. We haven't defined that secret yet so let's do it now. kubectl create secret generic mongodb --from-literal = username = guestbook-admin --from-literal = password = $USER_PASS -n mongo Now we are ready to deploy the application. Run the following commands: cd $WORK_DIR /guestbook-nodejs-config/ kubectl apply -f . -n mongo Ensure that the application pod is running: kubectl get pods -n mongo You should see both the mongo pod and the guestbook pod running now: NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-zdhqv 1 /1 Running 0 19s mongo-mongodb-757d9777d7-j4759 1 /1 Running 0 27m","title":"Deploying Guestbook"},{"location":"generatedContent/kubernetes-storage/Lab3/#test-out-the-application","text":"Now that we have deployed the application, let's test it out. Find the URL for the guestbook application by joining the worker node external IP and service node port. Run the following to get the IP and service node port of the application: HOSTNAME = ` kubectl get nodes -ojsonpath = '{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}' ` SERVICEPORT = ` kubectl get svc guestbook -n mongo -o = jsonpath = '{.spec.ports[0].nodePort}' ` echo \"http:// $HOSTNAME : $SERVICEPORT \" In your browser, open up the address that was output as part of the previous command. Type in a few test entries in the text box and press enter to submit them. These entries are now saved in the Mongo database. Let's take down the application and see if the data will truly persist. Find the name of the pod that is running our application: kubectl get pods -n mongo Copy the name of the pod that starts with guestbook . For me, the pod is named guestbook-v1-9465dcbb4-f6s9h . NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-f6s9h 1 /1 Running 0 4m7s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 5m47s Then, run the following command, replacing <pod name> with pod name that you just copied. kubectl delete pod -n mongo <pod name> You should then see a message saying that your pod has been deleted. $ kubectl delete pod -n mongo guestbook-v1-9465dcbb4-f6s9h pod \"guestbook-v1-9465dcbb4-f6s9h\" deleted Now, view your pods again: kubectl get pods -n mongo You should see the guestbook pod is back now with and the age has been reset. This means that it is a brand new pod that kubernetes has deployed automatically after our previous pod was deleted. NAME READY STATUS RESTARTS AGE guestbook-v1-9465dcbb4-8z8bt 1 /1 Running 0 87s mongo-mongodb-757d9777d7-q64lg 1 /1 Running 0 9m13s Refresh your browser tab that had the guestbook application and you will see that your data has indeed persisted after our pod went down.","title":"Test out the application"},{"location":"generatedContent/kubernetes-storage/Lab3/#summary","text":"In this lab we used block storage to run our own database on Kubernetes. Block storage allows for fast I/O operations making it ideal for our application database. We utilized configMaps and secrets to store the database configuration making it easy to use this application with different database configurations without making code changes.","title":"Summary"},{"location":"generatedContent/kubernetes-storage/Lab3/#cleanup-optional","text":"This part of the lab desrcibes the steps to delete what was built in the lab.","title":"Cleanup (Optional)"},{"location":"generatedContent/kubernetes-storage/Lab3/#deleting-the-application","text":"cd $WORK_DIR /guestbook-nodejs-config kubectl delete -f . -n mongo","title":"Deleting the application"},{"location":"generatedContent/kubernetes-storage/Lab3/#uninstalling-mongo","text":"helm uninstall mongo -n mongo","title":"Uninstalling Mongo"},{"location":"generatedContent/kubernetes-storage/Lab3/#remove-namespace","text":"kubectl delete namespace mongo","title":"Remove namespace"},{"location":"generatedContent/kubernetes-storage/Lab4/","text":"Lab 1. Title \u00b6 Let's investigate how Helm can help us focus on other things by letting a chart do the work for us. We'll first deploy an application to a Kubernetes cluster by using kubectl and then show how we can offload the work to a chart by deploying the same app with Helm. The application is the Guestbook App , which is a sample multi-tier web application. Scenario 1: Deploy the application using kubectl \u00b6 git clone https://github.com/IBM/workshop-template cd workshop-template","title":"Lab 1. Title"},{"location":"generatedContent/kubernetes-storage/Lab4/#lab-1-title","text":"Let's investigate how Helm can help us focus on other things by letting a chart do the work for us. We'll first deploy an application to a Kubernetes cluster by using kubectl and then show how we can offload the work to a chart by deploying the same app with Helm. The application is the Guestbook App , which is a sample multi-tier web application.","title":"Lab 1. Title"},{"location":"generatedContent/kubernetes-storage/Lab4/#scenario-1-deploy-the-application-using-kubectl","text":"git clone https://github.com/IBM/workshop-template cd workshop-template","title":"Scenario 1: Deploy the application using kubectl"},{"location":"generatedContent/kubernetes-storage/Lab5/","text":"Object Storage with Kubernetes \u00b6 About this Lab \u00b6 This hands-on lab for object storage on Kubernetes steps you through the creation and configuration of persistent storage for MongoDB using an encrypted IBM Cloud Object Storage bucket on IBM Cloud Object Storage. We use the IBM Cloud Object Storage plugin to enable Kubernetes pods to access IBM Cloud Object Storage buckets using the cloud native PersistentVolume (PV) and PersistentVolumeClaim (PVC) resources. The plugin has two components: a dynamic provisioner and a FlexVolume driver for mounting the buckets using s3fs-fuse on a worker node. FlexVolume and the Container Storage Interface (CSI) are so-called out-of-tree volume plugins. Out-of-tree volume plugins enable storage developers to create custom storage plugins not included ( in-tree ) in the core Kubernetes APIs. For background information about FlexVolume , go to the flexvolume readme. The FlexVolume driver uses s3fs to allow Linux and macOS to mount an S3 bucket via FUSE. If you want to learn more about s3fs-fuse and how FUSE works, you can do the additional s3fs lab . This Object Storage lab consists of the following steps: To setup client CLI and Kubernetes cluster, go to Setup , To learn more about what Object Storage is, go to About Object Storage Create Object Storage instance, go here , Configure your Kubernetes Cluster, go here , To configure the IBM Cloud Object Storage plugin , go here , Create the PersistentVolumeClaim with dynamic provisioning using the ibmc plugin, go here . Deploy MongoDB using Object Storage, go here . Start with Setup . Other Labs \u00b6 Related labs using Object Storage are: [Optional] (TBD) Deploy Guestbook with MongoDB and Object Storage. Share Documents using Cloud Object Storage . Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS Next \u00b6 1. Setup","title":"Object Storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab5/#object-storage-with-kubernetes","text":"","title":"Object Storage with Kubernetes"},{"location":"generatedContent/kubernetes-storage/Lab5/#about-this-lab","text":"This hands-on lab for object storage on Kubernetes steps you through the creation and configuration of persistent storage for MongoDB using an encrypted IBM Cloud Object Storage bucket on IBM Cloud Object Storage. We use the IBM Cloud Object Storage plugin to enable Kubernetes pods to access IBM Cloud Object Storage buckets using the cloud native PersistentVolume (PV) and PersistentVolumeClaim (PVC) resources. The plugin has two components: a dynamic provisioner and a FlexVolume driver for mounting the buckets using s3fs-fuse on a worker node. FlexVolume and the Container Storage Interface (CSI) are so-called out-of-tree volume plugins. Out-of-tree volume plugins enable storage developers to create custom storage plugins not included ( in-tree ) in the core Kubernetes APIs. For background information about FlexVolume , go to the flexvolume readme. The FlexVolume driver uses s3fs to allow Linux and macOS to mount an S3 bucket via FUSE. If you want to learn more about s3fs-fuse and how FUSE works, you can do the additional s3fs lab . This Object Storage lab consists of the following steps: To setup client CLI and Kubernetes cluster, go to Setup , To learn more about what Object Storage is, go to About Object Storage Create Object Storage instance, go here , Configure your Kubernetes Cluster, go here , To configure the IBM Cloud Object Storage plugin , go here , Create the PersistentVolumeClaim with dynamic provisioning using the ibmc plugin, go here . Deploy MongoDB using Object Storage, go here . Start with Setup .","title":"About this Lab"},{"location":"generatedContent/kubernetes-storage/Lab5/#other-labs","text":"Related labs using Object Storage are: [Optional] (TBD) Deploy Guestbook with MongoDB and Object Storage. Share Documents using Cloud Object Storage . Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS","title":"Other Labs"},{"location":"generatedContent/kubernetes-storage/Lab5/#next","text":"1. Setup","title":"Next"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/","text":"Lab 5: Add Object Storage to a Persistent Database \u00b6 About this Lab \u00b6 This hands-on lab for object storage on Kubernetes is called Add Object Storage to a Persistent Database and steps you through the setup and configuration of persistent storage for MongoDB using IBM Cloud Object Storage. This lab uses the IBM Cloud Object Storage plugin to mount an Object Storage bucket to a Kubernetes cluster using PersistentVolume by dynamic provisioning . A MongoDB database is setup that persists its data to the encrypted IBM Cloud Object Storage bucket using a PersistentVolumeClaim . IBM Cloud Object Storage plugin is a Kubernetes volume plugin that enables Kubernetes pods to access IBM Cloud Object Storage buckets. The plugin has two components: a dynamic provisioner and a FlexVolume driver for mounting the buckets using s3fs-fuse on a worker node. FlexVolume is a so-called out-of-tree volume plugin, as is the Container Storage Interface (CSI) . Out-of-tree volume plugins enable storage developers to create custom storage plugins. For more information about FlexVolume , go to flexvolume . s3fs allows Linux and macOS to mount an S3 bucket via FUSE. If you want to learn more about s3fs-fuse and FUSE you can do the optional s3fs lab . The lab consists of the following steps: Setup client CLI and Kubernetes cluster, Go to Setup , Create an Object Storage instance, go to Object Storage , Configure the Kubernetes cluster, go to Configure your Kubernetes Cluster , Deploy and configure the IBM Cloud Object Storage plugin , go to Cloud Object Storage plugin , Create a PersistentVolumeClaim with dynamic provisioning using the ibmc plugin, go to Create the PersistentVolumeClaim . Install MongoDB with Object Storage, go to MongoDB . Start with Setup . Other Labs \u00b6 Related labs using Object Storage are: [Optional] (TBD) Deploy Guestbook with MongoDB and Object Storage. Share Documents using Cloud Object Storage . Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS Next \u00b6 1. Setup","title":"Lab 5: Add Object Storage to a Persistent Database"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/#lab-5-add-object-storage-to-a-persistent-database","text":"","title":"Lab 5: Add Object Storage to a Persistent Database"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/#about-this-lab","text":"This hands-on lab for object storage on Kubernetes is called Add Object Storage to a Persistent Database and steps you through the setup and configuration of persistent storage for MongoDB using IBM Cloud Object Storage. This lab uses the IBM Cloud Object Storage plugin to mount an Object Storage bucket to a Kubernetes cluster using PersistentVolume by dynamic provisioning . A MongoDB database is setup that persists its data to the encrypted IBM Cloud Object Storage bucket using a PersistentVolumeClaim . IBM Cloud Object Storage plugin is a Kubernetes volume plugin that enables Kubernetes pods to access IBM Cloud Object Storage buckets. The plugin has two components: a dynamic provisioner and a FlexVolume driver for mounting the buckets using s3fs-fuse on a worker node. FlexVolume is a so-called out-of-tree volume plugin, as is the Container Storage Interface (CSI) . Out-of-tree volume plugins enable storage developers to create custom storage plugins. For more information about FlexVolume , go to flexvolume . s3fs allows Linux and macOS to mount an S3 bucket via FUSE. If you want to learn more about s3fs-fuse and FUSE you can do the optional s3fs lab . The lab consists of the following steps: Setup client CLI and Kubernetes cluster, Go to Setup , Create an Object Storage instance, go to Object Storage , Configure the Kubernetes cluster, go to Configure your Kubernetes Cluster , Deploy and configure the IBM Cloud Object Storage plugin , go to Cloud Object Storage plugin , Create a PersistentVolumeClaim with dynamic provisioning using the ibmc plugin, go to Create the PersistentVolumeClaim . Install MongoDB with Object Storage, go to MongoDB . Start with Setup .","title":"About this Lab"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/#other-labs","text":"Related labs using Object Storage are: [Optional] (TBD) Deploy Guestbook with MongoDB and Object Storage. Share Documents using Cloud Object Storage . Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS","title":"Other Labs"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/#next","text":"1. Setup","title":"Next"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/ABOUT-COS/","text":"About Object Storage \u00b6 In object storage or Object-based Storage Devices (OSD) , data is organized into flexible-sized objects that abstract the physical blocks of data, in contrast to block-oriented interfaces that read and write fixed sized blocks of data, like file storage or block storage . Objects include data, a globally unique identifier and metadata for indexing and management. Object storage also provides programmatic interfaces (mostly RESTful APIs) to manipulate data for CRUD, versioning, replication, life-cycle management and data transfer. Applications don't need to go through an operating system's storage drivers to manipulate data, they simply send get , put , or delete requests to the storage system. Object storage has the following benefits: durable, built-in data integrity (e.g. in case of disk failure), available, highly available via REST APIs at the manager layer, scalable, in order of terabytes (TBs), petabytes (PBs), and greater, unavailable in file or block storage, flexible, access from anywhere via REST APIs, secure, encrypt at-rest and in-transit. Usage \u00b6 Object storage is often used for handling large amounts of unstructured data, including email, video, photos, web pages, audio, sensor data and other types of media and web content, both textual and non-textual. Use cases are: Disaster recovery (DR) and backup (BC), AI and analytics, as a data lake in combination with Spark and Tensorflow, cloud native, startups combining cost-effectiveness of cloud native with flexibility of object storage, data archive, e.g. media files. Standards \u00b6 The International Committee for Information Technology Standards (INCITS) is an American standards organization for computer and communications standards. Its T10 committee is devoted to Small Computer Systems Interface (SCSI) technology and this T10 committee has published 2 standards for Object-Based Storage Devices (OSD): Object-Based Storage Device Commands (OSD), INCITS 400-2004 (R2013), InterNational Committee for Information Technology Standards. Retrieved 8 November 2013. Object-Based Storage Devices - 2 (OSD-2), INCITS 458-2011 (R2016), InterNational Committee for Information Technology Standards. 15 March 2011. Retrieved 8 November 2013. About IBM Cloud Object Storage \u00b6 The IBM Cloud Object Storage (COS) offers a few features that help secure your data. IBM Cloud Object Storage (COS) actively participates in several industry compliance programs and provides the following compliance, certifications, attestations, or reports as measure of proof: ISO 27001, PCI-DSS for Payment Card Industry (PCI) USA, HIPAA for Healthcare USA, (including administrative, physical, and technical safeguards required of Business Associates in 45 CFR Part 160 and Subparts A and C of Part 164), ISO 22301 Business Continuity Management, ISO 27017, ISO 27018, ISO 31000 Risk Management Principles, ISO 9001 Quality Management System, SOC1 Type 2 (SSAE 16), (System and Organization Controls 1), SOC2 Type 2 (SSAE 16), (System and Organization Controls 2), CSA STAR Level 1 (Self-Assessment), General Data Protection Regulation (GDPR) ready, Privacy shield certified. At a high level, information on IBM Cloud Object Storage (COS) is encrypted, then dispersed across multiple geographic locations, and accessed over popular protocols like HTTP with a RESTful API. SecureSlice distributes the data in slices across geo locations so that no full copy of data exists on any individual storage node, and automatically encrypts each segment of data before it is erasure coded and dispersed. The content can only be re-assembled through IBM Cloud\u2019s Accesser technology at the client\u2019s primary data center, where the data was originally received, and decrypted again by SecureSlice . Data-in-place or data-at-rest security is ensured when you persist database contents in IBM Cloud Object Storage. You also have a choice to use integration capabilities with IBM Cloud Key Management Services like IBM Key Protect (using FIPS 140-2 Level 3 certified hardware security modules (HSMs)) and Hyper Protect Crypto Services (built on FIPS 140-2 Level 4-certified hardware) for enhanced security features and compliance. Next \u00b6 3. Create Object Storage Instance .","title":"About Object Storage"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/ABOUT-COS/#about-object-storage","text":"In object storage or Object-based Storage Devices (OSD) , data is organized into flexible-sized objects that abstract the physical blocks of data, in contrast to block-oriented interfaces that read and write fixed sized blocks of data, like file storage or block storage . Objects include data, a globally unique identifier and metadata for indexing and management. Object storage also provides programmatic interfaces (mostly RESTful APIs) to manipulate data for CRUD, versioning, replication, life-cycle management and data transfer. Applications don't need to go through an operating system's storage drivers to manipulate data, they simply send get , put , or delete requests to the storage system. Object storage has the following benefits: durable, built-in data integrity (e.g. in case of disk failure), available, highly available via REST APIs at the manager layer, scalable, in order of terabytes (TBs), petabytes (PBs), and greater, unavailable in file or block storage, flexible, access from anywhere via REST APIs, secure, encrypt at-rest and in-transit.","title":"About Object Storage"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/ABOUT-COS/#usage","text":"Object storage is often used for handling large amounts of unstructured data, including email, video, photos, web pages, audio, sensor data and other types of media and web content, both textual and non-textual. Use cases are: Disaster recovery (DR) and backup (BC), AI and analytics, as a data lake in combination with Spark and Tensorflow, cloud native, startups combining cost-effectiveness of cloud native with flexibility of object storage, data archive, e.g. media files.","title":"Usage"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/ABOUT-COS/#standards","text":"The International Committee for Information Technology Standards (INCITS) is an American standards organization for computer and communications standards. Its T10 committee is devoted to Small Computer Systems Interface (SCSI) technology and this T10 committee has published 2 standards for Object-Based Storage Devices (OSD): Object-Based Storage Device Commands (OSD), INCITS 400-2004 (R2013), InterNational Committee for Information Technology Standards. Retrieved 8 November 2013. Object-Based Storage Devices - 2 (OSD-2), INCITS 458-2011 (R2016), InterNational Committee for Information Technology Standards. 15 March 2011. Retrieved 8 November 2013.","title":"Standards"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/ABOUT-COS/#about-ibm-cloud-object-storage","text":"The IBM Cloud Object Storage (COS) offers a few features that help secure your data. IBM Cloud Object Storage (COS) actively participates in several industry compliance programs and provides the following compliance, certifications, attestations, or reports as measure of proof: ISO 27001, PCI-DSS for Payment Card Industry (PCI) USA, HIPAA for Healthcare USA, (including administrative, physical, and technical safeguards required of Business Associates in 45 CFR Part 160 and Subparts A and C of Part 164), ISO 22301 Business Continuity Management, ISO 27017, ISO 27018, ISO 31000 Risk Management Principles, ISO 9001 Quality Management System, SOC1 Type 2 (SSAE 16), (System and Organization Controls 1), SOC2 Type 2 (SSAE 16), (System and Organization Controls 2), CSA STAR Level 1 (Self-Assessment), General Data Protection Regulation (GDPR) ready, Privacy shield certified. At a high level, information on IBM Cloud Object Storage (COS) is encrypted, then dispersed across multiple geographic locations, and accessed over popular protocols like HTTP with a RESTful API. SecureSlice distributes the data in slices across geo locations so that no full copy of data exists on any individual storage node, and automatically encrypts each segment of data before it is erasure coded and dispersed. The content can only be re-assembled through IBM Cloud\u2019s Accesser technology at the client\u2019s primary data center, where the data was originally received, and decrypted again by SecureSlice . Data-in-place or data-at-rest security is ensured when you persist database contents in IBM Cloud Object Storage. You also have a choice to use integration capabilities with IBM Cloud Key Management Services like IBM Key Protect (using FIPS 140-2 Level 3 certified hardware security modules (HSMs)) and Hyper Protect Crypto Services (built on FIPS 140-2 Level 4-certified hardware) for enhanced security features and compliance.","title":"About IBM Cloud Object Storage"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/ABOUT-COS/#next","text":"3. Create Object Storage Instance .","title":"Next"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/CLUSTER/","text":"4. Configure your Kubernetes Cluster \u00b6 You now have an Object Storage instance with a bucket, and have found the corresponding private endpoint for your Object Storage. Next, we can configure a Kubernetes cluster: Create a New Namespace in your Cluster, Create a Secret to Access the Object Storage, Create a New Namespace in your Cluster \u00b6 Previously, you logged in to your personal account to create a free instance of IBM Cloud Object Storage (COS). If the cluster exists in a different account, make sure to to switch accounts and log in to the IBM Cloud where your cluster exists. ibmcloud login -u $IBM_ID Note: if you use a single-sign-on provider, use the -sso flag. If you needed to switch accounts, you will have logged in again, and when prompted to Select an account , this time, choose the account with your cluster. In the example below, I have to choose account number 2 from the list, 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 , ibmcloud login -u b.newell2@remkoh.dev API endpoint: https://cloud.ibm.com Region: us-south Password> Authenticating... OK Select an account: 1. B Newell's Account (31296e3a285) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **2** Targeted account IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Retrieve your cluster information. ibmcloud ks clusters outputs, $ ibmcloud ks clusters Name ID State Created Workers Location Version Resource Group Name Provider <yourcluster> br78vuhd069a00er8s9g normal 1 day ago 1 Dallas 1.16.10_1533 default classic Retrieve the name of your cluster, in this example, I set the name of the first cluster with index 0 , CLUSTER_NAME=$(ibmcloud ks clusters --output json | jq -r '.[0].name') echo $CLUSTER_NAME In your browser: get the login command for your cluster: Go to the IBM Cloud resources page at https://cloud.ibm.com/resources , Under Clusters find and select your cluster, and load the cluster overview page. There are two ways to retrieve the login command with token: Click the Actions drop down next to the OpenShift web console button, and select Connect via CLI , in the pop-up window, click the oauth token request page link, or Click OpenShift web console button, in the OpenShift web console , click your profile name, such as IAM#name@email.com , and then click Copy Login Command . In the new page that opens for both options, click Display Token , Copy the oc login command, and paste the command into your terminal. $ oc login --token = HjXc6nNGyCB1imhqtc9csTmGQ5obrPcoe4SRJqTnnT8 --server = https://c100-e.us-south.containers.cloud.ibm.com:30712 Logged into \"https://c100-e.us-south.containers.cloud.ibm.com:30712\" as \"IAM#b.newell2@remkoh.dev\" using the token provided. You have one project on this server: \"<your-project>\" Using project \"<your-project>\". Welcome! See 'oc help' to get started. Create a new project cos-with-s3fs , oc new-project $NAMESPACE Make sure you're still logged in to your cluster and namespace, oc project Using project \"cos-with-s3fs\" Create a Secret to Access the Object Storage \u00b6 Create a Kubernetes Secret to store the COS service credentials named cos-write-access . oc create secret generic cos-write-access --type=ibm/ibmc-s3fs --from-literal=api-key=$COS_APIKEY --from-literal=service-instance-id=$COS_GUID outputs, $ oc create secret generic cos-write-access --type = ibm/ibmc-s3fs --from-literal = api-key = $COS_APIKEY --from-literal = service-instance-id = $COS_GUID secret/cos-write-access created Next \u00b6 5. Configure the Object Storage plugin","title":"4. Configure your Kubernetes Cluster"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/CLUSTER/#4-configure-your-kubernetes-cluster","text":"You now have an Object Storage instance with a bucket, and have found the corresponding private endpoint for your Object Storage. Next, we can configure a Kubernetes cluster: Create a New Namespace in your Cluster, Create a Secret to Access the Object Storage,","title":"4. Configure your Kubernetes Cluster"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/CLUSTER/#create-a-new-namespace-in-your-cluster","text":"Previously, you logged in to your personal account to create a free instance of IBM Cloud Object Storage (COS). If the cluster exists in a different account, make sure to to switch accounts and log in to the IBM Cloud where your cluster exists. ibmcloud login -u $IBM_ID Note: if you use a single-sign-on provider, use the -sso flag. If you needed to switch accounts, you will have logged in again, and when prompted to Select an account , this time, choose the account with your cluster. In the example below, I have to choose account number 2 from the list, 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 , ibmcloud login -u b.newell2@remkoh.dev API endpoint: https://cloud.ibm.com Region: us-south Password> Authenticating... OK Select an account: 1. B Newell's Account (31296e3a285) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **2** Targeted account IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Retrieve your cluster information. ibmcloud ks clusters outputs, $ ibmcloud ks clusters Name ID State Created Workers Location Version Resource Group Name Provider <yourcluster> br78vuhd069a00er8s9g normal 1 day ago 1 Dallas 1.16.10_1533 default classic Retrieve the name of your cluster, in this example, I set the name of the first cluster with index 0 , CLUSTER_NAME=$(ibmcloud ks clusters --output json | jq -r '.[0].name') echo $CLUSTER_NAME In your browser: get the login command for your cluster: Go to the IBM Cloud resources page at https://cloud.ibm.com/resources , Under Clusters find and select your cluster, and load the cluster overview page. There are two ways to retrieve the login command with token: Click the Actions drop down next to the OpenShift web console button, and select Connect via CLI , in the pop-up window, click the oauth token request page link, or Click OpenShift web console button, in the OpenShift web console , click your profile name, such as IAM#name@email.com , and then click Copy Login Command . In the new page that opens for both options, click Display Token , Copy the oc login command, and paste the command into your terminal. $ oc login --token = HjXc6nNGyCB1imhqtc9csTmGQ5obrPcoe4SRJqTnnT8 --server = https://c100-e.us-south.containers.cloud.ibm.com:30712 Logged into \"https://c100-e.us-south.containers.cloud.ibm.com:30712\" as \"IAM#b.newell2@remkoh.dev\" using the token provided. You have one project on this server: \"<your-project>\" Using project \"<your-project>\". Welcome! See 'oc help' to get started. Create a new project cos-with-s3fs , oc new-project $NAMESPACE Make sure you're still logged in to your cluster and namespace, oc project Using project \"cos-with-s3fs\"","title":"Create a New Namespace in your Cluster"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/CLUSTER/#create-a-secret-to-access-the-object-storage","text":"Create a Kubernetes Secret to store the COS service credentials named cos-write-access . oc create secret generic cos-write-access --type=ibm/ibmc-s3fs --from-literal=api-key=$COS_APIKEY --from-literal=service-instance-id=$COS_GUID outputs, $ oc create secret generic cos-write-access --type = ibm/ibmc-s3fs --from-literal = api-key = $COS_APIKEY --from-literal = service-instance-id = $COS_GUID secret/cos-write-access created","title":"Create a Secret to Access the Object Storage"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/CLUSTER/#next","text":"5. Configure the Object Storage plugin","title":"Next"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS-PLUGIN/","text":"5. Configure the Object Storage Plugin \u00b6 You are going to install the IBM Cloud Object Storage Plugin in your cluster, using the Helm CLI tool in this section. Add a Helm repository where IBM Cloud Object Storage Plugin chart resides. helm repo add ibm-charts https://icr.io/helm/ibm-charts outputs, $ helm repo add ibm-charts https://icr.io/helm/ibm-charts `ibm-charts` has been added to your repositories Refresh your local Helm repository. helm repo update outputs, $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ibm-charts\" chart repository Update Complete. \u2388 Happy Helming!\u2388 Download and unzip the IBM Cloud Object Storage plugin to your client, then install the plugin to your cluster from local client. helm pull --untar ibm-charts/ibm-object-storage-plugin ls -al helm plugin install ./ibm-object-storage-plugin/helm-ibmc should result in, $ helm plugin install ./ibm-object-storage-plugin/helm-ibmc Installed plugin: ibmc Housekeeping to allow execution of the ibmc.sh script by making the file executable. chmod 755 $HOME/.local/share/helm/plugins/helm-ibmc/ibmc.sh Verify the IBM Cloud Object Storage plugin installation. The plugin usage information should be displayed when running the command below. helm ibmc --help Before using the IBM Cloud Object Storage Plugin , configuration changes are required. In the Cloud Shell where you downloaded the IBM Cloud Object Storage plugin, navigate to the /project/cos-with-s3fs/ibm-object-storage-plugin/templates folder of the IBM Cloud Object Storage Plugin installation. ls -al ibm-object-storage-plugin/templates Make sure the provisioner-sa.yaml file is present and configure it to access the COS service using the COS service credentials secret cos-write-access that you created in the previous section. In the Theia editor, click File > Open , and browse to the /project/cos-with-s3fs/ibm-object-storage-plugin/templates directory and open the file provisioner-sa.yaml . Search for content ibmcloud-object-storage-secret-reader in the file around line 62. in a vi editor , - Type colon : - Type /ibmcloud-object-storage-secret-reader - Press <ENTER> key Scroll a few lines down to line #72 and find the section that is commented out #resourceNames: [\"\"] . rules: - apiGroups: [\"\"] resources: [\"secrets\"] # resourceNames: [ \"\" ] Uncomment the line and change the section to set the secret to cos-write-access and allow access to the COS instance, rules: - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"cos-write-access\"] Save the change and close the file. Install the configured storage classes for IBM Cloud Object Storage , which will use the edited template file. helm ibmc install ibm-object-storage-plugin ./ibm-object-storage-plugin outputs, $ helm ibmc install ibm-object-storage-plugin ./ibm-object-storage-plugin Helm version: v3.2.0+ge11b7ce Installing the Helm chart... PROVIDER: CLASSIC DC: hou02 Chart: ./ibm-object-storage-plugin NAME: ibm-object-storage-plugin LAST DEPLOYED: Sat May 23 17:45:25 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Thank you for installing: ibm-object-storage-plugin. Your release is named: ibm-object-storage-plugin .... <and a whole lot more instructions> Verify that the storage classes are created successfully. oc get storageclass | grep 'ibmc-s3fs' outputs, $ oc get storageclass | grep 'ibmc-s3fs' ibmc-s3fs-cold-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-cold-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-perf-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-perf-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-perf-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-perf-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-vault-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-vault-regional ibm.io/ibmc-s3fs 19s Review the storage class ibmc-s3fs-standard-regional . oc describe storageclass ibmc-s3fs-standard-regional outputs, $ oc describe storageclass ibmc-s3fs-standard-regional Name: ibmc-s3fs-standard-regional IsDefaultClass: No Annotations: meta.helm.sh/release-name=ibm-object-storage-plugin,meta.helm.sh/release-namespace=default Provisioner: ibm.io/ibmc-s3fs Parameters: ibm.io/chunk-size-mb=16,ibm.io/curl-debug=false,ibm.io/debug-level=warn,ibm.io/iam-endpoint=https://iam.bluemix.net,ibm.io/kernel-cache=true,ibm.io/multireq-max=20,ibm.io/object-store-endpoint=NA,ibm.io/object-store-storage-class=NA,ibm.io/parallel-count=2,ibm.io/s3fs-fuse-retry-count=5,ibm.io/stat-cache-size=100000,ibm.io/tls-cipher-suite=AESGCM AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Additional information is available at https://cloud.ibm.com/docs/containers?topic=containers-object_storage#configure_cos . Verify that plugin pods are in \"Running\" state and indicate READY state of 1/1 : oc get pods -n kube-system -o wide | grep object outputs, $ oc get pods -n kube-system -o wide | grep object ibmcloud-object-storage-driver-p4ljp 0/1 Running 0 32s 10.169.231.148 10.169.231.148 <none> <none> ibmcloud-object-storage-driver-zqb4h 0/1 Running 0 32s 10.169.231.153 10.169.231.153 <none> <none> ibmcloud-object-storage-plugin-fbb867887-msqcg 0/1 Running 0 32s 172.30.136.24 10.169.231.153 <none> <none> If the pods are not READY and indicate 0/1 then wait and re-run the command until the READY state says 1/1 . The installation is successful when one ibmcloud-object-storage-plugin pod and one or more ibmcloud-object-storage-driver pods are in running state. The number of ibmcloud-object-storage-driver pods equals the number of worker nodes in your cluster. All pods must be in a Running state for the plug-in to function properly. If the pods fail, run kubectl describe pod -n kube-system <pod_name> to find the root cause for the failure. Next \u00b6 6. Create the PersistentVolumeClaim","title":"5. Configure the Object Storage Plugin"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS-PLUGIN/#5-configure-the-object-storage-plugin","text":"You are going to install the IBM Cloud Object Storage Plugin in your cluster, using the Helm CLI tool in this section. Add a Helm repository where IBM Cloud Object Storage Plugin chart resides. helm repo add ibm-charts https://icr.io/helm/ibm-charts outputs, $ helm repo add ibm-charts https://icr.io/helm/ibm-charts `ibm-charts` has been added to your repositories Refresh your local Helm repository. helm repo update outputs, $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ibm-charts\" chart repository Update Complete. \u2388 Happy Helming!\u2388 Download and unzip the IBM Cloud Object Storage plugin to your client, then install the plugin to your cluster from local client. helm pull --untar ibm-charts/ibm-object-storage-plugin ls -al helm plugin install ./ibm-object-storage-plugin/helm-ibmc should result in, $ helm plugin install ./ibm-object-storage-plugin/helm-ibmc Installed plugin: ibmc Housekeeping to allow execution of the ibmc.sh script by making the file executable. chmod 755 $HOME/.local/share/helm/plugins/helm-ibmc/ibmc.sh Verify the IBM Cloud Object Storage plugin installation. The plugin usage information should be displayed when running the command below. helm ibmc --help Before using the IBM Cloud Object Storage Plugin , configuration changes are required. In the Cloud Shell where you downloaded the IBM Cloud Object Storage plugin, navigate to the /project/cos-with-s3fs/ibm-object-storage-plugin/templates folder of the IBM Cloud Object Storage Plugin installation. ls -al ibm-object-storage-plugin/templates Make sure the provisioner-sa.yaml file is present and configure it to access the COS service using the COS service credentials secret cos-write-access that you created in the previous section. In the Theia editor, click File > Open , and browse to the /project/cos-with-s3fs/ibm-object-storage-plugin/templates directory and open the file provisioner-sa.yaml . Search for content ibmcloud-object-storage-secret-reader in the file around line 62. in a vi editor , - Type colon : - Type /ibmcloud-object-storage-secret-reader - Press <ENTER> key Scroll a few lines down to line #72 and find the section that is commented out #resourceNames: [\"\"] . rules: - apiGroups: [\"\"] resources: [\"secrets\"] # resourceNames: [ \"\" ] Uncomment the line and change the section to set the secret to cos-write-access and allow access to the COS instance, rules: - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"cos-write-access\"] Save the change and close the file. Install the configured storage classes for IBM Cloud Object Storage , which will use the edited template file. helm ibmc install ibm-object-storage-plugin ./ibm-object-storage-plugin outputs, $ helm ibmc install ibm-object-storage-plugin ./ibm-object-storage-plugin Helm version: v3.2.0+ge11b7ce Installing the Helm chart... PROVIDER: CLASSIC DC: hou02 Chart: ./ibm-object-storage-plugin NAME: ibm-object-storage-plugin LAST DEPLOYED: Sat May 23 17:45:25 2020 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: Thank you for installing: ibm-object-storage-plugin. Your release is named: ibm-object-storage-plugin .... <and a whole lot more instructions> Verify that the storage classes are created successfully. oc get storageclass | grep 'ibmc-s3fs' outputs, $ oc get storageclass | grep 'ibmc-s3fs' ibmc-s3fs-cold-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-cold-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-perf-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-perf-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-flex-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-perf-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-perf-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-standard-regional ibm.io/ibmc-s3fs 19s ibmc-s3fs-vault-cross-region ibm.io/ibmc-s3fs 19s ibmc-s3fs-vault-regional ibm.io/ibmc-s3fs 19s Review the storage class ibmc-s3fs-standard-regional . oc describe storageclass ibmc-s3fs-standard-regional outputs, $ oc describe storageclass ibmc-s3fs-standard-regional Name: ibmc-s3fs-standard-regional IsDefaultClass: No Annotations: meta.helm.sh/release-name=ibm-object-storage-plugin,meta.helm.sh/release-namespace=default Provisioner: ibm.io/ibmc-s3fs Parameters: ibm.io/chunk-size-mb=16,ibm.io/curl-debug=false,ibm.io/debug-level=warn,ibm.io/iam-endpoint=https://iam.bluemix.net,ibm.io/kernel-cache=true,ibm.io/multireq-max=20,ibm.io/object-store-endpoint=NA,ibm.io/object-store-storage-class=NA,ibm.io/parallel-count=2,ibm.io/s3fs-fuse-retry-count=5,ibm.io/stat-cache-size=100000,ibm.io/tls-cipher-suite=AESGCM AllowVolumeExpansion: <unset> MountOptions: <none> ReclaimPolicy: Delete VolumeBindingMode: Immediate Events: <none> Additional information is available at https://cloud.ibm.com/docs/containers?topic=containers-object_storage#configure_cos . Verify that plugin pods are in \"Running\" state and indicate READY state of 1/1 : oc get pods -n kube-system -o wide | grep object outputs, $ oc get pods -n kube-system -o wide | grep object ibmcloud-object-storage-driver-p4ljp 0/1 Running 0 32s 10.169.231.148 10.169.231.148 <none> <none> ibmcloud-object-storage-driver-zqb4h 0/1 Running 0 32s 10.169.231.153 10.169.231.153 <none> <none> ibmcloud-object-storage-plugin-fbb867887-msqcg 0/1 Running 0 32s 172.30.136.24 10.169.231.153 <none> <none> If the pods are not READY and indicate 0/1 then wait and re-run the command until the READY state says 1/1 . The installation is successful when one ibmcloud-object-storage-plugin pod and one or more ibmcloud-object-storage-driver pods are in running state. The number of ibmcloud-object-storage-driver pods equals the number of worker nodes in your cluster. All pods must be in a Running state for the plug-in to function properly. If the pods fail, run kubectl describe pod -n kube-system <pod_name> to find the root cause for the failure.","title":"5. Configure the Object Storage Plugin"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS-PLUGIN/#next","text":"6. Create the PersistentVolumeClaim","title":"Next"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS/","text":"3. Create Object Storage Instance \u00b6 In this section, you will create an instance of IBM Cloud Object Storage (COS), create credentials and a bucket to store your persistent data for MongoDB. Steps: Preparation Create an Object Storage Instance Add Credentials Create a Bucket Get Private Endpoint Preparation \u00b6 Set the following environment variables: RESOURCEGROUP=Default COS_NAME_RANDOM=$(date | md5sum | head -c10) COS_NAME=$COS_NAME_RANDOM-cos-1 COS_CREDENTIALS=$COS_NAME-credentials COS_PLAN=Lite COS_BUCKET_NAME=$(date | md5sum | head -c10)-bucket-1 REGION=us-south COS_PRIVATE_ENDPOINT=s3.private.$REGION.cloud-object-storage.appdomain.cloud Create an instance of the IBM Cloud Object Storage service. For information about the IBM Cloud Object Storage service, go here . You can only have 1 single free Lite instance per account. Login to your personal account , ibmcloud login -u $IBM_ID Note: if you use a single-sign-on provider, use the -sso flag. You will be prompted to select an account. You must choose your own account under your own name. In the example below, account 1 is my own account under my own name, account 2 is where my Kubernetes cluster is located and that was provisioned to me, but on that second account I have no permission to create new resources. I have to select account 1 under my own name, e.g. `B Newell's Account', Select an account: 1. B Newell's Account (31296e3a285f) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **1** Targeted account B Newell's Account (31296e3a285f) You also need a resource group. Check if a resource-group exists, ibmcloud resource groups outputs, ibmcloud resource groups OK Name ID Default Group State Default 282d2f25256540499cf99b43b34025bf true ACTIVE If you have an existing resource group that is different than the default value Default , change the environment variable $RESOURCEGROUP . For example, if you have an existing resource group called default with lowercase d , change the environment variable, RESOURCEGROUP=default or use the following command to set the environment variable automatically, RESOURCEGROUP=$(ibmcloud resource groups --output json | jq -r '.[0].name') echo $RESOURCEGROUP If you do not have a resource group, create one, ibmcloud resource group-create $RESOURCEGROUP outputs, $ ibmcloud resource group-create $RESOURCEGROUP Creating resource group Default under account 5081ea1988f14a66a3ddf9d7fb3c6b29 as remko@remkoh.dev... OK Resource group Default was created. Resource Group ID: 93f7a4cd3c824c0cbe90d8f21b46f758 Set the environment variable $RESOURCEGROUP, RESOURCEGROUP=$(ibmcloud resource groups --output json | jq -r '.[0].name') echo $RESOURCEGROUP Create an IBM Cloud Object Storage Instance \u00b6 The Lite service plan for Cloud Object Storage includes Regional and Cross Regional resiliency, flexible data classes, and built in security. For the sample application, I will choose the standard and regional options in the ibmc-s3fs-standard-regional storageclass that is typical for web or mobile apps and we don't need cross-regional resilience beyond resilience per zones for our workshop app, but the options to choose for usage strategies and therefor the pricing of storageclasses for the bucket is very granular. Create a new Object Storage instance via CLI command, for the lab you can use a Lite plan. If you prefer a paid plan, choose Standard plan. ibmcloud resource service-instance-create $COS_NAME cloud-object-storage $COS_PLAN global -g $RESOURCEGROUP For example, outputs, $ ibmcloud resource service-instance-create cef84ff5ff-cos-1 cloud-object-storage Lite global -g Default OK Service instance cef84ff5ff-cos-1 was created. Name: cef84ff5ff-cos-1 ID: crn:v1:bluemix:public:cloud-object-storage:global:a/ e65910fa61ce9072d64902d03f3d4774:fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7:: GUID: fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7 Location: global State: active Type: service_instance Sub Type: Allow Cleanup: false Locked: false Created at: 2020-05-29T15:55:26Z Updated at: 2020-05-29T15:55:26Z Last Operation: Status create succeeded Message Completed create instance operation List the object storage instance you created, ibmcloud resource service-instance $COS_NAME Set the GUID of the object storage instance, COS_GUID=$(ibmcloud resource service-instance $COS_NAME --output json | jq -r '.[0].guid') echo $COS_GUID Add Credentials \u00b6 Now add credentials for authentication method IAM, ibmcloud resource service-key-create $COS_CREDENTIALS Writer --instance-name $COS_NAME --parameters '{\"HMAC\":true}' List the created credentials as json, COS_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.apikey') echo $COS_APIKEY Congratulations : you have now a free instance of IBM Cloud Object Storage. Next: create a bucket. Create a Bucket \u00b6 Data in IBM Cloud Object Storage is stored and organized in so-called buckets . To create a new bucket in your IBM Cloud Object Storage service instance, Retrieve the service instance id or Cloud Resource Name (CRN) from the credentials, and set the CRN on the Object Storage, COS_CRN=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.resource_instance_id') echo $COS_CRN Check the Object Storage configuration, ibmcloud cos config list Review the CRN property. $ ibmcloud cos config list Key Value Last Updated Default Region us-south Download Location /home/theia/Downloads CRN AccessKeyID SecretAccessKey Authentication Method IAM URL Style VHost Service Endpoint If the CRN is not set as in the example above, you can set it explicitly as follows, ibmcloud cos config crn --crn $COS_CRN Check the config again, to make sure the CRN is set now, ibmcloud cos config list Create a new bucket. ibmcloud cos bucket-create --bucket $COS_BUCKET_NAME --class Standard --ibm-service-instance-id $COS_CRN outputs, ibmcloud cos bucket-create --bucket $COS_BUCKET_NAME --class Standard --ibm-service-instance-id $COS_CRN OK Details about bucket 726ebedfcb-bucket-1: Region: us-south Class: Standard Verify the new bucket was created successfully. ibmcloud cos list-buckets --ibm-service-instance-id $COS_CRN Get Private Endpoint \u00b6 The IBM Cloud Object Storage plugin uses the private endpoint of the Object Storage instance to mount the bucket. The correct endpoint can be found using the region in which your Object Storage bucket is located. To list your bucket's location use, ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME outputs, $ ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME OK Details about bucket 726ebedfcb-bucket-1: Region: us-south Class: Standard or ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME --output json { \"LocationConstraint\": \"us-south-standard\" } Find the service default endpoint: ibmcloud cos config endpoint-url --list If the ServiceEndpointURL is empty as in the example below, you can find the service endpoint manually. $ ibmcloud cos config endpoint-url --list Key Value ServiceEndpointURL With your bucket's location, e.g. us-south , you can find your bucket's private endpoint here https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-endpoints#advanced-endpoint-types , OR in the following steps you find it in your Cloud Object Storage's bucket configuration. If your region is us-south the private endpoint is s3.private.us-south.cloud-object-storage.appdomain.cloud . Set an environment variable $REGION with the found region, and construct the service endpoint as follows. REGION=us-south COS_PRIVATE_ENDPOINT=s3.private.$REGION.cloud-object-storage.appdomain.cloud echo $COS_PRIVATE_ENDPOINT In a browser , you can verify the private endpoint for your region by navigating to https://cloud.ibm.com/resources . Expand the Storage section. Locate and select your IBM Cloud Object Storage service instance. In the left menu, select the buckets section Select your new bucket in the Buckets tab. Select the Configuration tab under Buckets iin the left pane. Take note of the Private endpoint. It should match your environment variable $COS_PRIVATE_ENDPOINT . Next \u00b6 4. Configure your Kubernetes Cluster","title":"3. Create Object Storage Instance"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS/#3-create-object-storage-instance","text":"In this section, you will create an instance of IBM Cloud Object Storage (COS), create credentials and a bucket to store your persistent data for MongoDB. Steps: Preparation Create an Object Storage Instance Add Credentials Create a Bucket Get Private Endpoint","title":"3. Create Object Storage Instance"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS/#preparation","text":"Set the following environment variables: RESOURCEGROUP=Default COS_NAME_RANDOM=$(date | md5sum | head -c10) COS_NAME=$COS_NAME_RANDOM-cos-1 COS_CREDENTIALS=$COS_NAME-credentials COS_PLAN=Lite COS_BUCKET_NAME=$(date | md5sum | head -c10)-bucket-1 REGION=us-south COS_PRIVATE_ENDPOINT=s3.private.$REGION.cloud-object-storage.appdomain.cloud Create an instance of the IBM Cloud Object Storage service. For information about the IBM Cloud Object Storage service, go here . You can only have 1 single free Lite instance per account. Login to your personal account , ibmcloud login -u $IBM_ID Note: if you use a single-sign-on provider, use the -sso flag. You will be prompted to select an account. You must choose your own account under your own name. In the example below, account 1 is my own account under my own name, account 2 is where my Kubernetes cluster is located and that was provisioned to me, but on that second account I have no permission to create new resources. I have to select account 1 under my own name, e.g. `B Newell's Account', Select an account: 1. B Newell's Account (31296e3a285f) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **1** Targeted account B Newell's Account (31296e3a285f) You also need a resource group. Check if a resource-group exists, ibmcloud resource groups outputs, ibmcloud resource groups OK Name ID Default Group State Default 282d2f25256540499cf99b43b34025bf true ACTIVE If you have an existing resource group that is different than the default value Default , change the environment variable $RESOURCEGROUP . For example, if you have an existing resource group called default with lowercase d , change the environment variable, RESOURCEGROUP=default or use the following command to set the environment variable automatically, RESOURCEGROUP=$(ibmcloud resource groups --output json | jq -r '.[0].name') echo $RESOURCEGROUP If you do not have a resource group, create one, ibmcloud resource group-create $RESOURCEGROUP outputs, $ ibmcloud resource group-create $RESOURCEGROUP Creating resource group Default under account 5081ea1988f14a66a3ddf9d7fb3c6b29 as remko@remkoh.dev... OK Resource group Default was created. Resource Group ID: 93f7a4cd3c824c0cbe90d8f21b46f758 Set the environment variable $RESOURCEGROUP, RESOURCEGROUP=$(ibmcloud resource groups --output json | jq -r '.[0].name') echo $RESOURCEGROUP","title":"Preparation"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS/#create-an-ibm-cloud-object-storage-instance","text":"The Lite service plan for Cloud Object Storage includes Regional and Cross Regional resiliency, flexible data classes, and built in security. For the sample application, I will choose the standard and regional options in the ibmc-s3fs-standard-regional storageclass that is typical for web or mobile apps and we don't need cross-regional resilience beyond resilience per zones for our workshop app, but the options to choose for usage strategies and therefor the pricing of storageclasses for the bucket is very granular. Create a new Object Storage instance via CLI command, for the lab you can use a Lite plan. If you prefer a paid plan, choose Standard plan. ibmcloud resource service-instance-create $COS_NAME cloud-object-storage $COS_PLAN global -g $RESOURCEGROUP For example, outputs, $ ibmcloud resource service-instance-create cef84ff5ff-cos-1 cloud-object-storage Lite global -g Default OK Service instance cef84ff5ff-cos-1 was created. Name: cef84ff5ff-cos-1 ID: crn:v1:bluemix:public:cloud-object-storage:global:a/ e65910fa61ce9072d64902d03f3d4774:fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7:: GUID: fef2d369-5f88-4dcc-bbf1-9afffcd9ccc7 Location: global State: active Type: service_instance Sub Type: Allow Cleanup: false Locked: false Created at: 2020-05-29T15:55:26Z Updated at: 2020-05-29T15:55:26Z Last Operation: Status create succeeded Message Completed create instance operation List the object storage instance you created, ibmcloud resource service-instance $COS_NAME Set the GUID of the object storage instance, COS_GUID=$(ibmcloud resource service-instance $COS_NAME --output json | jq -r '.[0].guid') echo $COS_GUID","title":"Create an IBM Cloud Object Storage Instance"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS/#add-credentials","text":"Now add credentials for authentication method IAM, ibmcloud resource service-key-create $COS_CREDENTIALS Writer --instance-name $COS_NAME --parameters '{\"HMAC\":true}' List the created credentials as json, COS_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.apikey') echo $COS_APIKEY Congratulations : you have now a free instance of IBM Cloud Object Storage. Next: create a bucket.","title":"Add Credentials"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS/#create-a-bucket","text":"Data in IBM Cloud Object Storage is stored and organized in so-called buckets . To create a new bucket in your IBM Cloud Object Storage service instance, Retrieve the service instance id or Cloud Resource Name (CRN) from the credentials, and set the CRN on the Object Storage, COS_CRN=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.resource_instance_id') echo $COS_CRN Check the Object Storage configuration, ibmcloud cos config list Review the CRN property. $ ibmcloud cos config list Key Value Last Updated Default Region us-south Download Location /home/theia/Downloads CRN AccessKeyID SecretAccessKey Authentication Method IAM URL Style VHost Service Endpoint If the CRN is not set as in the example above, you can set it explicitly as follows, ibmcloud cos config crn --crn $COS_CRN Check the config again, to make sure the CRN is set now, ibmcloud cos config list Create a new bucket. ibmcloud cos bucket-create --bucket $COS_BUCKET_NAME --class Standard --ibm-service-instance-id $COS_CRN outputs, ibmcloud cos bucket-create --bucket $COS_BUCKET_NAME --class Standard --ibm-service-instance-id $COS_CRN OK Details about bucket 726ebedfcb-bucket-1: Region: us-south Class: Standard Verify the new bucket was created successfully. ibmcloud cos list-buckets --ibm-service-instance-id $COS_CRN","title":"Create a Bucket"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS/#get-private-endpoint","text":"The IBM Cloud Object Storage plugin uses the private endpoint of the Object Storage instance to mount the bucket. The correct endpoint can be found using the region in which your Object Storage bucket is located. To list your bucket's location use, ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME outputs, $ ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME OK Details about bucket 726ebedfcb-bucket-1: Region: us-south Class: Standard or ibmcloud cos get-bucket-location --bucket $COS_BUCKET_NAME --output json { \"LocationConstraint\": \"us-south-standard\" } Find the service default endpoint: ibmcloud cos config endpoint-url --list If the ServiceEndpointURL is empty as in the example below, you can find the service endpoint manually. $ ibmcloud cos config endpoint-url --list Key Value ServiceEndpointURL With your bucket's location, e.g. us-south , you can find your bucket's private endpoint here https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-endpoints#advanced-endpoint-types , OR in the following steps you find it in your Cloud Object Storage's bucket configuration. If your region is us-south the private endpoint is s3.private.us-south.cloud-object-storage.appdomain.cloud . Set an environment variable $REGION with the found region, and construct the service endpoint as follows. REGION=us-south COS_PRIVATE_ENDPOINT=s3.private.$REGION.cloud-object-storage.appdomain.cloud echo $COS_PRIVATE_ENDPOINT In a browser , you can verify the private endpoint for your region by navigating to https://cloud.ibm.com/resources . Expand the Storage section. Locate and select your IBM Cloud Object Storage service instance. In the left menu, select the buckets section Select your new bucket in the Buckets tab. Select the Configuration tab under Buckets iin the left pane. Take note of the Private endpoint. It should match your environment variable $COS_PRIVATE_ENDPOINT .","title":"Get Private Endpoint"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/COS/#next","text":"4. Configure your Kubernetes Cluster","title":"Next"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/IBMC-S3FS/","text":"IBM Cloud Object Storage plugin \u00b6 IBM Cloud Object Storage plug-in is a Kubernetes Volume plug-in that enables Kubernetes pods to access IBM Cloud Object Storage buckets. The plug-in has two components: a dynamic provisioner (Object Storage Bucket Provisioner), and a FlexVolume driver (Kube FlexDriver) for mounting the buckets using s3fs-fuse on a worker node. You can read more about Filesystems for User Spaces (FUSE) and s3fs-fuse in the s3fs-fuse lab . See: Fundamentals of IBM Cloud Object Storage The IBM Cloud Object Storage driver depends on s3fs binaries and deploys them by launching a daemonset that runs one pod on each worker node that will then open a tunnel into the worker itself (which requires privileged access) to copy its binaries. A better approach in the near future will be using CSI drivers which will run completely containerized and thus not depend on advanced privileges. CSI is an independent standard that also applies to other cloud orchestrators (COs) like Docker and Mesos and it will be used through the same Kubernetes primitives mentioned above (PVs, PVCs and storage classes). S3, the Simple Storage Service, originated as Amazon. the central storage component for Netflix (which developed S3mper to provide a consistent secondary index on top of an eventually consistent storage) as well as Reddit, Pinterest, Tumblr and others. IBM\u2019s Cloud Object Storage is S3 compatible. Instead of always providing all parameters via the API, it is more convenient to mount the bucket as a folder onto the existing file system. This can be done via s3fs or goofys. Using s3fs \u00b6 Create a credentials file ~/.cos_creds with: <ACCESS_KEY>:<SECRET_ACCESS_KEY> Make sure neither your group nor others have access rights to this file, e.g. via chmod o-rwx ~/.cos_creds . You can then mount the bucket with, s3fs dlaas-ci-tf-training-data-us-standard ~/testmount -o passwd_file= ~/.cos_creds -o url=https://s3-api.us-geo.objectstorage.softlayer.net -o use_path_request_style Note that s3fs can optionally provide extensive logging information: s3fs dlaas-ci-tf-training-data-us-standard ~/testmount -o passwd_file= ~/.cos_creds -o dbglevel=info -f -o curldbg -o url=https://s3-api.us-geo.objectstorage.softlayer.net -o use_path_request_style & In simple test environments it might be sufficient to mount the folder as a host volume. you could achieve the same through a PVC. For Kubernetes clusters in production it is more desirable to properly mount volumes via drivers, using a Flex driver. s3fs allows Linux and macOS to mount an S3 bucket via FUSE. With the s3fs mountpoint, you can access your objects as if they were local files, i.e. list them with ls, copy them with cp, and access them seamlessly from any application built to work with local files. Unlike many file-to-object solutions, s3fs maintains a one-to-one mapping of files to objects. s3fs can yield good performance results when used with workloads reading or writing relatively large files (say, 20MB+) sequentially. On the other hand, you probably do not want to use s3fs with workloads accessing a database (as file locking is not supported), or workloads requiring random read or write access to files (because of the one-to-one file to object mapping). s3fs is not suitable for accessing data that is being mutated (other than by the s3fs instance itself). Next \u00b6 Lab 5: Add Object Storage to a Persistent Database","title":"IBM Cloud Object Storage plugin"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/IBMC-S3FS/#ibm-cloud-object-storage-plugin","text":"IBM Cloud Object Storage plug-in is a Kubernetes Volume plug-in that enables Kubernetes pods to access IBM Cloud Object Storage buckets. The plug-in has two components: a dynamic provisioner (Object Storage Bucket Provisioner), and a FlexVolume driver (Kube FlexDriver) for mounting the buckets using s3fs-fuse on a worker node. You can read more about Filesystems for User Spaces (FUSE) and s3fs-fuse in the s3fs-fuse lab . See: Fundamentals of IBM Cloud Object Storage The IBM Cloud Object Storage driver depends on s3fs binaries and deploys them by launching a daemonset that runs one pod on each worker node that will then open a tunnel into the worker itself (which requires privileged access) to copy its binaries. A better approach in the near future will be using CSI drivers which will run completely containerized and thus not depend on advanced privileges. CSI is an independent standard that also applies to other cloud orchestrators (COs) like Docker and Mesos and it will be used through the same Kubernetes primitives mentioned above (PVs, PVCs and storage classes). S3, the Simple Storage Service, originated as Amazon. the central storage component for Netflix (which developed S3mper to provide a consistent secondary index on top of an eventually consistent storage) as well as Reddit, Pinterest, Tumblr and others. IBM\u2019s Cloud Object Storage is S3 compatible. Instead of always providing all parameters via the API, it is more convenient to mount the bucket as a folder onto the existing file system. This can be done via s3fs or goofys.","title":"IBM Cloud Object Storage plugin"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/IBMC-S3FS/#using-s3fs","text":"Create a credentials file ~/.cos_creds with: <ACCESS_KEY>:<SECRET_ACCESS_KEY> Make sure neither your group nor others have access rights to this file, e.g. via chmod o-rwx ~/.cos_creds . You can then mount the bucket with, s3fs dlaas-ci-tf-training-data-us-standard ~/testmount -o passwd_file= ~/.cos_creds -o url=https://s3-api.us-geo.objectstorage.softlayer.net -o use_path_request_style Note that s3fs can optionally provide extensive logging information: s3fs dlaas-ci-tf-training-data-us-standard ~/testmount -o passwd_file= ~/.cos_creds -o dbglevel=info -f -o curldbg -o url=https://s3-api.us-geo.objectstorage.softlayer.net -o use_path_request_style & In simple test environments it might be sufficient to mount the folder as a host volume. you could achieve the same through a PVC. For Kubernetes clusters in production it is more desirable to properly mount volumes via drivers, using a Flex driver. s3fs allows Linux and macOS to mount an S3 bucket via FUSE. With the s3fs mountpoint, you can access your objects as if they were local files, i.e. list them with ls, copy them with cp, and access them seamlessly from any application built to work with local files. Unlike many file-to-object solutions, s3fs maintains a one-to-one mapping of files to objects. s3fs can yield good performance results when used with workloads reading or writing relatively large files (say, 20MB+) sequentially. On the other hand, you probably do not want to use s3fs with workloads accessing a database (as file locking is not supported), or workloads requiring random read or write access to files (because of the one-to-one file to object mapping). s3fs is not suitable for accessing data that is being mutated (other than by the s3fs instance itself).","title":"Using s3fs"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/IBMC-S3FS/#next","text":"Lab 5: Add Object Storage to a Persistent Database","title":"Next"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/MONGODB/","text":"7. Deploy MongoDB using Object Storage \u00b6 Deploy MongoDB to Cluster and Persist its Datastore in IBM Cloud Object Storage \u00b6 In this section, you are going to deploy an instance of MongoDB to your OpenShift cluster and store data on the IBM Cloud Object Storage. [Optional] If you want to configure the MongoDB via a values.yaml file, or want to review the default values of the Helm chart, in the Cloud Shell , you can download the default values.yaml file from the bitnami/mongodb Helm chart, which is used to configure and deploy the MongoDB Helm chart. In this lab we will overwrite the values from the commandline when we install the chart. wget https://raw.githubusercontent.com/bitnami/charts/master/bitnami/mongodb/values.yaml [Optional] To review the available configuration options, open the values.yaml file in a file editor and review the parameters that can be modified during mongdb deployment. In this exercise however, you'll overwrite the default values using Helm command parameters instead of a values.yaml file. Add the bitnami Helm repository. helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update outputs, $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ibm-charts\" chart repository ...Successfully got an update from the \"bitnami\" chart repository Update Complete. \u2388 Happy Helming!\u2388 The NAMESPACE environment variable should already be set, if not set it using the value from oc project , export NAMESPACE=<your project> echo $NAMESPACE Install MongoDB using helm with parameters, the flag persistence.enabled=true will enable storing your data to a PersistentVolume. oc get project $NAMESPACE -o yaml outputs $ oc get project $NAMESPACE -o yaml apiVersion: project.openshift.io/v1 kind: Project metadata: annotations: openshift.io/description: \"\" openshift.io/display-name: \"\" openshift.io/requester: IAM#remkohdev@us.ibm.com openshift.io/sa.scc.mcs: s0:c25,c15 openshift.io/sa.scc.supplemental-groups: 1000630000/10000 openshift.io/sa.scc.uid-range: 1000630000/10000 creationTimestamp: \"2020-11-23T18:52:49Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:metadata: f:annotations: f:openshift.io/sa.scc.mcs: {} f:openshift.io/sa.scc.supplemental-groups: {} f:openshift.io/sa.scc.uid-range: {} manager: cluster-policy-controller operation: Update time: \"2020-11-23T18:52:49Z\" ... or oc get project $NAMESPACE -o yaml | grep 'sa.scc.' Which defines that the sa.scc.supplemental-groups allowed are 1000630000/10000 , the sa.scc.uid-range for the project is 1000630000/10000 in format M/N, where M is the starting ID and N is the count. Using the fsGroup and user ids, create two environment variables, export SA_SCC_FSGROUP=<value of sa.scc.supplemental-groups> export SA_SCC_RUNASUSER=<value of sa.scc.uid-range> to deploy the bitnami Helm chart, helm install mongodb bitnami/mongodb --set persistence.enabled=true --set persistence.existingClaim=my-iks-pvc --set livenessProbe.initialDelaySeconds=180 --set auth.rootPassword=passw0rd --set auth.username=user1 --set auth.password=passw0rd --set auth.database=mydb --set service.type=ClusterIP --set podSecurityContext.enabled=true,podSecurityContext.fsGroup=$SA_SCC_FSGROUP,containerSecurityContext.enabled=true,containerSecurityContext.runAsUser=$SA_SCC_RUNASUSER outputs, $ helm install mongodb bitnami/mongodb --set persistence.enabled = true --set persistence.existingClaim = my-iks-pvc --set livenessProbe.initialDelaySeconds = 180 --set auth.rootPassword = passw0rd --set auth.username = user1 --set auth.password = passw0rd --set auth.database = mydb --set service.type = ClusterIP --set podSecurityContext.enabled = true,podSecurityContext.fsGroup = 1000630000 ,containerSecurityContext.enabled = true,containerSecurityContext.runAsUser = 1000630000 NAME: mongodb LAST DEPLOYED: Sat May 23 21:04:44 2020 NAMESPACE: <your-namespace> STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB can be accessed via port 27017 on the following DNS name from within your cluster: mongodb.<your-namespace>.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace <your-namespace> mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) To get the password for \"my-user\" run: export MONGODB_PASSWORD=$(kubectl get secret --namespace <your-namespace> mongodb -o jsonpath=\"{.data.mongodb-password}\" | base64 --decode) To connect to your database run the following command: kubectl run --namespace <your-namespace> mongodb-client --rm --tty -i --restart='Never' --image docker.io/bitnami/mongodb:4.2.7-debian-10-r0 --command -- mongo admin --host mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace <your-namespace> svc/mongodb 27017:27017 & mongo --host 127.0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Wait until the mongodb pods are running, oc get pod outputs, $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-c4b99b975-l2k7n 1/1 Running 0 81s Verify the MongoDB deployment. $ oc get deployment NAME READY UP-TO-DATE AVAILABLE AGE mongodb 1/1 1 1 6m30s Note: It may take several minutes until the deployment is completed and the container initialized, wait till the READY state is 1/1 . Verify that pods are running. $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-9f76c9485-sjtqx 1/1 Running 0 5m40s Note: It may take a few minutes until the deployment is completed and pod turns to Running state. Knowing the pod identifier now, you can verify the assigned fsGroup and uid of the SCC. oc get pod -o jsonpath='{range .items[*]}{@.metadata.name}{\" runAsUser: \"}{@.spec.containers[*].securityContext.runAsUser}{\" fsGroup: \"}{@.spec.securityContext.fsGroup}{\" seLinuxOptions: \"}{@.spec.securityContext.seLinuxOptions.level}{\"\\n\"}{end}' outputs, $ oc get pod -o jsonpath = '{range .items[*]}{@.metadata.name}{\" runAsUser: \"}{@.spec.containers[*].securityContext.runAsUser}{\" fsGroup: \"}{@.spec.securityContext.fsGroup}{\" seLinuxOptions: \"}{@.spec.securityContext.seLinuxOptions.level}{\"\\n\"}{end}' mongodb-c4b99b975-l2k7n runAsUser: 1000630000 fsGroup: 1000630000 seLinuxOptions: s0:c25,c15 Note, the service type for MongoDB is set to ClusterIP with the Helm parameter --set service.type=ClusterIP , so that MongoDB can only be accessed within the cluster. Retrieve and save MongoDB passwords in environment variables. export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace $NAMESPACE mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) export MONGODB_PASSWORD=$(kubectl get secret --namespace $NAMESPACE mongodb -o jsonpath=\"{.data.mongodb-password}\" | base64 --decode) echo $MONGODB_ROOT_PASSWORD echo $MONGODB_PASSWORD Verify that the internal MongoDB port 27017 within the container is not exposed externally, $ oc get svc mongodb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mongodb ClusterIP 172.21.131.154 <none> 27017/TCP 41s Verify MongoDB Deployment \u00b6 To verify MongoDB deployment, In the terminal, retrieve pod ID. oc get pod outputs, $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-c4b99b975-l2k7n 1/1 Running 0 5m44s Start an interactive terminal to the pod, you need to use your own unique pod name with the hashes. oc exec -it <your pod name> bash outputs, $ oc exec -it <your pod name> bash 1000630000@mongodb-9f76c9485-sjtqx:/$ Start a MongoDB CLI session. ```console $ mongo --host 127.0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD connecting to: mongodb://127.0.0.1:27017/?authSource=admin&compressors=disabled&gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"80b52ae7-b35a-4da9-827e-9daad510aadf\") } MongoDB server version: 4.4.2 > ``` Switch to your database. > use mydb switched to db mydb Authenticate a MongoDB connection. > db.auth(\"user1\", \"passw0rd\") 1 Create a collection . > db.createCollection(\"customer\") { \"ok\" : 1 } Verify the collection creation. > db.getCollection('customer') mydb.customer Create one data entry in MongoDB. > db.customer.insertOne( { firstName: \"John\", lastName: \"Smith\" } ) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"5ed1e4319bdb52022d624bdf\") } Retrieve the data entry in the MongoDB. > db.customer.find({ lastName: \"Smith\" }) { \"_id\" : ObjectId(\"5ed1e4319bdb52022d624bdf\"), \"firstName\" : \"John\", \"lastName\" : \"Smith\" } Type exit twice to back to the terminal. Your mongodb is now saving values, and if your Cloud Object Storage and bucket were configured correctly, your customer information is now securely stored. If you review the bucket in your Object Storage, MongoDB should now be writing its data files to the object storage. Conclusion \u00b6 You are awesome! You have now added IBM Cloud Object Storage persistent storage to your MongoDB database using dynamic provisioning the IBM Cloud Object Storage plugin based on s3fs-fuse . What remains is configuring your application to use the MongoDB service. You can use the instructions in the FUSE lab to mount a local filesystem to the remote Object Storage and inspect the documents.","title":"7. Deploy MongoDB using Object Storage"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/MONGODB/#7-deploy-mongodb-using-object-storage","text":"","title":"7. Deploy MongoDB using Object Storage"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/MONGODB/#deploy-mongodb-to-cluster-and-persist-its-datastore-in-ibm-cloud-object-storage","text":"In this section, you are going to deploy an instance of MongoDB to your OpenShift cluster and store data on the IBM Cloud Object Storage. [Optional] If you want to configure the MongoDB via a values.yaml file, or want to review the default values of the Helm chart, in the Cloud Shell , you can download the default values.yaml file from the bitnami/mongodb Helm chart, which is used to configure and deploy the MongoDB Helm chart. In this lab we will overwrite the values from the commandline when we install the chart. wget https://raw.githubusercontent.com/bitnami/charts/master/bitnami/mongodb/values.yaml [Optional] To review the available configuration options, open the values.yaml file in a file editor and review the parameters that can be modified during mongdb deployment. In this exercise however, you'll overwrite the default values using Helm command parameters instead of a values.yaml file. Add the bitnami Helm repository. helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update outputs, $ helm repo add bitnami https://charts.bitnami.com/bitnami \"bitnami\" has been added to your repositories $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"ibm-charts\" chart repository ...Successfully got an update from the \"bitnami\" chart repository Update Complete. \u2388 Happy Helming!\u2388 The NAMESPACE environment variable should already be set, if not set it using the value from oc project , export NAMESPACE=<your project> echo $NAMESPACE Install MongoDB using helm with parameters, the flag persistence.enabled=true will enable storing your data to a PersistentVolume. oc get project $NAMESPACE -o yaml outputs $ oc get project $NAMESPACE -o yaml apiVersion: project.openshift.io/v1 kind: Project metadata: annotations: openshift.io/description: \"\" openshift.io/display-name: \"\" openshift.io/requester: IAM#remkohdev@us.ibm.com openshift.io/sa.scc.mcs: s0:c25,c15 openshift.io/sa.scc.supplemental-groups: 1000630000/10000 openshift.io/sa.scc.uid-range: 1000630000/10000 creationTimestamp: \"2020-11-23T18:52:49Z\" managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:metadata: f:annotations: f:openshift.io/sa.scc.mcs: {} f:openshift.io/sa.scc.supplemental-groups: {} f:openshift.io/sa.scc.uid-range: {} manager: cluster-policy-controller operation: Update time: \"2020-11-23T18:52:49Z\" ... or oc get project $NAMESPACE -o yaml | grep 'sa.scc.' Which defines that the sa.scc.supplemental-groups allowed are 1000630000/10000 , the sa.scc.uid-range for the project is 1000630000/10000 in format M/N, where M is the starting ID and N is the count. Using the fsGroup and user ids, create two environment variables, export SA_SCC_FSGROUP=<value of sa.scc.supplemental-groups> export SA_SCC_RUNASUSER=<value of sa.scc.uid-range> to deploy the bitnami Helm chart, helm install mongodb bitnami/mongodb --set persistence.enabled=true --set persistence.existingClaim=my-iks-pvc --set livenessProbe.initialDelaySeconds=180 --set auth.rootPassword=passw0rd --set auth.username=user1 --set auth.password=passw0rd --set auth.database=mydb --set service.type=ClusterIP --set podSecurityContext.enabled=true,podSecurityContext.fsGroup=$SA_SCC_FSGROUP,containerSecurityContext.enabled=true,containerSecurityContext.runAsUser=$SA_SCC_RUNASUSER outputs, $ helm install mongodb bitnami/mongodb --set persistence.enabled = true --set persistence.existingClaim = my-iks-pvc --set livenessProbe.initialDelaySeconds = 180 --set auth.rootPassword = passw0rd --set auth.username = user1 --set auth.password = passw0rd --set auth.database = mydb --set service.type = ClusterIP --set podSecurityContext.enabled = true,podSecurityContext.fsGroup = 1000630000 ,containerSecurityContext.enabled = true,containerSecurityContext.runAsUser = 1000630000 NAME: mongodb LAST DEPLOYED: Sat May 23 21:04:44 2020 NAMESPACE: <your-namespace> STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** MongoDB can be accessed via port 27017 on the following DNS name from within your cluster: mongodb.<your-namespace>.svc.cluster.local To get the root password run: export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace <your-namespace> mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) To get the password for \"my-user\" run: export MONGODB_PASSWORD=$(kubectl get secret --namespace <your-namespace> mongodb -o jsonpath=\"{.data.mongodb-password}\" | base64 --decode) To connect to your database run the following command: kubectl run --namespace <your-namespace> mongodb-client --rm --tty -i --restart='Never' --image docker.io/bitnami/mongodb:4.2.7-debian-10-r0 --command -- mongo admin --host mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace <your-namespace> svc/mongodb 27017:27017 & mongo --host 127.0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD Wait until the mongodb pods are running, oc get pod outputs, $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-c4b99b975-l2k7n 1/1 Running 0 81s Verify the MongoDB deployment. $ oc get deployment NAME READY UP-TO-DATE AVAILABLE AGE mongodb 1/1 1 1 6m30s Note: It may take several minutes until the deployment is completed and the container initialized, wait till the READY state is 1/1 . Verify that pods are running. $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-9f76c9485-sjtqx 1/1 Running 0 5m40s Note: It may take a few minutes until the deployment is completed and pod turns to Running state. Knowing the pod identifier now, you can verify the assigned fsGroup and uid of the SCC. oc get pod -o jsonpath='{range .items[*]}{@.metadata.name}{\" runAsUser: \"}{@.spec.containers[*].securityContext.runAsUser}{\" fsGroup: \"}{@.spec.securityContext.fsGroup}{\" seLinuxOptions: \"}{@.spec.securityContext.seLinuxOptions.level}{\"\\n\"}{end}' outputs, $ oc get pod -o jsonpath = '{range .items[*]}{@.metadata.name}{\" runAsUser: \"}{@.spec.containers[*].securityContext.runAsUser}{\" fsGroup: \"}{@.spec.securityContext.fsGroup}{\" seLinuxOptions: \"}{@.spec.securityContext.seLinuxOptions.level}{\"\\n\"}{end}' mongodb-c4b99b975-l2k7n runAsUser: 1000630000 fsGroup: 1000630000 seLinuxOptions: s0:c25,c15 Note, the service type for MongoDB is set to ClusterIP with the Helm parameter --set service.type=ClusterIP , so that MongoDB can only be accessed within the cluster. Retrieve and save MongoDB passwords in environment variables. export MONGODB_ROOT_PASSWORD=$(kubectl get secret --namespace $NAMESPACE mongodb -o jsonpath=\"{.data.mongodb-root-password}\" | base64 --decode) export MONGODB_PASSWORD=$(kubectl get secret --namespace $NAMESPACE mongodb -o jsonpath=\"{.data.mongodb-password}\" | base64 --decode) echo $MONGODB_ROOT_PASSWORD echo $MONGODB_PASSWORD Verify that the internal MongoDB port 27017 within the container is not exposed externally, $ oc get svc mongodb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mongodb ClusterIP 172.21.131.154 <none> 27017/TCP 41s","title":"Deploy MongoDB to Cluster and Persist its Datastore in IBM Cloud Object Storage"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/MONGODB/#verify-mongodb-deployment","text":"To verify MongoDB deployment, In the terminal, retrieve pod ID. oc get pod outputs, $ oc get pod NAME READY STATUS RESTARTS AGE mongodb-c4b99b975-l2k7n 1/1 Running 0 5m44s Start an interactive terminal to the pod, you need to use your own unique pod name with the hashes. oc exec -it <your pod name> bash outputs, $ oc exec -it <your pod name> bash 1000630000@mongodb-9f76c9485-sjtqx:/$ Start a MongoDB CLI session. ```console $ mongo --host 127.0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD connecting to: mongodb://127.0.0.1:27017/?authSource=admin&compressors=disabled&gssapiServiceName=mongodb Implicit session: session { \"id\" : UUID(\"80b52ae7-b35a-4da9-827e-9daad510aadf\") } MongoDB server version: 4.4.2 > ``` Switch to your database. > use mydb switched to db mydb Authenticate a MongoDB connection. > db.auth(\"user1\", \"passw0rd\") 1 Create a collection . > db.createCollection(\"customer\") { \"ok\" : 1 } Verify the collection creation. > db.getCollection('customer') mydb.customer Create one data entry in MongoDB. > db.customer.insertOne( { firstName: \"John\", lastName: \"Smith\" } ) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"5ed1e4319bdb52022d624bdf\") } Retrieve the data entry in the MongoDB. > db.customer.find({ lastName: \"Smith\" }) { \"_id\" : ObjectId(\"5ed1e4319bdb52022d624bdf\"), \"firstName\" : \"John\", \"lastName\" : \"Smith\" } Type exit twice to back to the terminal. Your mongodb is now saving values, and if your Cloud Object Storage and bucket were configured correctly, your customer information is now securely stored. If you review the bucket in your Object Storage, MongoDB should now be writing its data files to the object storage.","title":"Verify MongoDB Deployment"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/MONGODB/#conclusion","text":"You are awesome! You have now added IBM Cloud Object Storage persistent storage to your MongoDB database using dynamic provisioning the IBM Cloud Object Storage plugin based on s3fs-fuse . What remains is configuring your application to use the MongoDB service. You can use the instructions in the FUSE lab to mount a local filesystem to the remote Object Storage and inspect the documents.","title":"Conclusion"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/PVC/","text":"6. Create the PersistentVolumeClaim \u00b6 Depending on the settings that you choose in your PVC, you can provision IBM Cloud Object Storage in the following ways: Dynamic provisioning : When you create the PVC, the matching persistent volume (PV) and the bucket in your IBM Cloud Object Storage service instance are automatically created. Static provisioning : You can reference an existing bucket in your IBM Cloud Object Storage service instance in your PVC. When you create the PVC, only the matching PV is automatically created and linked to your existing bucket in IBM Cloud Object Storage. In this exercise, you are going to use an existing bucket when assigning persistant storage to IKS container. In the cloud shell connected to your cluster, create a PersistentVolumeClaim configuration file. Note: Replace the values for: ibm.io/bucket , ibm.io/secret-name and ibm.io/endpoint . If your values are not exactly matching with the bucket name you created, the secret name you created and the private endpoint of your bucket, the PVC will remain in state pending and fail to create. Note: The secret-name should be set to cos-write-access unless you changed the name of the secret we created earlier, Note: ibm.io/endpoint should be set to the output of command echo \"https://$COS_PRIVATE_ENDPOINT\" Create the file first and then edit the file with vi if changes are needed, You need the bucket name and namespace to configure the PVC, echo \"https://$COS_PRIVATE_ENDPOINT\" echo $COS_BUCKET_NAME oc project Create the file, echo 'kind: PersistentVolumeClaim apiVersion: v1 metadata: name: my-iks-pvc namespace: <your-namespace> annotations: ibm.io/auto-create-bucket: \"false\" ibm.io/auto-delete-bucket: \"false\" ibm.io/bucket: \"<your-cos-bucket>\" ibm.io/secret-name: \"cos-write-access\" ibm.io/endpoint: \"https://s3.private.us-south.cloud-object-storage.appdomain.cloud\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ibmc-s3fs-standard-regional' > my-iks-pvc.yaml Note : indentation in YAML is important. If the PVC status remains Pending , the two usual suspects will be the secret with its credentials and the indentation in the YAML of the PVC. In Theia the integrated browser IDE, in the directory /project/cos-with-s3fs , open the file my-iks-pvc.yaml , and set the right values if changes are still needed, change the namespace value to the project name found with oc project , the ibm.io/bucket should be set to the value defined in echo $COS_BUCKET_NAME , ibm.io/secret-name should be set to \"cos-write-access\" , validate the ibm.io/endpoint to be set to the private service endpoint for your Object Storage bucket for the correct region, Create a PersistentVolumeClaim . oc apply -f my-iks-pvc.yaml outputs, $ oc apply -f my-iks-pvc.yaml persistentvolumeclaim/my-iks-pvc created Verify the PersistentVolumeClaim and through the PVC also the PersistentVolume or PV was created successfully and that the PVC has STATUS of Bound . oc get pvc should output a status of Bound , $ oc get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-iks-pvc Bound pvc-1a1f4bce-a8fe-4bd8-a160-f9268af2d18a 8Gi RWO ibmc-s3fs-standard-regional 4s Note: If the state of the PVC remains Pending , you can inspect the error for why the PVC remains pending by using the describe command: oc describe pvc <pvc_name> . For example, oc describe pvc my-iks-pvc . Note: If the state of the PVC stays as Pending , the problem must be resolved before you move to the next step. Verify a new PersistentVolume was also created successfully. oc get pv outputs, $ oc get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-70ac9454-27d8-43db-807f-d75474b0d61c 100Gi RWX Delete Bound openshift-image-registry/image-registry-storage ibmc-file-gold 36h pvc-86d739c4-86c1-4496-ab4e-c077d947acc0 8Gi RWO Delete Bound remkohdev-project1/my-iks-pvc ibmc-s3fs-standard-regional 4m26s You're now ready to persist data on the IBM Cloud Object Storage within your containers in your cluster. Next \u00b6 7. Deploy MongoDB using Object Storage","title":"6. Create the PersistentVolumeClaim"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/PVC/#6-create-the-persistentvolumeclaim","text":"Depending on the settings that you choose in your PVC, you can provision IBM Cloud Object Storage in the following ways: Dynamic provisioning : When you create the PVC, the matching persistent volume (PV) and the bucket in your IBM Cloud Object Storage service instance are automatically created. Static provisioning : You can reference an existing bucket in your IBM Cloud Object Storage service instance in your PVC. When you create the PVC, only the matching PV is automatically created and linked to your existing bucket in IBM Cloud Object Storage. In this exercise, you are going to use an existing bucket when assigning persistant storage to IKS container. In the cloud shell connected to your cluster, create a PersistentVolumeClaim configuration file. Note: Replace the values for: ibm.io/bucket , ibm.io/secret-name and ibm.io/endpoint . If your values are not exactly matching with the bucket name you created, the secret name you created and the private endpoint of your bucket, the PVC will remain in state pending and fail to create. Note: The secret-name should be set to cos-write-access unless you changed the name of the secret we created earlier, Note: ibm.io/endpoint should be set to the output of command echo \"https://$COS_PRIVATE_ENDPOINT\" Create the file first and then edit the file with vi if changes are needed, You need the bucket name and namespace to configure the PVC, echo \"https://$COS_PRIVATE_ENDPOINT\" echo $COS_BUCKET_NAME oc project Create the file, echo 'kind: PersistentVolumeClaim apiVersion: v1 metadata: name: my-iks-pvc namespace: <your-namespace> annotations: ibm.io/auto-create-bucket: \"false\" ibm.io/auto-delete-bucket: \"false\" ibm.io/bucket: \"<your-cos-bucket>\" ibm.io/secret-name: \"cos-write-access\" ibm.io/endpoint: \"https://s3.private.us-south.cloud-object-storage.appdomain.cloud\" spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ibmc-s3fs-standard-regional' > my-iks-pvc.yaml Note : indentation in YAML is important. If the PVC status remains Pending , the two usual suspects will be the secret with its credentials and the indentation in the YAML of the PVC. In Theia the integrated browser IDE, in the directory /project/cos-with-s3fs , open the file my-iks-pvc.yaml , and set the right values if changes are still needed, change the namespace value to the project name found with oc project , the ibm.io/bucket should be set to the value defined in echo $COS_BUCKET_NAME , ibm.io/secret-name should be set to \"cos-write-access\" , validate the ibm.io/endpoint to be set to the private service endpoint for your Object Storage bucket for the correct region, Create a PersistentVolumeClaim . oc apply -f my-iks-pvc.yaml outputs, $ oc apply -f my-iks-pvc.yaml persistentvolumeclaim/my-iks-pvc created Verify the PersistentVolumeClaim and through the PVC also the PersistentVolume or PV was created successfully and that the PVC has STATUS of Bound . oc get pvc should output a status of Bound , $ oc get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-iks-pvc Bound pvc-1a1f4bce-a8fe-4bd8-a160-f9268af2d18a 8Gi RWO ibmc-s3fs-standard-regional 4s Note: If the state of the PVC remains Pending , you can inspect the error for why the PVC remains pending by using the describe command: oc describe pvc <pvc_name> . For example, oc describe pvc my-iks-pvc . Note: If the state of the PVC stays as Pending , the problem must be resolved before you move to the next step. Verify a new PersistentVolume was also created successfully. oc get pv outputs, $ oc get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-70ac9454-27d8-43db-807f-d75474b0d61c 100Gi RWX Delete Bound openshift-image-registry/image-registry-storage ibmc-file-gold 36h pvc-86d739c4-86c1-4496-ab4e-c077d947acc0 8Gi RWO Delete Bound remkohdev-project1/my-iks-pvc ibmc-s3fs-standard-regional 4m26s You're now ready to persist data on the IBM Cloud Object Storage within your containers in your cluster.","title":"6. Create the PersistentVolumeClaim"},{"location":"generatedContent/kubernetes-storage/Lab5/cos-with-s3fs/PVC/#next","text":"7. Deploy MongoDB using Object Storage","title":"Next"},{"location":"generatedContent/kubernetes-storage/Lab5/guestbook-to-mongo/","text":"","title":"Index"},{"location":"generatedContent/kubernetes-storage/Lab5/setup/","text":"1. Setup \u00b6 Execute the following steps: Sign up for IBM Cloud](#1-sign-up-for-ibm-cloud), go here Setup Client CLI using CognitiveLabs, go here , or using IBM Cloud Shell, go here , Connect to an OpenShift Cluster, go here Setup Environment Variables Setup Environment Variables \u00b6 Create an environment variable for your IBM ID, IBM_ID=<your ibm id> If completed, in your terminal , create a working directory named cos-with-s3fs to start the lab, NAMESPACE=cos-with-s3fs-lab mkdir $NAMESPACE cd $NAMESPACE export WORKDIR=$(pwd) echo $WORKDIR In the CognitiveLabs terminal this should output the following directory /home/project/cos-with-s3fs-lab . Next \u00b6 3. Create Object Storage Instance . Optionally you can first read more about what Object Storage is here .","title":"1. Setup"},{"location":"generatedContent/kubernetes-storage/Lab5/setup/#1-setup","text":"Execute the following steps: Sign up for IBM Cloud](#1-sign-up-for-ibm-cloud), go here Setup Client CLI using CognitiveLabs, go here , or using IBM Cloud Shell, go here , Connect to an OpenShift Cluster, go here Setup Environment Variables","title":"1. Setup"},{"location":"generatedContent/kubernetes-storage/Lab5/setup/#setup-environment-variables","text":"Create an environment variable for your IBM ID, IBM_ID=<your ibm id> If completed, in your terminal , create a working directory named cos-with-s3fs to start the lab, NAMESPACE=cos-with-s3fs-lab mkdir $NAMESPACE cd $NAMESPACE export WORKDIR=$(pwd) echo $WORKDIR In the CognitiveLabs terminal this should output the following directory /home/project/cos-with-s3fs-lab .","title":"Setup Environment Variables"},{"location":"generatedContent/kubernetes-storage/Lab5/setup/#next","text":"3. Create Object Storage Instance . Optionally you can first read more about what Object Storage is here .","title":"Next"},{"location":"generatedContent/kubernetes-storage/Lab5/share-docs-with-cos/","text":"Share Documents with Cloud Object Storage \u00b6 Login to your IBM Cloud account, ibmcloud login -u <username> If you are using Single Sign-On (SSO) use the -sso flag to log in. Create an IAM APIKEY for the Cloud Object Storage service, e.g. with service name remkohdev-cos1 . Download and save the iam_apikey by adding the --file flag, COS_NAME=<service-name> IAM_APIKEY_NAME=$COS_NAME-apikey1 ibmcloud iam api-key-create $IAM_APIKEY_NAME --file $IAM_APIKEY_NAME.txt Set the IAM apikey environment variable, IAM_APIKEY=$(cat $IAM_APIKEY_NAME.txt | jq -r '.apikey') echo $IAM_APIKEY To create an object storage instance with a Lite plan, you need a resource group . Check if you already have a resource-group ibmcloud resource groups outputs, ibmcloud resource groups OK Name ID Default Group State Default 282d2f25256540499cf99b43b34025bf true ACTIVE If you do not have a resource group yet, create one, ibmcloud resource group-create Default Create a new Object Storage instance with a Lite plan. If you prefer a paid plan, choose Standard plan. Set environment variables, COS_PLAN=Lite RESOURCEGROUP=Default Then create the Cloud Object instance, ibmcloud resource service-instance-create $COS_NAME cloud-object-storage $COS_PLAN global -g $RESOURCEGROUP Get the GUID for the Cloud Object Storage service, COS_GUID=$(ibmcloud resource service-instance $COS_NAME --output json | jq -r '.[0].guid') echo $COS_GUID Create new service credentials with Role: Reader to share read-only access, and another service credentials with Role: Writer to upload documents, COS_CREDENTIALS1=$COS_NAME-reader-credentials1 COS_CREDENTIALS2=$COS_NAME-writer-credentials2 ibmcloud resource service-key-create $COS_CREDENTIALS1 Reader --instance-name $COS_NAME ibmcloud resource service-key-create $COS_CREDENTIALS2 Writer --instance-name $COS_NAME Create environment variables for the apikeys, COS_READER_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS1 --output json | jq -r '.[0].credentials.apikey') echo $COS_READER_APIKEY COS_WRITER_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS2 --output json | jq -r '.[0].credentials.apikey') echo $COS_WRITER_APIKEY Create a new bucket with a Standard storage class, COS_BUCKET=$COS_NAME-bucket1 COS_STORAGECLASS=Standard ibmcloud cos create-bucket --bucket $COS_BUCKET --ibm-service-instance-id $COS_GUID --class $COS_STORAGECLASS Verify the new bucket was created successfully. ibmcloud cos list-buckets --ibm-service-instance-id $COS_GUID Retrieve the region of your object storage configuration, ibmcloud cos config region list Or list your bucket's `LocationRestraint' ibmcloud cos get-bucket-location --bucket $COS_BUCKET --json | jq -r '.LocationConstraint' Set the environment variable for region, e.g. us-south , COS_REGION=<region> Create a new document, COS_OBJECT_KEY=helloworld.txt echo \"Hello World! Today is $(date)\" > $COS_OBJECT_KEY Upload a document using the S3Manager, ibmcloud cos upload --bucket $COS_BUCKET --key $COS_OBJECT_KEY --file ./helloworld.txt --content-language en-US --content-type \"text/plain\" OK Successfully uploaded object 'helloworld.txt' to bucket 'e59a327194-cos-1-bucket1'. Get IAM Token using the IAM Apikey: curl --location --request POST \"https://iam.cloud.ibm.com/identity/token\" --header \"Accept: application/json\" --header \"Content-Type: application/x-www-form-urlencoded\" --header \"apikey: $COS_READER_APIKEY\" --data-urlencode \"apikey=$IAM_APIKEY\" --data-urlencode \"response_type=cloud_iam\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" Set the response ACCESS_TOKEN=<access_token> Or using the curl statement above, ACCESS_TOKEN=$(curl --location --request POST \"https://iam.cloud.ibm.com/identity/token\" --header \"Accept: application/json\" --header \"Content-Type: application/x-www-form-urlencoded\" --header \"apikey: $COS_READER_APIKEY\" --data-urlencode \"apikey=$IAM_APIKEY\" --data-urlencode \"response_type=cloud_iam\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" | jq -r '.access_token') echo $ACCESS_TOKEN get bucket: curl --location --request GET \"https://s3.$COS_REGION.cloud-object-storage.appdomain.cloud/$COS_BUCKET\" --header \"Authorization: Bearer $ACCESS_TOKEN\" --header \"Accept: application/json\" Get object key: ibmcloud cos list-objects --bucket $COS_BUCKET curl --location --request GET \"https://s3.$COS_REGION.cloud-object-storage.appdomain.cloud/$COS_BUCKET/$COS_OBJECT_KEY\" --header \"Authorization: Bearer $ACCESS_TOKEN\" Get document","title":"Share Documents with Cloud Object Storage"},{"location":"generatedContent/kubernetes-storage/Lab5/share-docs-with-cos/#share-documents-with-cloud-object-storage","text":"Login to your IBM Cloud account, ibmcloud login -u <username> If you are using Single Sign-On (SSO) use the -sso flag to log in. Create an IAM APIKEY for the Cloud Object Storage service, e.g. with service name remkohdev-cos1 . Download and save the iam_apikey by adding the --file flag, COS_NAME=<service-name> IAM_APIKEY_NAME=$COS_NAME-apikey1 ibmcloud iam api-key-create $IAM_APIKEY_NAME --file $IAM_APIKEY_NAME.txt Set the IAM apikey environment variable, IAM_APIKEY=$(cat $IAM_APIKEY_NAME.txt | jq -r '.apikey') echo $IAM_APIKEY To create an object storage instance with a Lite plan, you need a resource group . Check if you already have a resource-group ibmcloud resource groups outputs, ibmcloud resource groups OK Name ID Default Group State Default 282d2f25256540499cf99b43b34025bf true ACTIVE If you do not have a resource group yet, create one, ibmcloud resource group-create Default Create a new Object Storage instance with a Lite plan. If you prefer a paid plan, choose Standard plan. Set environment variables, COS_PLAN=Lite RESOURCEGROUP=Default Then create the Cloud Object instance, ibmcloud resource service-instance-create $COS_NAME cloud-object-storage $COS_PLAN global -g $RESOURCEGROUP Get the GUID for the Cloud Object Storage service, COS_GUID=$(ibmcloud resource service-instance $COS_NAME --output json | jq -r '.[0].guid') echo $COS_GUID Create new service credentials with Role: Reader to share read-only access, and another service credentials with Role: Writer to upload documents, COS_CREDENTIALS1=$COS_NAME-reader-credentials1 COS_CREDENTIALS2=$COS_NAME-writer-credentials2 ibmcloud resource service-key-create $COS_CREDENTIALS1 Reader --instance-name $COS_NAME ibmcloud resource service-key-create $COS_CREDENTIALS2 Writer --instance-name $COS_NAME Create environment variables for the apikeys, COS_READER_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS1 --output json | jq -r '.[0].credentials.apikey') echo $COS_READER_APIKEY COS_WRITER_APIKEY=$(ibmcloud resource service-key $COS_CREDENTIALS2 --output json | jq -r '.[0].credentials.apikey') echo $COS_WRITER_APIKEY Create a new bucket with a Standard storage class, COS_BUCKET=$COS_NAME-bucket1 COS_STORAGECLASS=Standard ibmcloud cos create-bucket --bucket $COS_BUCKET --ibm-service-instance-id $COS_GUID --class $COS_STORAGECLASS Verify the new bucket was created successfully. ibmcloud cos list-buckets --ibm-service-instance-id $COS_GUID Retrieve the region of your object storage configuration, ibmcloud cos config region list Or list your bucket's `LocationRestraint' ibmcloud cos get-bucket-location --bucket $COS_BUCKET --json | jq -r '.LocationConstraint' Set the environment variable for region, e.g. us-south , COS_REGION=<region> Create a new document, COS_OBJECT_KEY=helloworld.txt echo \"Hello World! Today is $(date)\" > $COS_OBJECT_KEY Upload a document using the S3Manager, ibmcloud cos upload --bucket $COS_BUCKET --key $COS_OBJECT_KEY --file ./helloworld.txt --content-language en-US --content-type \"text/plain\" OK Successfully uploaded object 'helloworld.txt' to bucket 'e59a327194-cos-1-bucket1'. Get IAM Token using the IAM Apikey: curl --location --request POST \"https://iam.cloud.ibm.com/identity/token\" --header \"Accept: application/json\" --header \"Content-Type: application/x-www-form-urlencoded\" --header \"apikey: $COS_READER_APIKEY\" --data-urlencode \"apikey=$IAM_APIKEY\" --data-urlencode \"response_type=cloud_iam\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" Set the response ACCESS_TOKEN=<access_token> Or using the curl statement above, ACCESS_TOKEN=$(curl --location --request POST \"https://iam.cloud.ibm.com/identity/token\" --header \"Accept: application/json\" --header \"Content-Type: application/x-www-form-urlencoded\" --header \"apikey: $COS_READER_APIKEY\" --data-urlencode \"apikey=$IAM_APIKEY\" --data-urlencode \"response_type=cloud_iam\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" | jq -r '.access_token') echo $ACCESS_TOKEN get bucket: curl --location --request GET \"https://s3.$COS_REGION.cloud-object-storage.appdomain.cloud/$COS_BUCKET\" --header \"Authorization: Bearer $ACCESS_TOKEN\" --header \"Accept: application/json\" Get object key: ibmcloud cos list-objects --bucket $COS_BUCKET curl --location --request GET \"https://s3.$COS_REGION.cloud-object-storage.appdomain.cloud/$COS_BUCKET/$COS_OBJECT_KEY\" --header \"Authorization: Bearer $ACCESS_TOKEN\" Get document","title":"Share Documents with Cloud Object Storage"},{"location":"generatedContent/kubernetes-storage/Lab6/","text":"Lab 6. Using Software Defined Storage (SDS) with Portworx \u00b6 Coming soon","title":"Lab 6. Using Software Defined Storage (SDS) with Portworx"},{"location":"generatedContent/kubernetes-storage/Lab6/#lab-6-using-software-defined-storage-sds-with-portworx","text":"Coming soon","title":"Lab 6. Using Software Defined Storage (SDS) with Portworx"},{"location":"generatedContent/kubernetes-storage/Lab7/","text":"Lab 7. Connecting to External Storage \u00b6 This lab configures our nodejs guestbook application to connect to an external database, outside of the kubernetes cluster where the guestbook app is deployed. We will be using a managed database service offered on IBM Cloud, but you can apply the concepts in this lab to connect to any external database service such as a legacy database you might have running on premise. With a managed database service, you can take advantage of the provided service's built features for scaling, security, etc. If you'd rather implement your own database service, check out the previous labs in this workshop. Prereqs \u00b6 Before you begin, follow the prereqs in Lab0 . Clone the repos cd $HOME git clone https://github.com/IBM/guestbook-nodejs.git guestbook-cloudant git clone --branch storage https://github.com/IBM/guestbook-nodejs-config.git cd $HOME/guestbook-nodejs-config/storage/lab1 Create a new Kubernetes namespace. This will help us avoid conflicts with previous labs. Switch to the new namespace so all subsequent commands will run within that namespace: kubectl create namespace cloudant kubectl config set-context --current --namespace=cloudant Please choose one of the two options for setting up the database service Approach 1 Approach 2 Approach 1: Create a database service using the IBM Cloud console \u00b6 Follow these steps to create a free lite CloudantDB on IBM Cloud using a free IBM Cloud account. Create an account if you haven't already. Navigate to the IBM Cloud Catalog . Make sure your personal account in selected in the dropdown in the upper right. Search for Cloudant in the search bar and click the Cloudant tile. (Click Log In in the upper righthand side if you are not logged in). Set the instance name to \"mycloudant\". Select \"IAM and legacy credentials\" to Authentication method . Ensure the \"Lite\" plan is selected. Then hit create . Create a credential for your CloudantDB service \u00b6 Locate your credentials in your CloudantDB service on IBM Cloud. From the IBM Cloud resource page, search for mycloudant to find your Cloudant service. From the Cloudant DB service, select Service Credentials on the left. Then click the blue New credential on the right. Select the default name and role (should be manager ) for the credentials, and click Create . Expand the credential and take note of the url parameter. We will be using this value to populate a Kubernetes secret in the next step. Save your credentials in a Kubernetes secret \u00b6 From a terminal where you are connected to your kubernetes cluster, run the following command to save the URL to your cloudant service in your cluster as a secret: kubectl create secret generic binding-cloudant --from-literal=url=[CLOUDANT_URL] Once completed, skip ahead to the next section Approach 2: Use the IBM Cloud Operator to provision a database instance on IBM Cloud \u00b6 The Operator Framework provides support for Kubernetes-native extensions to manage custom resource types through operators. Many operators are available through operatorhub.io , including the IBM Cloud operator. The IBM Cloud operator simplifies the creation of IBM Cloud services and resouces and binding credentials from these resources into a Kubernetes cluster. The instructions in this guide are adapted from the IBM Developer tutorial Simplify the lifecycle management process for your Kubernetes apps with operators . With the IBM Cloud Kubernetes Service clusters at version 1.16 and later, the Operator Framework is already installed. So all you will need to do is install the IBM Cloud Operator. New clusters created after March 1 st , 2020 should all be at this level (or later). Create an API Key for your Target Account \u00b6 We will configure the IBM Cloud Operator to manage resources on your personal IBM Cloud Account. You will be able to create and manage a Cloudant DB lite service that only you will have access to. Note: The account that your Cloudant service will be created on MAY be different than the account where your Kubernetes cluster is located, so please keep that in mind. If you are participating in a workshop with the IBM Developer Advocacy team, we do this to avoid creating multiple lite cloudantDB services on the shared account where all the k8s clusters are running (IBM Cloud accounts are limited to 1 lite instance per service) Login to your personal IBM Cloud account. Use --sso if using single-sign-on. Select your personal account when asked upon logging in. ibmcloud login $ ibmcloud login API endpoint: https://cloud.ibm.com Region: us-south Authenticating... OK Select an account: 1. John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) 2. Another Account (12345) Enter a number> 1 Targeted account John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) API endpoint: https://cloud.ibm.com Region: us-south User: John.Zaccone@ibm.com Account: John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) Resource group: No resource group targeted, use 'ibmcloud target -g RESOURCE_GROUP' CF API endpoint: Org: Create a service ID in IBM Cloud IAM. If possible, do not use spaces in the names for your IAM credentials. When you use the operator binding feature, any spaces are replaced with underscores. ibmcloud iam service-id-create serviceid-ico Assign the service ID access to the required permissions to work with the IBM Cloud services. You will need the Manager role to provision a Cloudant service. ibmcloud iam service-policy-create serviceid-ico --roles Manager,Administrator --resource-group-name default --region us-south We will also need to provide Account Management to allow us to create an cloudant service on our IBM Cloud account: ibmcloud iam service-policy-create serviceid-ico --account-management --roles Administrator Create an API key for the service ID. ibmcloud iam service-api-key-create apikey-ico serviceid-ico Set the API key of the service ID as your CLI environment variable. Now, when you run the installation script, the script uses the service ID's API key. export IBMCLOUD_API_KEY=<apikey-ico-value> Confirm that the API key environment variable is set in your CLI. echo $IBMCLOUD_API_KEY Installing the IBM Cloud operator \u00b6 If you don't already have kubectl configured to point to your cluster, follow the setup steps in Lab0 to configure. Target the default resource group that your service ID has privledges to. ibmcloud target -g default The operator marketplace catalog provides a URL for the resources to install for each operator. Install the IBM Cloud Operator with the following command: curl -sL https://raw.githubusercontent.com/IBM/cloud-operators/master/hack/configure-operator.sh | bash -s -- install Check that the pod for the IBM Cloud operator is running with: kubectl get pods --namespace ibmcloud-operator-system You should see after a minute or two that the pod for the operator is running: $ kubectl get pods --namespace ibmcloud-operator-system NAMESPACE NAME READY STATUS RESTARTS AGE ibmcloud-operator-system ibmcloud-operator-controller-manager-56c8548f89-stzdq 2/2 Running 0 14m Understanding Operators \u00b6 The Operator Pattern is an emerging approach to extend through automation the expertise of human operators into the cluster environment. Operators are intended to support applications and management of other resources in and related to kubernetes clusters starting at installation, but continuing to day 2 operations of monitoring, backup, fault recovery and, of course, updates. Operators are custom code that uses the Kubernetes API (as a client) to implement a controller for a Custom Resource . Unlike the controllers built into the Kubernetes control plane which run on the Kubernetes master node, operators run outside of the Kubernetes control plan as pods on the worker nodes in the cluster. You can verify that fact by the kubectl get pods command above, which lists the pods of the operator running on a worker node. In addition to the IBM Cloud Operator, there are many operators that can manage resources within your cluster available from the Operator Hub . The Operator Hub includes many useful operators including operators that implement database installation, monitoring tools, application development frameworks, application runtimes and more. Your cluster now has the IBM Cloud operator installed. This operator is able to configure two custom resources in the cluster, a Service and a Binding . The Service defines a specific IBM Cloud service instance type to create, and the Binding specifies a named binding of a service instance to a secret in the cluster. For more details about the IBM Cloud operator see the project repository . Creating an instance of Cloudant \u00b6 For an application running within a Kubernetes cluster to be able to access an IBM Cloud service, the service needs to be created and the credentials to access the service must be added to the cluster so that they can be read by deployed applications. The Kubernetes cluster running the application accessing the service instance can be anywhere, but in this case you'll be using your Kubernetes cluster on IBM Cloud. We will be using a Cloudant DB service on IBM Cloud for this lab because it is free, json document datastore that will be easy for us to swap from our previous MongoDB database connection. Create the service instance and bind to the cluster \u00b6 Change into the yaml directory. apply the cloudant-ibmcloud.yaml file. cd $HOME/guestbook-nodejs-config/storage/lab7 Apply the cloudant-ibmcloud.yaml file using kubectl. This file defines a Service and Binding resource: kubectl apply -f cloudant-ibmcloud.yaml This file defines a Service and Binding resource and if successful there will be confirmation for both: $ kubectl apply -f cloudant-ibmcloud.yaml service.ibmcloud.ibm.com/mycloudant created binding.ibmcloud.ibm.com/binding-cloudant created Check for the secret for the CloudantDB service instance added to the current namespace: kubectl get secret binding-cloudant You should see confirmation of the secret, but there may be a short delay as the credentials are obtained by the operator, so repeat this command until you no longer see an error like: Error from server (NotFound): secrets \"binding-cloudant\" not found $ kubectl get secret binding-cloudant NAME TYPE DATA AGE binding-cloudant Opaque 6 40s Debug \u00b6 If the credentials have not been created after a few moments, check the logs of the kubernetes object you created. kubectl describe service.ibmcloud.ibm.com/mycloudant Check the IBM Cloud console - verify the Cloudant DB service \u00b6 Go back to your IBM Cloud tab in the browser and click on the words IBM Cloud on the upper left of the top menu. Now your Dashboard view will show a Services item under the Resource summary . Click on the Services label, and search for mycloudant to find your newly created instance Click on the mycloudant label in the Services list. This will open up the control panel for the IBM CloudantDB service. Click on the Service Credentials label and expands the service credential listed to see your service API Key - make a note of it or just keep the credentials visible. Return to the Kubernetes Terminal tab in your web browser and enter this command to extract and decode the apikey from the secret created by the IBM Cloud Operator: kubectl get secret binding-cloudant -o = jsonpath = '{.data.apikey}' | base64 -d && echo Notice how the string displayed is exactly the same as the service API Key visible from the control panel for the service. Lifecycle management with the IBM Cloud operator \u00b6 Let's take a look at the custom resource definition (CRD) file that was used in this exercise ( cloudant-ibmcloud.yaml ). ```yaml apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: mycloudant spec: plan: lite serviceClass: cloudantnosqldb --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-cloudant spec: serviceName: mycloudant role: Manager ``` Note that the API version is different from what you may have seen in other resource files in this lab. Since Kubernetes objects are scoped by the API, there's no conflict with the re-use of the kind Service in this CRD. Recall that in the internal Kubernetes API, a resource of kind Service is used to expose network ports running on pods. Here, the Service object type is used to descibe an IBM Cloud platform service from the catalog. The operator uses the spec of the resource to select the desired IBM Cloud service type and offering plan. The role of the IBM Cloud operator is to manage instances of these services and also create a Binding to the service that is stored as a secret in the cluster. The operator will monitor the IBM Cloud account service instances. If something happens to the service instance, the operator will detect the change and take action. For example, if a the service instance is deleted, the operator will create a new service instance and update the credentials stored in the binding secret. Next Steps \u00b6 Regardless of whether you did approach 1 or approach 2, the end result is the same. You should now have a CloudantDB service created on IBM Cloud with credentials to that service saved inside your Kubernetes as a secret . The next steps walk you through 1) Create a new database on the CloudantDB Service 1) Modify the guestbook application to read from CloudantDB 1) Build and push a new guestbook docker image 1) Edit the Kubernetes deployment yaml files to pull the new version of the application AND the credentials saved in the secret 1) Check your changes by deploying to Kubernetes and testing the application Create a new database on the CloudantDB service \u00b6 From your newly created Cloudant service on the IBM Cloud console, click Manage then Launch Dashboard . Use your IBM Credentials to login if necessary. From the Cloudant Dashboard screen, click Create Database and give it name, such as \"mydatabase\". Select Non-partitioned , then click Create . Remember this name as we will be using it later when we deploy our application. Modify the guestbook application to read from CloudantDB \u00b6 You will have to make minor changes to the Guestbook nodejs application to read from your newly created CloudantDB service. Navigate to your guestbook application: cd guestbook-cloudant/src (Optional) Install the Loopback connector for CloudantDB. This has been done for you already. cd guestbook-cloudant/src npm install loopback-connector-cloudant --save The connector provides the boilerplate code to connect to different backends. All we have to do is provide some basic configuration. Define the Cloudant as a datasource by replacing src/server/datasources.json file with the following: { \"in-memory\" : { \"name\" : \"in-memory\" , \"localStorage\" : \"\" , \"file\" : \"\" , \"connector\" : \"memory\" }, \"cloudant\" : { \"name\" : \"cloudant\" , \"connector\" : \"cloudant\" , \"url\" : \"${CLOUDANT_URL}\" , \"database\" : \"${CLOUDANT_DB}\" } } The environment variables CLOUDANT_URL and CLOUDANT_DB will be loaded in our environment via our Kubernetes deployment . Modify src/server/model-config.json to reference the datasource you just created: ... \"entry\" : { \"dataSource\" : \"cloudant\" , \"public\" : true } ... Build and push a new docker image \u00b6 Build a docker images with the changes and push to DockerHub. In this lab, we are building and pushing locally. In real-life, we would use CI/CD process to build and push our docker image from source control. Build the docker image, docker build -t $DOCKERUSER/guestbook-nodejs:cloudant . docker login -u $DOCKERUSER docker push $DOCKERUSER/guestbook-nodejs:cloudant Your guestbook application is all set to talk to a Cloudant database. Next, we will configure our Kubernetes deployment to use the image you just pushed, and to load the missing environment variables: CLOUDANT_URL and CLOUDANT_DB from our binding-cloudant secret. Configure Kubernetes yamls \u00b6 We have a yaml file created for you, but you will need to enter the location of the Docker Image you built in the previous step. Navigate to the location of your yaml deployment files, and inspect. cd $HOME/guestbook-nodejs-config/storage/lab7 cat guestbook-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: guestbook-cloudant labels: app: guestbook spec: selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: containers: - name: guestbook image: [IMAGE_NAME] resources: requests: cpu: 100m memory: 100Mi ports: - name: http containerPort: 3000 env: - name: CLOUDANT_URL valueFrom: secretKeyRef: name: binding-cloudant key: url - name : CLOUDANT_DB value: \"[DB_NAME]\" Replace [IMAGE_NAME] in the file guestbook-deployment-cloudant.yaml with the name of the image you uploaded to Docker Hub. Replace [DB_NAME] with the name of the Cloudant Database your created in a previous step. Notice how we load the environment variable CLOUDANT_URL from the binding-cloudant secret. This yaml files now defines all the environment variables our guestbook application needs to connect to our Cloudant DB. Test your changes by deploying to Kubernetes \u00b6 Deploy to kubernetes using kubectl apply : kubectl apply -f guestbook-deployment.yaml kubectl apply -f guestbook-service.yaml Check your pods. If there are any errors, use kubectl describe pod [POD NAME] to debug, kubectl get pods Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME=`kubectl get nodes -ojsonpath='{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}'` SERVICEPORT=`kubectl get svc guestbook -o=jsonpath='{.spec.ports[0].nodePort}'` echo \"http://$HOSTNAME:$SERVICEPORT\" Navigate to the guestbook in a broswer, and add some entries: From the Cloudant Dashboard, selected myDatabase and you should see documents created for the entries you created. This Cloudant database service is external to the Kubernetes service and data persists outside of the lifecycle of the container/pod/kubernetes cluster. The Cloudant service is a scalable json document storage solution that can be distributed across regions. For more information, check out the Cloudant Product Page .","title":"Lab 7. Connecting to External Storage"},{"location":"generatedContent/kubernetes-storage/Lab7/#lab-7-connecting-to-external-storage","text":"This lab configures our nodejs guestbook application to connect to an external database, outside of the kubernetes cluster where the guestbook app is deployed. We will be using a managed database service offered on IBM Cloud, but you can apply the concepts in this lab to connect to any external database service such as a legacy database you might have running on premise. With a managed database service, you can take advantage of the provided service's built features for scaling, security, etc. If you'd rather implement your own database service, check out the previous labs in this workshop.","title":"Lab 7. Connecting to External Storage"},{"location":"generatedContent/kubernetes-storage/Lab7/#prereqs","text":"Before you begin, follow the prereqs in Lab0 . Clone the repos cd $HOME git clone https://github.com/IBM/guestbook-nodejs.git guestbook-cloudant git clone --branch storage https://github.com/IBM/guestbook-nodejs-config.git cd $HOME/guestbook-nodejs-config/storage/lab1 Create a new Kubernetes namespace. This will help us avoid conflicts with previous labs. Switch to the new namespace so all subsequent commands will run within that namespace: kubectl create namespace cloudant kubectl config set-context --current --namespace=cloudant Please choose one of the two options for setting up the database service Approach 1 Approach 2","title":"Prereqs"},{"location":"generatedContent/kubernetes-storage/Lab7/#approach-1-create-a-database-service-using-the-ibm-cloud-console","text":"Follow these steps to create a free lite CloudantDB on IBM Cloud using a free IBM Cloud account. Create an account if you haven't already. Navigate to the IBM Cloud Catalog . Make sure your personal account in selected in the dropdown in the upper right. Search for Cloudant in the search bar and click the Cloudant tile. (Click Log In in the upper righthand side if you are not logged in). Set the instance name to \"mycloudant\". Select \"IAM and legacy credentials\" to Authentication method . Ensure the \"Lite\" plan is selected. Then hit create .","title":"Approach 1: Create a database service using the IBM Cloud console"},{"location":"generatedContent/kubernetes-storage/Lab7/#create-a-credential-for-your-cloudantdb-service","text":"Locate your credentials in your CloudantDB service on IBM Cloud. From the IBM Cloud resource page, search for mycloudant to find your Cloudant service. From the Cloudant DB service, select Service Credentials on the left. Then click the blue New credential on the right. Select the default name and role (should be manager ) for the credentials, and click Create . Expand the credential and take note of the url parameter. We will be using this value to populate a Kubernetes secret in the next step.","title":"Create a credential for your CloudantDB service"},{"location":"generatedContent/kubernetes-storage/Lab7/#save-your-credentials-in-a-kubernetes-secret","text":"From a terminal where you are connected to your kubernetes cluster, run the following command to save the URL to your cloudant service in your cluster as a secret: kubectl create secret generic binding-cloudant --from-literal=url=[CLOUDANT_URL] Once completed, skip ahead to the next section","title":"Save your credentials in a Kubernetes secret"},{"location":"generatedContent/kubernetes-storage/Lab7/#approach-2-use-the-ibm-cloud-operator-to-provision-a-database-instance-on-ibm-cloud","text":"The Operator Framework provides support for Kubernetes-native extensions to manage custom resource types through operators. Many operators are available through operatorhub.io , including the IBM Cloud operator. The IBM Cloud operator simplifies the creation of IBM Cloud services and resouces and binding credentials from these resources into a Kubernetes cluster. The instructions in this guide are adapted from the IBM Developer tutorial Simplify the lifecycle management process for your Kubernetes apps with operators . With the IBM Cloud Kubernetes Service clusters at version 1.16 and later, the Operator Framework is already installed. So all you will need to do is install the IBM Cloud Operator. New clusters created after March 1 st , 2020 should all be at this level (or later).","title":"Approach 2: Use the IBM Cloud Operator to provision a database instance on IBM Cloud"},{"location":"generatedContent/kubernetes-storage/Lab7/#create-an-api-key-for-your-target-account","text":"We will configure the IBM Cloud Operator to manage resources on your personal IBM Cloud Account. You will be able to create and manage a Cloudant DB lite service that only you will have access to. Note: The account that your Cloudant service will be created on MAY be different than the account where your Kubernetes cluster is located, so please keep that in mind. If you are participating in a workshop with the IBM Developer Advocacy team, we do this to avoid creating multiple lite cloudantDB services on the shared account where all the k8s clusters are running (IBM Cloud accounts are limited to 1 lite instance per service) Login to your personal IBM Cloud account. Use --sso if using single-sign-on. Select your personal account when asked upon logging in. ibmcloud login $ ibmcloud login API endpoint: https://cloud.ibm.com Region: us-south Authenticating... OK Select an account: 1. John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) 2. Another Account (12345) Enter a number> 1 Targeted account John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) API endpoint: https://cloud.ibm.com Region: us-south User: John.Zaccone@ibm.com Account: John H. Zaccone's Account (a21524842fc807640e69bf89c00009fc) Resource group: No resource group targeted, use 'ibmcloud target -g RESOURCE_GROUP' CF API endpoint: Org: Create a service ID in IBM Cloud IAM. If possible, do not use spaces in the names for your IAM credentials. When you use the operator binding feature, any spaces are replaced with underscores. ibmcloud iam service-id-create serviceid-ico Assign the service ID access to the required permissions to work with the IBM Cloud services. You will need the Manager role to provision a Cloudant service. ibmcloud iam service-policy-create serviceid-ico --roles Manager,Administrator --resource-group-name default --region us-south We will also need to provide Account Management to allow us to create an cloudant service on our IBM Cloud account: ibmcloud iam service-policy-create serviceid-ico --account-management --roles Administrator Create an API key for the service ID. ibmcloud iam service-api-key-create apikey-ico serviceid-ico Set the API key of the service ID as your CLI environment variable. Now, when you run the installation script, the script uses the service ID's API key. export IBMCLOUD_API_KEY=<apikey-ico-value> Confirm that the API key environment variable is set in your CLI. echo $IBMCLOUD_API_KEY","title":"Create an API Key for your Target Account"},{"location":"generatedContent/kubernetes-storage/Lab7/#installing-the-ibm-cloud-operator","text":"If you don't already have kubectl configured to point to your cluster, follow the setup steps in Lab0 to configure. Target the default resource group that your service ID has privledges to. ibmcloud target -g default The operator marketplace catalog provides a URL for the resources to install for each operator. Install the IBM Cloud Operator with the following command: curl -sL https://raw.githubusercontent.com/IBM/cloud-operators/master/hack/configure-operator.sh | bash -s -- install Check that the pod for the IBM Cloud operator is running with: kubectl get pods --namespace ibmcloud-operator-system You should see after a minute or two that the pod for the operator is running: $ kubectl get pods --namespace ibmcloud-operator-system NAMESPACE NAME READY STATUS RESTARTS AGE ibmcloud-operator-system ibmcloud-operator-controller-manager-56c8548f89-stzdq 2/2 Running 0 14m","title":"Installing the IBM Cloud operator"},{"location":"generatedContent/kubernetes-storage/Lab7/#understanding-operators","text":"The Operator Pattern is an emerging approach to extend through automation the expertise of human operators into the cluster environment. Operators are intended to support applications and management of other resources in and related to kubernetes clusters starting at installation, but continuing to day 2 operations of monitoring, backup, fault recovery and, of course, updates. Operators are custom code that uses the Kubernetes API (as a client) to implement a controller for a Custom Resource . Unlike the controllers built into the Kubernetes control plane which run on the Kubernetes master node, operators run outside of the Kubernetes control plan as pods on the worker nodes in the cluster. You can verify that fact by the kubectl get pods command above, which lists the pods of the operator running on a worker node. In addition to the IBM Cloud Operator, there are many operators that can manage resources within your cluster available from the Operator Hub . The Operator Hub includes many useful operators including operators that implement database installation, monitoring tools, application development frameworks, application runtimes and more. Your cluster now has the IBM Cloud operator installed. This operator is able to configure two custom resources in the cluster, a Service and a Binding . The Service defines a specific IBM Cloud service instance type to create, and the Binding specifies a named binding of a service instance to a secret in the cluster. For more details about the IBM Cloud operator see the project repository .","title":"Understanding Operators"},{"location":"generatedContent/kubernetes-storage/Lab7/#creating-an-instance-of-cloudant","text":"For an application running within a Kubernetes cluster to be able to access an IBM Cloud service, the service needs to be created and the credentials to access the service must be added to the cluster so that they can be read by deployed applications. The Kubernetes cluster running the application accessing the service instance can be anywhere, but in this case you'll be using your Kubernetes cluster on IBM Cloud. We will be using a Cloudant DB service on IBM Cloud for this lab because it is free, json document datastore that will be easy for us to swap from our previous MongoDB database connection.","title":"Creating an instance of Cloudant"},{"location":"generatedContent/kubernetes-storage/Lab7/#create-the-service-instance-and-bind-to-the-cluster","text":"Change into the yaml directory. apply the cloudant-ibmcloud.yaml file. cd $HOME/guestbook-nodejs-config/storage/lab7 Apply the cloudant-ibmcloud.yaml file using kubectl. This file defines a Service and Binding resource: kubectl apply -f cloudant-ibmcloud.yaml This file defines a Service and Binding resource and if successful there will be confirmation for both: $ kubectl apply -f cloudant-ibmcloud.yaml service.ibmcloud.ibm.com/mycloudant created binding.ibmcloud.ibm.com/binding-cloudant created Check for the secret for the CloudantDB service instance added to the current namespace: kubectl get secret binding-cloudant You should see confirmation of the secret, but there may be a short delay as the credentials are obtained by the operator, so repeat this command until you no longer see an error like: Error from server (NotFound): secrets \"binding-cloudant\" not found $ kubectl get secret binding-cloudant NAME TYPE DATA AGE binding-cloudant Opaque 6 40s","title":"Create the service instance and bind to the cluster"},{"location":"generatedContent/kubernetes-storage/Lab7/#debug","text":"If the credentials have not been created after a few moments, check the logs of the kubernetes object you created. kubectl describe service.ibmcloud.ibm.com/mycloudant","title":"Debug"},{"location":"generatedContent/kubernetes-storage/Lab7/#check-the-ibm-cloud-console-verify-the-cloudant-db-service","text":"Go back to your IBM Cloud tab in the browser and click on the words IBM Cloud on the upper left of the top menu. Now your Dashboard view will show a Services item under the Resource summary . Click on the Services label, and search for mycloudant to find your newly created instance Click on the mycloudant label in the Services list. This will open up the control panel for the IBM CloudantDB service. Click on the Service Credentials label and expands the service credential listed to see your service API Key - make a note of it or just keep the credentials visible. Return to the Kubernetes Terminal tab in your web browser and enter this command to extract and decode the apikey from the secret created by the IBM Cloud Operator: kubectl get secret binding-cloudant -o = jsonpath = '{.data.apikey}' | base64 -d && echo Notice how the string displayed is exactly the same as the service API Key visible from the control panel for the service.","title":"Check the IBM Cloud console - verify the Cloudant DB service"},{"location":"generatedContent/kubernetes-storage/Lab7/#lifecycle-management-with-the-ibm-cloud-operator","text":"Let's take a look at the custom resource definition (CRD) file that was used in this exercise ( cloudant-ibmcloud.yaml ). ```yaml apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: mycloudant spec: plan: lite serviceClass: cloudantnosqldb --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-cloudant spec: serviceName: mycloudant role: Manager ``` Note that the API version is different from what you may have seen in other resource files in this lab. Since Kubernetes objects are scoped by the API, there's no conflict with the re-use of the kind Service in this CRD. Recall that in the internal Kubernetes API, a resource of kind Service is used to expose network ports running on pods. Here, the Service object type is used to descibe an IBM Cloud platform service from the catalog. The operator uses the spec of the resource to select the desired IBM Cloud service type and offering plan. The role of the IBM Cloud operator is to manage instances of these services and also create a Binding to the service that is stored as a secret in the cluster. The operator will monitor the IBM Cloud account service instances. If something happens to the service instance, the operator will detect the change and take action. For example, if a the service instance is deleted, the operator will create a new service instance and update the credentials stored in the binding secret.","title":"Lifecycle management with the IBM Cloud operator"},{"location":"generatedContent/kubernetes-storage/Lab7/#next-steps","text":"Regardless of whether you did approach 1 or approach 2, the end result is the same. You should now have a CloudantDB service created on IBM Cloud with credentials to that service saved inside your Kubernetes as a secret . The next steps walk you through 1) Create a new database on the CloudantDB Service 1) Modify the guestbook application to read from CloudantDB 1) Build and push a new guestbook docker image 1) Edit the Kubernetes deployment yaml files to pull the new version of the application AND the credentials saved in the secret 1) Check your changes by deploying to Kubernetes and testing the application","title":"Next Steps"},{"location":"generatedContent/kubernetes-storage/Lab7/#create-a-new-database-on-the-cloudantdb-service","text":"From your newly created Cloudant service on the IBM Cloud console, click Manage then Launch Dashboard . Use your IBM Credentials to login if necessary. From the Cloudant Dashboard screen, click Create Database and give it name, such as \"mydatabase\". Select Non-partitioned , then click Create . Remember this name as we will be using it later when we deploy our application.","title":"Create a new database on the CloudantDB service"},{"location":"generatedContent/kubernetes-storage/Lab7/#modify-the-guestbook-application-to-read-from-cloudantdb","text":"You will have to make minor changes to the Guestbook nodejs application to read from your newly created CloudantDB service. Navigate to your guestbook application: cd guestbook-cloudant/src (Optional) Install the Loopback connector for CloudantDB. This has been done for you already. cd guestbook-cloudant/src npm install loopback-connector-cloudant --save The connector provides the boilerplate code to connect to different backends. All we have to do is provide some basic configuration. Define the Cloudant as a datasource by replacing src/server/datasources.json file with the following: { \"in-memory\" : { \"name\" : \"in-memory\" , \"localStorage\" : \"\" , \"file\" : \"\" , \"connector\" : \"memory\" }, \"cloudant\" : { \"name\" : \"cloudant\" , \"connector\" : \"cloudant\" , \"url\" : \"${CLOUDANT_URL}\" , \"database\" : \"${CLOUDANT_DB}\" } } The environment variables CLOUDANT_URL and CLOUDANT_DB will be loaded in our environment via our Kubernetes deployment . Modify src/server/model-config.json to reference the datasource you just created: ... \"entry\" : { \"dataSource\" : \"cloudant\" , \"public\" : true } ...","title":"Modify the guestbook application to read from CloudantDB"},{"location":"generatedContent/kubernetes-storage/Lab7/#build-and-push-a-new-docker-image","text":"Build a docker images with the changes and push to DockerHub. In this lab, we are building and pushing locally. In real-life, we would use CI/CD process to build and push our docker image from source control. Build the docker image, docker build -t $DOCKERUSER/guestbook-nodejs:cloudant . docker login -u $DOCKERUSER docker push $DOCKERUSER/guestbook-nodejs:cloudant Your guestbook application is all set to talk to a Cloudant database. Next, we will configure our Kubernetes deployment to use the image you just pushed, and to load the missing environment variables: CLOUDANT_URL and CLOUDANT_DB from our binding-cloudant secret.","title":"Build and push a new docker image"},{"location":"generatedContent/kubernetes-storage/Lab7/#configure-kubernetes-yamls","text":"We have a yaml file created for you, but you will need to enter the location of the Docker Image you built in the previous step. Navigate to the location of your yaml deployment files, and inspect. cd $HOME/guestbook-nodejs-config/storage/lab7 cat guestbook-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: guestbook-cloudant labels: app: guestbook spec: selector: matchLabels: app: guestbook template: metadata: labels: app: guestbook spec: containers: - name: guestbook image: [IMAGE_NAME] resources: requests: cpu: 100m memory: 100Mi ports: - name: http containerPort: 3000 env: - name: CLOUDANT_URL valueFrom: secretKeyRef: name: binding-cloudant key: url - name : CLOUDANT_DB value: \"[DB_NAME]\" Replace [IMAGE_NAME] in the file guestbook-deployment-cloudant.yaml with the name of the image you uploaded to Docker Hub. Replace [DB_NAME] with the name of the Cloudant Database your created in a previous step. Notice how we load the environment variable CLOUDANT_URL from the binding-cloudant secret. This yaml files now defines all the environment variables our guestbook application needs to connect to our Cloudant DB.","title":"Configure Kubernetes yamls"},{"location":"generatedContent/kubernetes-storage/Lab7/#test-your-changes-by-deploying-to-kubernetes","text":"Deploy to kubernetes using kubectl apply : kubectl apply -f guestbook-deployment.yaml kubectl apply -f guestbook-service.yaml Check your pods. If there are any errors, use kubectl describe pod [POD NAME] to debug, kubectl get pods Find the URL for the guestbook application by joining the worker node external IP and service node port. HOSTNAME=`kubectl get nodes -ojsonpath='{.items[0].metadata.labels.ibm-cloud\\.kubernetes\\.io\\/external-ip}'` SERVICEPORT=`kubectl get svc guestbook -o=jsonpath='{.spec.ports[0].nodePort}'` echo \"http://$HOSTNAME:$SERVICEPORT\" Navigate to the guestbook in a broswer, and add some entries: From the Cloudant Dashboard, selected myDatabase and you should see documents created for the entries you created. This Cloudant database service is external to the Kubernetes service and data persists outside of the lifecycle of the container/pod/kubernetes cluster. The Cloudant service is a scalable json document storage solution that can be distributed across regions. For more information, check out the Cloudant Product Page .","title":"Test your changes by deploying to Kubernetes"},{"location":"generatedContent/kubernetes-storage/admin-guide/","text":"Admin Guide \u00b6 This section is comprised of the following steps: Instructor Step 1. Instructor Step \u00b6 Things specific to instructors can go here.","title":"Admin Guide"},{"location":"generatedContent/kubernetes-storage/admin-guide/#admin-guide","text":"This section is comprised of the following steps: Instructor Step","title":"Admin Guide"},{"location":"generatedContent/kubernetes-storage/admin-guide/#1-instructor-step","text":"Things specific to instructors can go here.","title":"1. Instructor Step"},{"location":"generatedContent/kubernetes-storage/flexvolume/","text":"FlexVolume \u00b6 The Kubernetes Storage Special Interest Group (SIG) defines three methods to implement a volume plugin: In-tree volume plugin [deprecated], Out-of-tree FlexVolume driver [deprecated], Out-of-tree CSI driver. Flexvolume is deprecated, but the Kubernetes Storage-SIG plans to continue to support and maintain the Flex Volume API. As of Kubernetes 1.9, there are two out-of-tree methods to implement volume plugins: Container Storage Interface (CSI) and FlexVolume . Out-of-tree volume plugins enable storage developers to create custom storage plugins. Before the introduction of the CSI and FlexVolume, all volume plugins were in-tree meaning they were built, linked, compiled, and shipped with the core Kubernetes binaries and extend the core Kubernetes API. FlexVolume has existed since Kubernetes 1.2, and is a GA feature since Kubernetes 1.8. FlexVolume uses an exec-based model to interface with drivers. The FlexVolume driver binaries must be installed in a pre-defined volume plugin path on each node and in some cases the control plane nodes as well. Pods interact with FlexVolume drivers through the flexvolume in-tree volume plugin. The plugin expects the following call-outs are implemented for the backend drivers. Call-outs are invoked from Kubelet and Controller Manager. Init, Attach, Detach, Wait for attach, Volume is attached, Mount device, Unmount device, Mount, Unmount. For more information about FlexVolume, see flex .","title":"FlexVolume"},{"location":"generatedContent/kubernetes-storage/flexvolume/#flexvolume","text":"The Kubernetes Storage Special Interest Group (SIG) defines three methods to implement a volume plugin: In-tree volume plugin [deprecated], Out-of-tree FlexVolume driver [deprecated], Out-of-tree CSI driver. Flexvolume is deprecated, but the Kubernetes Storage-SIG plans to continue to support and maintain the Flex Volume API. As of Kubernetes 1.9, there are two out-of-tree methods to implement volume plugins: Container Storage Interface (CSI) and FlexVolume . Out-of-tree volume plugins enable storage developers to create custom storage plugins. Before the introduction of the CSI and FlexVolume, all volume plugins were in-tree meaning they were built, linked, compiled, and shipped with the core Kubernetes binaries and extend the core Kubernetes API. FlexVolume has existed since Kubernetes 1.2, and is a GA feature since Kubernetes 1.8. FlexVolume uses an exec-based model to interface with drivers. The FlexVolume driver binaries must be installed in a pre-defined volume plugin path on each node and in some cases the control plane nodes as well. Pods interact with FlexVolume drivers through the flexvolume in-tree volume plugin. The plugin expects the following call-outs are implemented for the backend drivers. Call-outs are invoked from Kubelet and Controller Manager. Init, Attach, Detach, Wait for attach, Volume is attached, Mount device, Unmount device, Mount, Unmount. For more information about FlexVolume, see flex .","title":"FlexVolume"},{"location":"generatedContent/kubernetes-storage/fuse/","text":"Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS \u00b6 About FUSE \u00b6 Filesystem in Userspace (FUSE) lets non-privileged users create a file system in their user space. The FUSE project consists of two components: a FUSE kernel module that is part of the Linux kernel since version 2.6.14, and the libfuse userspace library. The libfuse library provides a reference implementation for communication with the FUSE kernel module, providing client functions to mount the file system, unmount it, read requests from the kernel, and send responses back. FUSE is particularly useful for writing Virtual File Systems (VFS). s3fs \u00b6 If you want to use Object Storage as the underlying storage for FUSE-based filesystem disk management, you can use s3fs or s3fs-fuse . s3fs is an Amazon S3 (Services Simple Storage) and S3-based object stores compatible utility for FUSE-based filesystem disk management that supports a subset of Single UNIX Specification (SUS) or POSIX including reading/writing files, directories, symlinks, mode, uid/gid, and extended attributes, while preserving the original file format, e.g. a plain text or MS Word document. Applications that need to read and write to an NFS-style filesystem can use s3fs, which integrates applications with S3 compatible storage like IBM Cloud Object Storage . s3fs also allows you to interact with your cloud storage using familiar shell commands, like ls for listing or cp to copy files. Performance is not equal to a true local filesystem, but you can use some advanced options to increase throughput. Object storage services have high-latency for time to first byte and lack random write access (requires rewriting the full object). Workloads that only read big files, like deep learning workloads, can achieve good throughput using s3fs. s3fs on macOS uses osxfuse . osxfuse is FUSE for macOS. Lab \u00b6 In this lab, you will mount a bucket using s3fs . You need admin access to install s3fs-fuse . In this lab, I used brew on macOS, for other Operating Systems, see the installation instructions . Pre-requirements: A free IBM Cloud account, A free IBM Cloud Object Storage (COS) instance with a bucket, Admin access to your client OS, Connect to IBM Cloud \u00b6 You have to be logged in to your IBM Cloud account, ibmcloud login -u <IBMId> If you are using Single Sign-On (SSO) use the -sso flag to log in. Select the account with your instance of Object Storage. In the example below, I have to select account 1 under my own name, e.g. `B Newell's Account', Select an account: 1. B Newell's Account (31296e3a285f) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **1** Targeted account B Newell's Account (31296e3a285f) Then, set an environment variable with the bucket name, and upload a document to your bucket in the IBM Cloud Object Storage instance. COS_NAME=<cos-instance-name> COS_BUCKET_NAME=<cos-bucket-name> Upload an Object to Object Storage \u00b6 Create a new document, COS_OBJECT_KEY=helloworld.txt echo \"Hello World! Today is $(date)\" > $COS_OBJECT_KEY Upload an object using S3Manager , ibmcloud cos upload --bucket $COS_BUCKET_NAME --key $COS_OBJECT_KEY --file ./helloworld.txt --content-language en-US --content-type \"text/plain\" OK Successfully uploaded object 'helloworld.txt' to bucket 'e59a327194-cos-1-bucket1'. Install s3fs \u00b6 Install s3fs , brew install s3fs If you already have credentials with HMAC keys created for your Object Storage instance, you can retrieve credentials using, ibmcloud resource service-keys --output json Set an environment variable COS_CREDENTIALS with the name of the credentials, COS_CREDENTIALS=$(ibmcloud resource service-keys --output json | jq -r '.[0].credentials.iam_apikey_name') If you do not have credentials yet, create credentials for your IBM Cloud Object Storage instance with HMAC keys, ibmcloud resource service-key-create $COS_CREDENTIALS Writer --instance-name $COS_NAME --parameters '{\"HMAC\":true}' Will create credentials including among other HMAC keys, \"cos_hmac_keys\": { \"access_key_id\": \"c407e90c41c3463b8e7722048aa48edc\", \"secret_access_key\": \"0f8c2cb6ef82c63d8d1f935a8ef6a87fe1bc16ed1ba8483c\" }, Create environment variables with the HMAC keys from the first credentials listed in your service-keys list. Change the index value if you have multiple service-keys or credentials. Create an S3FS password file, COS_ACCESS_KEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.cos_hmac_keys.access_key_id') COS_SECRET_KEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.cos_hmac_keys.secret_access_key') COS_S3FS_PASSWORD_FILE=s3fs-passwd echo $COS_ACCESS_KEY:$COS_SECRET_KEY > $COS_S3FS_PASSWORD_FILE chmod 0600 $COS_S3FS_PASSWORD_FILE Mount Local File System \u00b6 Mount the local directory using S3FS, mkdir cos_data LOCAL_MOUNTPOINT=$(pwd)/cos_data COS_PUBLIC_ENDPOINT=s3.us-south.cloud-object-storage.appdomain.cloud s3fs $COS_BUCKET_NAME -o passwd_file=$(pwd)/$COS_S3FS_PASSWORD_FILE -o url=https://$COS_PUBLIC_ENDPOINT $LOCAL_MOUNTPOINT You should see the content of your IBM Cloud Object Storage, e.g using the Finder on macos or using the cli on macos, If you are using a bucket mounted to a MongoDB instance using s3fs-fuse , you will also see a directory data/db with all the MongoDB database files mounted to your local filesystem. References \u00b6 Mounting a bucket using s3fs","title":"Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS"},{"location":"generatedContent/kubernetes-storage/fuse/#mount-a-remote-object-storage-as-local-filesystem-in-userspace-fuse-with-s3fs","text":"","title":"Mount a Remote Object Storage as Local Filesystem in Userspace (FUSE) with S3FS"},{"location":"generatedContent/kubernetes-storage/fuse/#about-fuse","text":"Filesystem in Userspace (FUSE) lets non-privileged users create a file system in their user space. The FUSE project consists of two components: a FUSE kernel module that is part of the Linux kernel since version 2.6.14, and the libfuse userspace library. The libfuse library provides a reference implementation for communication with the FUSE kernel module, providing client functions to mount the file system, unmount it, read requests from the kernel, and send responses back. FUSE is particularly useful for writing Virtual File Systems (VFS).","title":"About FUSE"},{"location":"generatedContent/kubernetes-storage/fuse/#s3fs","text":"If you want to use Object Storage as the underlying storage for FUSE-based filesystem disk management, you can use s3fs or s3fs-fuse . s3fs is an Amazon S3 (Services Simple Storage) and S3-based object stores compatible utility for FUSE-based filesystem disk management that supports a subset of Single UNIX Specification (SUS) or POSIX including reading/writing files, directories, symlinks, mode, uid/gid, and extended attributes, while preserving the original file format, e.g. a plain text or MS Word document. Applications that need to read and write to an NFS-style filesystem can use s3fs, which integrates applications with S3 compatible storage like IBM Cloud Object Storage . s3fs also allows you to interact with your cloud storage using familiar shell commands, like ls for listing or cp to copy files. Performance is not equal to a true local filesystem, but you can use some advanced options to increase throughput. Object storage services have high-latency for time to first byte and lack random write access (requires rewriting the full object). Workloads that only read big files, like deep learning workloads, can achieve good throughput using s3fs. s3fs on macOS uses osxfuse . osxfuse is FUSE for macOS.","title":"s3fs"},{"location":"generatedContent/kubernetes-storage/fuse/#lab","text":"In this lab, you will mount a bucket using s3fs . You need admin access to install s3fs-fuse . In this lab, I used brew on macOS, for other Operating Systems, see the installation instructions . Pre-requirements: A free IBM Cloud account, A free IBM Cloud Object Storage (COS) instance with a bucket, Admin access to your client OS,","title":"Lab"},{"location":"generatedContent/kubernetes-storage/fuse/#connect-to-ibm-cloud","text":"You have to be logged in to your IBM Cloud account, ibmcloud login -u <IBMId> If you are using Single Sign-On (SSO) use the -sso flag to log in. Select the account with your instance of Object Storage. In the example below, I have to select account 1 under my own name, e.g. `B Newell's Account', Select an account: 1. B Newell's Account (31296e3a285f) 2. IBM Client Developer Advocacy (e65910fa61) <-> 1234567 Enter a number> **1** Targeted account B Newell's Account (31296e3a285f) Then, set an environment variable with the bucket name, and upload a document to your bucket in the IBM Cloud Object Storage instance. COS_NAME=<cos-instance-name> COS_BUCKET_NAME=<cos-bucket-name>","title":"Connect to IBM Cloud"},{"location":"generatedContent/kubernetes-storage/fuse/#upload-an-object-to-object-storage","text":"Create a new document, COS_OBJECT_KEY=helloworld.txt echo \"Hello World! Today is $(date)\" > $COS_OBJECT_KEY Upload an object using S3Manager , ibmcloud cos upload --bucket $COS_BUCKET_NAME --key $COS_OBJECT_KEY --file ./helloworld.txt --content-language en-US --content-type \"text/plain\" OK Successfully uploaded object 'helloworld.txt' to bucket 'e59a327194-cos-1-bucket1'.","title":"Upload an Object to Object Storage"},{"location":"generatedContent/kubernetes-storage/fuse/#install-s3fs","text":"Install s3fs , brew install s3fs If you already have credentials with HMAC keys created for your Object Storage instance, you can retrieve credentials using, ibmcloud resource service-keys --output json Set an environment variable COS_CREDENTIALS with the name of the credentials, COS_CREDENTIALS=$(ibmcloud resource service-keys --output json | jq -r '.[0].credentials.iam_apikey_name') If you do not have credentials yet, create credentials for your IBM Cloud Object Storage instance with HMAC keys, ibmcloud resource service-key-create $COS_CREDENTIALS Writer --instance-name $COS_NAME --parameters '{\"HMAC\":true}' Will create credentials including among other HMAC keys, \"cos_hmac_keys\": { \"access_key_id\": \"c407e90c41c3463b8e7722048aa48edc\", \"secret_access_key\": \"0f8c2cb6ef82c63d8d1f935a8ef6a87fe1bc16ed1ba8483c\" }, Create environment variables with the HMAC keys from the first credentials listed in your service-keys list. Change the index value if you have multiple service-keys or credentials. Create an S3FS password file, COS_ACCESS_KEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.cos_hmac_keys.access_key_id') COS_SECRET_KEY=$(ibmcloud resource service-key $COS_CREDENTIALS --output json | jq -r '.[0].credentials.cos_hmac_keys.secret_access_key') COS_S3FS_PASSWORD_FILE=s3fs-passwd echo $COS_ACCESS_KEY:$COS_SECRET_KEY > $COS_S3FS_PASSWORD_FILE chmod 0600 $COS_S3FS_PASSWORD_FILE","title":"Install s3fs"},{"location":"generatedContent/kubernetes-storage/fuse/#mount-local-file-system","text":"Mount the local directory using S3FS, mkdir cos_data LOCAL_MOUNTPOINT=$(pwd)/cos_data COS_PUBLIC_ENDPOINT=s3.us-south.cloud-object-storage.appdomain.cloud s3fs $COS_BUCKET_NAME -o passwd_file=$(pwd)/$COS_S3FS_PASSWORD_FILE -o url=https://$COS_PUBLIC_ENDPOINT $LOCAL_MOUNTPOINT You should see the content of your IBM Cloud Object Storage, e.g using the Finder on macos or using the cli on macos, If you are using a bucket mounted to a MongoDB instance using s3fs-fuse , you will also see a directory data/db with all the MongoDB database files mounted to your local filesystem.","title":"Mount Local File System"},{"location":"generatedContent/kubernetes-storage/fuse/#references","text":"Mounting a bucket using s3fs","title":"References"},{"location":"generatedContent/kubernetes-storage/getting-started/pre-work/","text":"Pre-work \u00b6 This section is broken up into the following steps: Sign up for IBM Cloud Download or clone the repo 1. Sign up for IBM Cloud \u00b6 Ensure you have an IBM Cloud ID 2. Download or clone the repo \u00b6 Various parts of this workshop will require the attendee to upload files or run scripts that we've stored in the repository. So let's get that done early on, you'll need git on your laptop to clone the repository directly, or access to GitHub.com to download the zip file. To Download, go to the GitHub repo for this workshop and download the archived version of the workshop and extract it on your laptop. Alternately, run the following command: git clone https://github.com/IBM/workshop-template cd workshop-template","title":"Pre-work"},{"location":"generatedContent/kubernetes-storage/getting-started/pre-work/#pre-work","text":"This section is broken up into the following steps: Sign up for IBM Cloud Download or clone the repo","title":"Pre-work"},{"location":"generatedContent/kubernetes-storage/getting-started/pre-work/#1-sign-up-for-ibm-cloud","text":"Ensure you have an IBM Cloud ID","title":"1. Sign up for IBM Cloud"},{"location":"generatedContent/kubernetes-storage/getting-started/pre-work/#2-download-or-clone-the-repo","text":"Various parts of this workshop will require the attendee to upload files or run scripts that we've stored in the repository. So let's get that done early on, you'll need git on your laptop to clone the repository directly, or access to GitHub.com to download the zip file. To Download, go to the GitHub repo for this workshop and download the archived version of the workshop and extract it on your laptop. Alternately, run the following command: git clone https://github.com/IBM/workshop-template cd workshop-template","title":"2. Download or clone the repo"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/","text":"Introduction to Microservice \u00b6 Prerequisite \u00b6 Access to a Kubernetes cluster - A cluster was created when the session started IBM Cloud Function sending email notification - was created before the session Docker image of each service has been made available on Docker Hub. Setup \u00b6 Lab Environment Setup Every time when you start a new terminal/command window, steps in the section must be performed to setup a new environment. Follow the instructions here Lab 1 - Deploy Office Space sample application on Kubernetes \u00b6 This lab demonstrates how a sample cloud native application can be deployed on top of Kubernetes. Follow the lab instructions here Lab 2 - Develop a Java Microservice with Spring Boot \u00b6 Optionally, if your IBM Cloud account supported a paid Kubernetes cluster, you may perform the steps in this lab to develop a Java Microservice in no time. In this lab, you develop a Java Microservice using the Java Microservice with Spring starter kit. With this kit, you can have a basic Java microservice developed and deployed in a few minutes. You create a CI/CD pipeline in IBM Cloud which automates your service's deployment to Kubernetes cluster. Follow the lab instructions here","title":"Deploy Microservices Lab"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/#introduction-to-microservice","text":"","title":"Introduction to Microservice"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/#prerequisite","text":"Access to a Kubernetes cluster - A cluster was created when the session started IBM Cloud Function sending email notification - was created before the session Docker image of each service has been made available on Docker Hub.","title":"Prerequisite"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/#setup","text":"Lab Environment Setup Every time when you start a new terminal/command window, steps in the section must be performed to setup a new environment. Follow the instructions here","title":"Setup"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/#lab-1-deploy-office-space-sample-application-on-kubernetes","text":"This lab demonstrates how a sample cloud native application can be deployed on top of Kubernetes. Follow the lab instructions here","title":"Lab 1 - Deploy Office Space sample application on Kubernetes"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/#lab-2-develop-a-java-microservice-with-spring-boot","text":"Optionally, if your IBM Cloud account supported a paid Kubernetes cluster, you may perform the steps in this lab to develop a Java Microservice in no time. In this lab, you develop a Java Microservice using the Java Microservice with Spring starter kit. With this kit, you can have a basic Java microservice developed and deployed in a few minutes. You create a CI/CD pipeline in IBM Cloud which automates your service's deployment to Kubernetes cluster. Follow the lab instructions here","title":"Lab 2 - Develop a Java Microservice with Spring Boot"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/CONTRIBUTING/","text":"Contributing \u00b6 This is an open source project, and we appreciate your help! We use the GitHub issue tracker to discuss new features and non-trivial bugs. In addition to the issue tracker, #journeys on Slack is the best way to get into contact with the project's maintainers. To contribute code, documentation, or tests, please submit a pull request to the GitHub repository. Generally, we expect two maintainers to review your pull request before it is approved for merging. For more details, see the MAINTAINERS page.","title":"Contributing"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/CONTRIBUTING/#contributing","text":"This is an open source project, and we appreciate your help! We use the GitHub issue tracker to discuss new features and non-trivial bugs. In addition to the issue tracker, #journeys on Slack is the best way to get into contact with the project's maintainers. To contribute code, documentation, or tests, please submit a pull request to the GitHub repository. Generally, we expect two maintainers to review your pull request before it is approved for merging. For more details, see the MAINTAINERS page.","title":"Contributing"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/MAINTAINERS/","text":"Maintainers Guide \u00b6 This guide is intended for maintainers - anybody with commit access to one or more Code Pattern repositories. Methodology \u00b6 This repository does not have a traditional release management cycle, but should instead be maintained as a useful, working, and polished reference at all times. While all work can therefore be focused on the master branch, the quality of this branch should never be compromised. The remainder of this document details how to merge pull requests to the repositories. Merge approval \u00b6 The project maintainers use LGTM (Looks Good To Me) in comments on the pull request to indicate acceptance prior to merging. A change requires LGTMs from two project maintainers. If the code is written by a maintainer, the change only requires one additional LGTM. Reviewing Pull Requests \u00b6 We recommend reviewing pull requests directly within GitHub. This allows a public commentary on changes, providing transparency for all users. When providing feedback be civil, courteous, and kind. Disagreement is fine, so long as the discourse is carried out politely. If we see a record of uncivil or abusive comments, we will revoke your commit privileges and invite you to leave the project. During your review, consider the following points: Does the change have positive impact? \u00b6 Some proposed changes may not represent a positive impact to the project. Ask whether or not the change will make understanding the code easier, or if it could simply be a personal preference on the part of the author (see bikeshedding ). Pull requests that do not have a clear positive impact should be closed without merging. Do the changes make sense? \u00b6 If you do not understand what the changes are or what they accomplish, ask the author for clarification. Ask the author to add comments and/or clarify test case names to make the intentions clear. At times, such clarification will reveal that the author may not be using the code correctly, or is unaware of features that accommodate their needs. If you feel this is the case, work up a code sample that would address the pull request for them, and feel free to close the pull request once they confirm. Does the change introduce a new feature? \u00b6 For any given pull request, ask yourself \"is this a new feature?\" If so, does the pull request (or associated issue) contain narrative indicating the need for the feature? If not, ask them to provide that information. Are new unit tests in place that test all new behaviors introduced? If not, do not merge the feature until they are! Is documentation in place for the new feature? (See the documentation guidelines). If not do not merge the feature until it is! Is the feature necessary for general use cases? Try and keep the scope of any given component narrow. If a proposed feature does not fit that scope, recommend to the user that they maintain the feature on their own, and close the request. You may also recommend that they see if the feature gains traction among other users, and suggest they re-submit when they can show such support.","title":"Maintainers Guide"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/MAINTAINERS/#maintainers-guide","text":"This guide is intended for maintainers - anybody with commit access to one or more Code Pattern repositories.","title":"Maintainers Guide"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/MAINTAINERS/#methodology","text":"This repository does not have a traditional release management cycle, but should instead be maintained as a useful, working, and polished reference at all times. While all work can therefore be focused on the master branch, the quality of this branch should never be compromised. The remainder of this document details how to merge pull requests to the repositories.","title":"Methodology"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/MAINTAINERS/#merge-approval","text":"The project maintainers use LGTM (Looks Good To Me) in comments on the pull request to indicate acceptance prior to merging. A change requires LGTMs from two project maintainers. If the code is written by a maintainer, the change only requires one additional LGTM.","title":"Merge approval"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/MAINTAINERS/#reviewing-pull-requests","text":"We recommend reviewing pull requests directly within GitHub. This allows a public commentary on changes, providing transparency for all users. When providing feedback be civil, courteous, and kind. Disagreement is fine, so long as the discourse is carried out politely. If we see a record of uncivil or abusive comments, we will revoke your commit privileges and invite you to leave the project. During your review, consider the following points:","title":"Reviewing Pull Requests"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/MAINTAINERS/#does-the-change-have-positive-impact","text":"Some proposed changes may not represent a positive impact to the project. Ask whether or not the change will make understanding the code easier, or if it could simply be a personal preference on the part of the author (see bikeshedding ). Pull requests that do not have a clear positive impact should be closed without merging.","title":"Does the change have positive impact?"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/MAINTAINERS/#do-the-changes-make-sense","text":"If you do not understand what the changes are or what they accomplish, ask the author for clarification. Ask the author to add comments and/or clarify test case names to make the intentions clear. At times, such clarification will reveal that the author may not be using the code correctly, or is unaware of features that accommodate their needs. If you feel this is the case, work up a code sample that would address the pull request for them, and feel free to close the pull request once they confirm.","title":"Do the changes make sense?"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/MAINTAINERS/#does-the-change-introduce-a-new-feature","text":"For any given pull request, ask yourself \"is this a new feature?\" If so, does the pull request (or associated issue) contain narrative indicating the need for the feature? If not, ask them to provide that information. Are new unit tests in place that test all new behaviors introduced? If not, do not merge the feature until they are! Is documentation in place for the new feature? (See the documentation guidelines). If not do not merge the feature until it is! Is the feature necessary for general use cases? Try and keep the scope of any given component narrow. If a proposed feature does not fit that scope, recommend to the user that they maintain the feature on their own, and close the request. You may also recommend that they see if the feature gains traction among other users, and suggest they re-submit when they can show such support.","title":"Does the change introduce a new feature?"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/","text":"\u5728 Kubernetes \u4e0a\u6784\u5efa\u548c\u90e8\u7f72 Java Spring Boot \u5fae\u670d\u52a1 \u00b6 \u9605\u8bfb\u672c\u6587\u7684\u5176\u4ed6\u8bed\u8a00\u7248\u672c\uff1a English \u3002 Spring Boot \u662f\u5e38\u7528 Java \u5fae\u670d\u52a1\u6846\u67b6\u4e4b\u4e00\u3002Spring Cloud \u62e5\u6709\u4e00\u7ec4\u4e30\u5bcc\u7684\u3001\u826f\u597d\u96c6\u6210\u7684 Java \u7c7b\u5e93\uff0c\u7528\u4e8e\u5e94\u5bf9 Java \u5e94\u7528\u7a0b\u5e8f\u5806\u6808\u4e2d\u53d1\u751f\u7684\u8fd0\u884c\u65f6\u95ee\u9898\uff1b\u800c Kubernetes \u5219\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u529f\u80fd\u96c6\u6765\u8fd0\u884c\u591a\u8bed\u8a00\u5fae\u670d\u52a1\u3002\u8fd9\u4e9b\u6280\u672f\u5f7c\u6b64\u4e92\u8865\uff0c\u4e3a Spring Boot \u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5e73\u53f0\u3002 \u5728\u6b64\u4ee3\u7801\u4e2d\uff0c\u6211\u4eec\u6f14\u793a\u4e86\u5982\u4f55\u5728 Kubernetes \u4e0a\u90e8\u7f72\u4e00\u4e2a\u7b80\u5355\u7684 Spring Boot \u5e94\u7528\u7a0b\u5e8f\u3002\u6b64\u5e94\u7528\u7a0b\u5e8f\u79f0\u4e3a Office Space\uff0c\u5b83\u6a21\u4eff\u4e86\u7535\u5f71 \u4e0a\u73ed\u4e00\u6761\u866b (Office Space) \u4e2d Michael Bolton \u7684\u865a\u6784\u5e94\u7528\u7a0b\u5e8f\u521b\u610f\u3002\u8be5\u5e94\u7528\u7a0b\u5e8f\u5229\u7528\u4e86\u8fd9\u6837\u4e00\u4e2a\u91d1\u878d\u65b9\u6848\uff1a\u901a\u5e38\u4e0d\u6ee1\u4e00\u5206\u94b1\u7684\u5206\u5e01\u4f1a\u56db\u820d\u4e94\u5165\uff0c\u800c\u6b64\u65b9\u6848\u5c06\u8fd9\u90e8\u5206\u5e01\u503c\u8f6c\u79fb\u5230\u4e00\u4e2a\u72ec\u7acb\u7684\u94f6\u884c\u8d26\u6237\u4e2d\u6765\u8ba1\u7b97\u4ea4\u6613\u5229\u606f\u3002 \u8be5\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528 Java 8/Spring Boot \u5fae\u670d\u52a1\u8ba1\u7b97\u5229\u606f\uff0c\u7136\u540e\u5c06\u5206\u5e01\u5b58\u5165\u6570\u636e\u5e93\u3002\u53e6\u4e00\u4e2a Spring Boot \u5fae\u670d\u52a1\u662f\u901a\u77e5\u670d\u52a1\u3002\u5f53\u8d26\u6237\u4f59\u989d\u8d85\u8fc7 50,000 \u7f8e\u5143\u65f6\uff0c\u5b83\u4f1a\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u3002\u5b83\u662f\u7531\u8ba1\u7b97\u5229\u606f\u7684 Spring Boot Web \u670d\u52a1\u5668\u89e6\u53d1\u7684\u3002\u524d\u7aef\u4f7f\u7528 Node.js \u5e94\u7528\u7a0b\u5e8f\u6765\u663e\u793a Spring Boot \u5e94\u7528\u7a0b\u5e8f\u7d2f\u79ef\u7684\u5f53\u524d\u8d26\u6237\u4f59\u989d\u3002\u540e\u7aef\u4f7f\u7528 MySQL \u6570\u636e\u5e93\u6765\u5b58\u50a8\u8d26\u6237\u4f59\u989d\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u4f7f\u7528 Minikube \u521b\u5efa\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u7528\u4e8e\u672c\u5730\u6d4b\u8bd5\uff0c\u4f7f\u7528 IBM Cloud Private \u6216\u8005 IBM Cloud Container Service \u521b\u5efa\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u4ee5\u90e8\u7f72\u5230\u4e91\u4e2d\u3002\u672c\u6587\u4e2d\u7684\u4ee3\u7801\u4f7f\u7528 Travis \u5b9a\u671f\u4f7f\u7528 \u57fa\u4e8e IBM Cloud Container Service \u7684 Kubernetes \u96c6\u7fa4 \u8fdb\u884c\u6d4b\u8bd5\u3002 \u6b65\u9aa4 \u00b6 1. \u521b\u5efa\u6570\u636e\u5e93\u670d\u52a1 1.1 \u5728\u5bb9\u5668\u4e2d\u4f7f\u7528 MySQL \u6216\u8005 1.2 \u4f7f\u7528 IBM Cloud MySQL\u670d\u52a1 2. \u521b\u5efa Spring Boot \u5fae\u670d\u52a1 2.1 \u4f7f\u7528 Maven \u6784\u5efa\u9879\u76ee 2.2 \u6784\u5efa\u548c\u63a8\u9001 Docker \u955c\u50cf 2.3 \u4e3a Spring Boot \u670d\u52a1\u4fee\u6539 yaml \u6587\u4ef6 2.3.1 \u5728\u901a\u77e5\u670d\u52a1\u4e2d\u4f7f\u7528\u9ed8\u8ba4\u7535\u5b50\u90ae\u4ef6\u670d\u52a1 \u6216\u8005 2.3.2 \u5728\u901a\u77e5\u670d\u52a1\u4e2d\u4f7f\u7528 OpenWhisk Actions 2.4 \u90e8\u7f72 Spring Boot \u5fae\u670d\u52a1 3. \u521b\u5efa\u524d\u7aef\u670d\u52a1 4. \u521b\u5efa\u4ea4\u6613\u751f\u6210\u5668\u670d\u52a1 5. \u8bbf\u95ee\u5e94\u7528\u7a0b\u5e8f \u6545\u969c\u6392\u9664 \u00b6 1.\u521b\u5efa\u6570\u636e\u5e93\u670d\u52a1 \u00b6 \u540e\u7aef\u5305\u542b MySQL \u6570\u636e\u5e93\u548c Spring Boot \u5e94\u7528\u7a0b\u5e8f\u3002\u6bcf\u4e00\u9879 \u5fae\u670d\u52a1\u90fd\u5305\u542b\u4e00\u4e2a Kubernetes Deployment \u548c\u4e00\u4e2a Kubernetes Service\u3002Kubernetes Deployment \u7528\u4e8e\u7ba1\u7406\u6bcf\u4e00\u9879\u5fae\u670d\u52a1\u6240\u542f\u52a8\u7684 pod\u3002Kubernetes Service \u7528\u4e8e \u4e3a\u6bcf\u4e00\u9879\u5fae\u670d\u52a1\u521b\u5efa\u4e00\u4e2a\u7a33\u5b9a\u7684 DNS \u8bb0\u5f55\uff0c\u4ee5\u4fbf\u5b83\u4eec\u53ef\u4ee5 \u6839\u636e\u57df\u540d\u76f8\u4e92\u5f15\u7528\u3002 \u521b\u5efa MySQL \u6570\u636e\u5e93\u540e\u7aef\u7684\u65b9\u6cd5\u6709\u4e24\u79cd\uff1a \u5728\u5bb9\u5668\u4e2d\u4f7f\u7528 MySQL \u6216\u8005 \u4f7f\u7528 IBM Cloud MySQL \u670d\u52a1 1.1 \u5728\u5bb9\u5668\u4e2d\u4f7f\u7528 MySQL \u00b6 $ kubectl create -f account-database.yaml service \"account-database\" created deployment \"account-database\" created \u9ed8\u8ba4\u8ba4\u8bc1\u4fe1\u606f\u5df2\u4f7f\u7528 base64 \u5728 secrets.yaml \u4e2d\u8fdb\u884c\u4e86\u7f16\u7801\u3002 base64 \u7f16\u7801\u4e0d\u4f1a\u52a0\u5bc6\u6216\u9690\u85cf\u60a8\u7684\u5bc6\u94a5\u3002\u8bf7\u52ff\u5c06\u5176\u4e0a\u4f20\u81f3\u60a8\u7684 Github \u4ed3\u5e93\u4e2d\u3002 $ kubectl apply -f secrets.yaml secret \"demo-credentials\" created \u4e0b\u4e00\u6b65\u8bf7\u53c2\u8003 \u6b65\u9aa4 2 \u3002 1.2 \u4f7f\u7528 IBM Cloud MySQL \u670d\u52a1 \u00b6 \u901a\u8fc7 https://console.ng.bluemix.net/catalog/services/compose-for-mysql \u5728 IBM Cloud \u4e2d\u4e3a MySQL \u63d0\u4f9b Provision Compose \u8f6c\u81f3 Service credentials \u5e76\u67e5\u770b\u60a8\u7684\u51ed\u8bc1\u3002\u5305\u62ec MySQL \u4e3b\u673a\u540d\u3001\u7aef\u53e3\u3001\u7528\u6237\u540d\u548c\u5bc6\u7801\u7b49\u4fe1\u606f\u4f4d\u4e8e\u51ed\u8bc1 URI \u4e0b\uff0c\u5982\u4e0b\u6240\u793a\uff1a \u60a8\u5c06\u9700\u8981\u5e94\u7528\u8fd9\u4e9b\u51ed\u8bc1\u4f5c\u4e3a Kubernetes \u96c6\u7fa4\u4e2d\u7684\u5bc6\u94a5\u3002\u8fd9\u4e9b\u4fe1\u606f\u5e94\u5df2\u88ab base64 \u7f16\u7801\u3002 \u4f7f\u7528\u811a\u672c ./scripts/create-secrets.sh \u3002\u7cfb\u7edf\u5c06\u63d0\u793a\u60a8\u8f93\u5165\u81ea\u5df1\u7684\u51ed\u8bc1\u3002\u8fd9\u5c06\u5bf9\u60a8\u8f93\u5165\u7684\u51ed\u8bc1\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u521b\u5efa Kubenetes Secret \u5bf9\u8c61\u3002 $ ./scripts/create-secrets.sh Enter MySQL username: admin Enter MySQL password: password Enter MySQL host: hostname Enter MySQL port: 23966 secret \"demo-credentials\" created \u60a8\u4e5f\u53ef\u4ee5\u7f16\u8f91 secrets.yaml \u6587\u4ef6\uff0c\u5c06\u5176\u4e2d\u7684\u6570\u636e\u503c\u7f16\u8f91\u4e3a\u81ea\u5df1\u7684 base64 \u7f16\u7801\u7684\u51ed\u8bc1\u3002\u7136\u540e\u6267\u884c kubectl apply -f secrets.yaml \u3002 \u4e0b\u4e00\u6b65\u8bf7\u53c2\u8003 \u6b65\u9aa4 2 \u3002 2.\u521b\u5efa Spring Boot \u5fae\u670d\u52a1 \u00b6 \u60a8\u9700\u8981 \u5b89\u88c5 Maven \u5de5\u5177\u3002 \u5982\u679c\u8981\u4fee\u6539 Spring Boot \u5e94\u7528\u7a0b\u5e8f\uff0c\u8bf7\u5728\u6784\u5efa Java \u9879\u76ee\u548c Docker \u955c\u50cf\u4e4b\u524d\u5b8c\u6210\u4fee\u6539\u3002 Spring Boot \u5fae\u670d\u52a1\u5305\u62ec Compute-Interest-API \u548c Send-Notification \u3002 Compute-Interest-API \u662f\u4e00\u4e2a\u9700\u8981\u4f7f\u7528 MySQL \u6570\u636e\u5e93\u7684 Spring Boot \u5e94\u7528\u7a0b\u5e8f\u3002\u76f8\u5173\u914d\u7f6e\u4f4d\u4e8e spring.datasource* \u4e2d\u7684 application.properties \u4e2d\u3002 compute-interest-api/src/main/resources/application.properties spring.datasource.url = jdbc:mysql://${MYSQL_DB_HOST}:${MYSQL_DB_PORT}/dockercon2017 # Username and password spring.datasource.username = ${MYSQL_DB_USER} spring.datasource.password = ${MYSQL_DB_PASSWORD} application.properties \u914d\u7f6e\u4e3a\u4f7f\u7528 MYSQL_DB_* \u73af\u5883\u53d8\u91cf\u3002\u8fd9\u4e9b\u53d8\u91cf\u5728 compute-interest-api.yaml \u6587\u4ef6\u4e2d\u5b9a\u4e49\u3002 compute-interest-api.yaml spec : containers : - image : anthonyamanse/compute-interest-api:secrets imagePullPolicy : Always name : compute-interest-api env : - name : MYSQL_DB_USER valueFrom : secretKeyRef : name : demo-credentials key : username - name : MYSQL_DB_PASSWORD valueFrom : secretKeyRef : name : demo-credentials key : password - name : MYSQL_DB_HOST valueFrom : secretKeyRef : name : demo-credentials key : host - name : MYSQL_DB_PORT valueFrom : secretKeyRef : name : demo-credentials key : port ports : - containerPort : 8080 YAML \u6587\u4ef6\u5df2\u914d\u7f6e\u4e3a\u4ece\u5148\u524d\u521b\u5efa\u7684 Kubernetes Secret \u4e2d\u83b7\u53d6\u503c\u3002\u8fd9\u4e9b\u4fe1\u606f\u5c06\u6700\u7ec8\u5199\u5165 application.properties \u5e76\u6700\u7ec8\u4e3a Spring Boot \u5e94\u7528\u7a0b\u5e8f\u6240\u7528\u3002 Send-Notification \u53ef\u914d\u7f6e\u4e3a\u901a\u8fc7 Gmail \u548c/\u6216 Slack \u53d1\u9001\u901a\u77e5\u3002\u901a\u77e5\u4ec5\u5728 MySQL \u6570\u636e\u5e93\u4e2d\u7684\u8d26\u6237\u4f59\u989d\u8d85\u8fc7 50,000 \u7f8e\u5143\u65f6\u63a8\u9001\u4e00\u6b21\u3002\u9ed8\u8ba4\u8bbe\u7f6e\u4e3a\u4f7f\u7528 Gmail \u3002\u901a\u77e5\u3002\u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528\u4e8b\u4ef6\u9a71\u52a8\u6280\u672f\uff08\u5728\u672c\u4f8b\u4e2d\u4e3a OpenWhisk \uff09 \u6765\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u548c Slack \u6d88\u606f\u3002\u8981\u5c06 OpenWhisk \u4e0e\u60a8\u7684\u901a\u77e5\u5fae\u670d\u52a1\u914d\u5408\u4f7f\u7528\uff0c\u8bf7\u5728\u6784\u5efa\u548c\u90e8\u7f72\u5fae\u670d\u52a1\u6620\u50cf\u4e4b\u524d\u9075\u5faa \u6b64\u5904 \u7684\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c\u3002\u5426\u5219\uff0c\u53ea\u6709\u5728\u9009\u62e9\u4ec5\u4f7f\u7528\u7535\u5b50\u90ae\u4ef6\u901a\u77e5\u540e\u624d\u80fd\u7ee7\u7eed\u3002 2.1.\u4f7f\u7528 Maven \u6784\u5efa\u9879\u76ee \u00b6 \u5f53 Maven \u6210\u529f\u6784\u5efa Java \u9879\u76ee\u540e\uff0c\u60a8\u9700\u8981\u4f7f\u7528\u5728\u5176\u76f8\u5e94\u6587\u4ef6\u5939\u4e2d\u63d0\u4f9b\u7684 Dockerfile \u6784\u5efa Docker \u955c\u50cf\u3002 \u5907\u6ce8\uff1acompute-interest-api \u4f1a\u5c06\u5206\u5e01\u4e58\u4ee5 100,000\uff0c\u7528\u4e8e\u6267\u884c\u6a21\u62df\u3002\u60a8\u53ef\u4ee5\u7f16\u8f91/\u79fb\u9664 src/main/java/officespace/controller/MainController.java \u4e2d\u7684 remainingInterest *= 100000 \u884c\u3002\u5f53\u4f59\u989d\u8d85\u8fc7 50,000 \u7f8e\u5143\u65f6\uff0c\u7a0b\u5e8f\u8fd8\u4f1a\u53d1\u9001\u901a\u77e5\uff0c \u60a8\u53ef\u4ee5\u7f16\u8f91 if (updatedBalance > 50000 && emailSent == false ) \u884c\u4e2d\u7684\u6570\u5b57\u3002\u4fdd\u5b58\u66f4\u6539\u540e\uff0c\u5c31\u53ef\u4ee5\u6784\u5efa\u9879\u76ee\u4e86\u3002 Go to containers/compute-interest-api $ mvn package Go to containers/send-notification $ mvn package \u6211\u4eec\u5c06\u4f7f\u7528 IBM Cloud \u5bb9\u5668\u955c\u50cf\u4ed3\u5e93\u6765\u4fdd\u5b58\u955c\u50cf\uff08\u7531\u6b64\u8fdb\u884c\u6620\u50cf\u547d\u540d\uff09\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528 Docker Hub \u4fdd\u5b58\u955c\u50cf\u3002 2.2 \u4e3a Spring Boot \u670d\u52a1\u6784\u5efa Docker \u6620\u50cf \u00b6 \u5907\u6ce8\uff1a\u672c\u6587\u4f7f\u7528 IBM Cloud \u5bb9\u5668\u955c\u50cf\u5e93\u4e2d\u4fdd\u5b58\u955c\u50cf\u3002 \u5982\u679c\u60a8\u8ba1\u5212\u4f7f\u7528 IBM Cloud \u5bb9\u5668\u955c\u50cf\u5e93\uff0c\u9700\u8981\u9996\u5148\u8bbe\u7f6e\u5e10\u6237\u3002\u8bf7\u9075\u5faa \u6b64\u5904 \u7684\u6559\u7a0b\u8fdb\u884c\u64cd\u4f5c\u3002 \u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528 Docker Hub \u4fdd\u5b58\u955c\u50cf\u3002 $ docker build -t registry.ng.bluemix.net/<namespace>/compute-interest-api . $ docker build -t registry.ng.bluemix.net/<namespace>/send-notification . $ docker push registry.ng.bluemix.net/<namespace>/compute-interest-api $ docker push registry.ng.bluemix.net/<namespace>/send-notification 2.3 \u4e3a\u4f7f\u7528\u60a8\u6240\u6784\u5efa\u7684\u955c\u50cf\uff0c\u9700\u8981\u4fee\u6539 compute-interest-api.yaml \u548c send-notification.yaml \u6587\u4ef6 \u00b6 \u6210\u529f\u63a8\u9001\u955c\u50cf\u540e\uff0c\u60a8\u5c06\u9700\u8981\u4fee\u6539 yaml \u6587\u4ef6\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u955c\u50cf\u3002 // compute-interest-api.yaml spec : containers : - image : registry.ng.bluemix.net/<namespace>/compute-interest-api # replace with your image name // send-notification.yaml spec : containers : - image : registry.ng.bluemix.net/<namespace>/send-notification # replace with your image name \u5b58\u5728\u4e24\u79cd\u53ef\u80fd\u7684\u901a\u77e5\u65b9\u5f0f\uff0c\u8bf7\u53c2\u89c1\uff1a 2.3.1 \u4f7f\u7528\u9ed8\u8ba4\u7535\u5b50\u90ae\u4ef6\u670d\u52a1 \u6216 2.3.2 \u4f7f\u7528 OpenWhisk Actions \u3002 2.3.1 \u4f7f\u7528\u9ed8\u8ba4\u7535\u5b50\u90ae\u4ef6\u670d\u52a1 (Gmail) \u6765\u5904\u7406\u901a\u77e5\u670d\u52a1 \u00b6 \u60a8\u5c06\u9700\u8981\u4fee\u6539 send-notification.yaml \u4e2d\u7684 \u73af\u5883\u53d8\u91cf \uff1a env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # change this to the gmail that will send the email - name : GMAIL_SENDER_PASSWORD value : 'password' # change this to the the password of the gmail above - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # change this to the email of the receiver \u73b0\u5728\uff0c\u60a8\u53ef\u4ee5\u7ee7\u7eed\u6267\u884c \u6b65\u9aa4 2.4 \u3002 2.3.2 \u4f7f\u7528 OpenWhisk Action \u6765\u5904\u7406\u901a\u77e5\u670d\u52a1 \u00b6 \u672c\u90e8\u5206\u7684\u8981\u6c42\uff1a * \u60a8\u7684 Slack \u56e2\u961f\u4e2d\u5177\u6709 Slack Incoming Webhook \u3002 * IBM Cloud\u5e10\u6237 \uff0c\u4ee5\u4fbf\u4f7f\u7528 OpenWhisk CLI \u3002 2.3.2.1 \u521b\u5efa Actions \u00b6 \u672c\u4ee3\u7801\u5e93\u7684\u6839\u76ee\u5f55\u4e2d\u5305\u542b\u60a8\u521b\u5efa OpenWhisk Actions \u65f6\u6240\u9700\u7684\u4ee3\u7801\u3002 \u5982\u679c\u60a8\u5c1a\u672a\u5b89\u88c5 OpenWhisk CLI\uff0c\u8bf7\u8f6c\u81f3 \u6b64\u5904 \u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 wsk \u547d\u4ee4\u6765\u521b\u5efa OpenWhisk Actions\u3002\u521b\u5efa\u64cd\u4f5c\u4f7f\u7528\u4ee5\u4e0b\u8bed\u6cd5\uff1a wsk action create < action_name > < source code for action> [add --param for optional Default parameters] * \u521b\u5efa\u7528\u4e8e\u53d1\u9001 Slack \u901a\u77e5 \u7684 Action $ wsk action create sendSlackNotification sendSlack.js --param url https://hooks.slack.com/services/XXXX/YYYY/ZZZZ Replace the url with your Slack team ' s incoming webhook url. * \u521b\u5efa\u7528\u4e8e\u53d1\u9001 Gmail \u901a\u77e5 \u7684 Action $ wsk action create sendEmailNotification sendEmail.js 2.3.2.2 \u6d4b\u8bd5 Actions \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 wsk action invoke [action name] [add --param to pass parameters] \u6d4b\u8bd5 OpenWhisk Actions * \u8c03\u7528 Slack \u901a\u77e5 $ wsk action invoke sendSlackNotification --param text \"Hello from OpenWhisk\" * \u8c03\u7528\u7535\u5b50\u90ae\u4ef6\u901a\u77e5 $ wsk action invoke sendEmailNotification --param sender [ sender 's email] --param password [sender' s password ] --param receiver [ receiver ' s email ] --param subject [ Email subject ] --param text [ Email Body ] \u81f3\u6b64\uff0c\u60a8\u5e94\u5206\u522b\u6536\u5230\u4e00\u6761 Slack \u6d88\u606f\u548c\u4e00\u5c01\u7535\u5b50\u90ae\u4ef6\u3002 2.3.2.3 \u4e3a Actions \u521b\u5efa REST API \u00b6 \u60a8\u53ef\u4ee5\u4f7f\u7528 wsk api create \u4e3a\u521b\u5efa\u7684 Action \u6620\u5c04 REST API \u7aef\u70b9\u3002\u5176\u8bed\u6cd5\u4e3a wsk api create [base-path] [api-path] [verb (GET PUT POST etc)] [action name] * \u521b\u5efa\u7528\u4e8e Slack \u901a\u77e5 \u7684 REST API \u7aef\u70b9 $ wsk api create /v1 /slack POST sendSlackNotification ok: created API /v1/email POST for action /_/sendEmailNotification https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack * \u521b\u5efa\u7528\u4e8e Gmail \u901a\u77e5 \u7684 REST API \u7aef\u70b9 $ wsk api create /v1 /email POST sendEmailNotification ok: created API /v1/email POST for action /_/sendEmailNotification https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email \u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b API \u5217\u8868\uff1a $ wsk api list ok: APIs Action Verb API Name URL /Anthony.Amanse_dev/sendEmailNotificatio post /v1 https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email /Anthony.Amanse_dev/testDefault post /v1 https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack \u8bf7\u8bb0\u5f55 \u8fd9\u4e9b API URL\uff0c\u7a0d\u540e\u6211\u4eec\u5c06\u4f7f\u7528\u5b83\u4eec \u3002 2.3.2.4 \u6d4b\u8bd5 REST API URL \u00b6 \u6d4b\u8bd5\u7528\u4e8e Slack \u901a\u77e5 \u7684 REST API \u7aef\u70b9\u3002\u8fd9\u91cc\u8bf7\u4f7f\u7528\u60a8\u81ea\u5df1\u7684 API URL\u3002 $ curl -X POST -d '{ \"text\": \"Hello from OpenWhisk\" }' https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack \u6d4b\u8bd5\u7528\u4e8e Gmail \u901a\u77e5 \u7684 REST API \u7aef\u70b9\u3002\u8fd9\u91cc\u8bf7\u4f7f\u7528\u60a8\u81ea\u5df1\u7684 API URL\u3002\u5c06\u53c2\u6570 sender\u3001password\u3001receiver \u548c subject \u7684\u503c\u66ff\u6362\u4e3a\u60a8\u81ea\u5df1\u7684\u503c\u3002 $ curl -X POST -d '{ \"text\": \"Hello from OpenWhisk\", \"subject\": \"Email Notification\", \"sender\": \"testemail@gmail.com\", \"password\": \"passwordOfSender\", \"receiver\": \"receiversEmail\" }' https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email 2.3.2.5 \u5c06 REST API URL \u6dfb\u52a0\u5230 yaml \u6587\u4ef6\u4e2d \u00b6 \u4e00\u65e6\u786e\u8ba4\u60a8\u7684 API \u8fd0\u884c\u6b63\u5e38\uff0c\u5c31\u53ef\u4ee5\u5c06\u8fd9\u4e9b URL \u653e\u5165 send-notification.yaml \u6587\u4ef6\u4e2d\u4e86 env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # the sender's email - name : GMAIL_SENDER_PASSWORD value : 'password' # the sender's password - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # the receiver's email - name : OPENWHISK_API_URL_SLACK value : 'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack' # your API endpoint for slack notifications - name : SLACK_MESSAGE value : 'Your balance is over $50,000.00' # your custom message - name : OPENWHISK_API_URL_EMAIL value : 'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email' # your API endpoint for email notifications 2.4 \u90e8\u7f72 Spring Boot \u5fae\u670d\u52a1 \u00b6 $ kubectl create -f compute-interest-api.yaml service \"compute-interest-api\" created deployment \"compute-interest-api\" created $ kubectl create -f send-notification.yaml service \"send-notification\" created deployment \"send-notification\" created 3.\u521b\u5efa\u524d\u7aef\u670d\u52a1 \u00b6 \u6b64\u7528\u6237\u754c\u9762\u662f Node.js \u5e94\u7528\u7a0b\u5e8f\uff0c\u53ef\u663e\u793a\u8d26\u6237\u603b\u4f59\u989d\u3002 \u5982\u679c\u60a8\u5728 IBM Cloud \u4e2d\u4f7f\u7528 MySQL \u6570\u636e\u5e93\uff0c\u8bf7\u8bb0\u5f97\u586b\u5145 account-summary.yaml \u6587\u4ef6\u4e2d\u73af\u5883\u53d8\u91cf\u7684\u503c\uff0c\u5426\u5219\u8bf7\u5c06\u5176\u7559\u7a7a\u3002\u8fd9\u662f\u5728 \u6b65\u9aa4 1 \u4e2d\u6267\u884c\u7684\u64cd\u4f5c\u3002 \u521b\u5efa Node.js \u524d\u7aef\uff1a $ kubectl create -f account-summary.yaml service \"account-summary\" created deployment \"account-summary\" created 4.\u521b\u5efa\u4ea4\u6613\u751f\u6210\u5668\u670d\u52a1 \u00b6 \u4ea4\u6613\u751f\u6210\u5668\u662f Python \u5e94\u7528\u7a0b\u5e8f\uff0c\u53ef\u4f7f\u7528\u7d2f\u79ef\u5229\u606f\u751f\u6210\u968f\u673a\u4ea4\u6613\u3002 * \u521b\u5efa\u4ea4\u6613\u751f\u6210\u5668 Python \u5e94\u7528\u7a0b\u5e8f\uff1a $ kubectl create -f transaction-generator.yaml service \"transaction-generator\" created deployment \"transaction-generator\" created 5.\u8bbf\u95ee\u5e94\u7528\u7a0b\u5e8f \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7 Kubernetes Cluster IP \u548c NodePort \u8bbf\u95ee\u5e94\u7528\u7a0b\u5e8f\u3002NodePort \u5e94\u4e3a 30080 \u3002 \u8981\u67e5\u627e Cluster IP\uff0c\u8bf7\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a $ bx cs workers <cluster-name> ID Public IP Private IP Machine Type State Status kube-dal10-paac005a5fa6c44786b5dfb3ed8728548f-w1 169 .47.241.213 10 .177.155.13 free normal Ready \u8981\u67e5\u627e\u8d26\u6237\u6458\u8981 (account-summary) \u670d\u52a1\u7684 NodePort\uff0c\u8bf7\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ... account-summary 10 .10.10.74 <nodes> 80 :30080/TCP 2d ... \u5728\u60a8\u7684\u6d4f\u89c8\u5668\u4e0a\uff0c\u8f6c\u81f3 http://<your-cluster-IP>:30080 \u6545\u969c\u6392\u9664 \u00b6 \u8981\u4ece\u5934\u5f00\u59cb\uff0c\u8bf7\u5220\u9664\u6240\u6709\u5185\u5bb9\uff1a kubectl delete svc,deploy -l app=office-space \u53c2\u8003\u8d44\u6599 \u00b6 John Zaccone - \u901a\u8fc7 Docker \u90e8\u7f72\u7684 Office Space \u5e94\u7528\u7a0b\u5e8f \u7684\u539f\u59cb\u4f5c\u8005\u3002 Office Space \u5e94\u7528\u7a0b\u5e8f\u662f\u4ee5 1999 \u5e74\u8fd0\u7528\u6b64\u7406\u5ff5\u7684\u540c\u540d\u7535\u5f71\u4e3a\u57fa\u7840\u7f16\u5199\u7684\u3002 \u8bb8\u53ef \u00b6 Apache 2.0 \u9690\u79c1\u58f0\u660e \u00b6 \u53ef\u4ee5\u914d\u7f6e\u5305\u542b\u8fd9\u4e2a\u5305\u7684\u6837\u672c Kubernetes Yaml \u6587\u4ef6\uff0c\u4ee5\u8ddf\u8e2a\u5bf9 IBM Cloud \u548c\u5176\u4ed6 Kubernetes \u5e73\u53f0\u7684\u90e8\u7f72\u3002\u6bcf\u6b21\u90e8\u7f72\u65f6\uff0c\u90fd\u4f1a\u5c06\u4ee5\u4e0b\u4fe1\u606f\u53d1\u9001\u5230 Deployment Tracker \u670d\u52a1\uff1a Kubernetes \u96c6\u7fa4\u63d0\u4f9b\u8005\uff08 IBM Cloud\u3001Minikube \u7b49 \uff09 Kubernetes \u673a\u5668 ID ( MachineID ) \u8fd9\u4e2a Kubernetes \u4f5c\u4e1a\u4e2d\u7684\u73af\u5883\u53d8\u91cf\u3002 \u6b64\u6570\u636e\u662f\u4ece\u6837\u672c\u5e94\u7528\u7a0b\u5e8f\u7684 yaml \u6587\u4ef6\u4e2d\u7684 Kubernetes Job \u6536\u96c6\u800c\u6765\u3002IBM \u4f7f\u7528\u6b64\u6570\u636e\u6765\u8ddf\u8e2a\u5c06\u6837\u672c\u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u5230 IBM Cloud \u76f8\u5173\u7684\u6307\u6807\uff0c\u4ee5\u5ea6\u91cf\u6211\u4eec\u7684\u793a\u4f8b\u7684\u5b9e\u7528\u6027\uff0c\u4ece\u800c\u8ba9\u6211\u4eec\u80fd\u591f\u6301\u7eed\u6539\u8fdb\u4e3a\u60a8\u63d0\u4f9b\u7684\u5185\u5bb9\u3002\u4ec5\u8ddf\u8e2a\u90a3\u4e9b\u5305\u542b\u4ee3\u7801\u4ee5\u5bf9 Deployment Tracker \u670d\u52a1\u6267\u884c ping \u64cd\u4f5c\u7684\u6837\u672c\u5e94\u7528\u7a0b\u5e8f\u7684\u90e8\u7f72\u8fc7\u7a0b\u3002 \u7981\u7528\u90e8\u7f72\u8ddf\u8e2a \u00b6 \u8bf7\u6ce8\u91ca\u6389/\u79fb\u9664 account-summary.yaml \u6587\u4ef6\u672b\u5c3e\u7684 Kubernetes Job \u90e8\u5206\u3002","title":"README cn"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#kubernetes-java-spring-boot","text":"\u9605\u8bfb\u672c\u6587\u7684\u5176\u4ed6\u8bed\u8a00\u7248\u672c\uff1a English \u3002 Spring Boot \u662f\u5e38\u7528 Java \u5fae\u670d\u52a1\u6846\u67b6\u4e4b\u4e00\u3002Spring Cloud \u62e5\u6709\u4e00\u7ec4\u4e30\u5bcc\u7684\u3001\u826f\u597d\u96c6\u6210\u7684 Java \u7c7b\u5e93\uff0c\u7528\u4e8e\u5e94\u5bf9 Java \u5e94\u7528\u7a0b\u5e8f\u5806\u6808\u4e2d\u53d1\u751f\u7684\u8fd0\u884c\u65f6\u95ee\u9898\uff1b\u800c Kubernetes \u5219\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u529f\u80fd\u96c6\u6765\u8fd0\u884c\u591a\u8bed\u8a00\u5fae\u670d\u52a1\u3002\u8fd9\u4e9b\u6280\u672f\u5f7c\u6b64\u4e92\u8865\uff0c\u4e3a Spring Boot \u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5e73\u53f0\u3002 \u5728\u6b64\u4ee3\u7801\u4e2d\uff0c\u6211\u4eec\u6f14\u793a\u4e86\u5982\u4f55\u5728 Kubernetes \u4e0a\u90e8\u7f72\u4e00\u4e2a\u7b80\u5355\u7684 Spring Boot \u5e94\u7528\u7a0b\u5e8f\u3002\u6b64\u5e94\u7528\u7a0b\u5e8f\u79f0\u4e3a Office Space\uff0c\u5b83\u6a21\u4eff\u4e86\u7535\u5f71 \u4e0a\u73ed\u4e00\u6761\u866b (Office Space) \u4e2d Michael Bolton \u7684\u865a\u6784\u5e94\u7528\u7a0b\u5e8f\u521b\u610f\u3002\u8be5\u5e94\u7528\u7a0b\u5e8f\u5229\u7528\u4e86\u8fd9\u6837\u4e00\u4e2a\u91d1\u878d\u65b9\u6848\uff1a\u901a\u5e38\u4e0d\u6ee1\u4e00\u5206\u94b1\u7684\u5206\u5e01\u4f1a\u56db\u820d\u4e94\u5165\uff0c\u800c\u6b64\u65b9\u6848\u5c06\u8fd9\u90e8\u5206\u5e01\u503c\u8f6c\u79fb\u5230\u4e00\u4e2a\u72ec\u7acb\u7684\u94f6\u884c\u8d26\u6237\u4e2d\u6765\u8ba1\u7b97\u4ea4\u6613\u5229\u606f\u3002 \u8be5\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528 Java 8/Spring Boot \u5fae\u670d\u52a1\u8ba1\u7b97\u5229\u606f\uff0c\u7136\u540e\u5c06\u5206\u5e01\u5b58\u5165\u6570\u636e\u5e93\u3002\u53e6\u4e00\u4e2a Spring Boot \u5fae\u670d\u52a1\u662f\u901a\u77e5\u670d\u52a1\u3002\u5f53\u8d26\u6237\u4f59\u989d\u8d85\u8fc7 50,000 \u7f8e\u5143\u65f6\uff0c\u5b83\u4f1a\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u3002\u5b83\u662f\u7531\u8ba1\u7b97\u5229\u606f\u7684 Spring Boot Web \u670d\u52a1\u5668\u89e6\u53d1\u7684\u3002\u524d\u7aef\u4f7f\u7528 Node.js \u5e94\u7528\u7a0b\u5e8f\u6765\u663e\u793a Spring Boot \u5e94\u7528\u7a0b\u5e8f\u7d2f\u79ef\u7684\u5f53\u524d\u8d26\u6237\u4f59\u989d\u3002\u540e\u7aef\u4f7f\u7528 MySQL \u6570\u636e\u5e93\u6765\u5b58\u50a8\u8d26\u6237\u4f59\u989d\u3002","title":"\u5728 Kubernetes \u4e0a\u6784\u5efa\u548c\u90e8\u7f72 Java Spring Boot \u5fae\u670d\u52a1"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#_1","text":"\u4f7f\u7528 Minikube \u521b\u5efa\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u7528\u4e8e\u672c\u5730\u6d4b\u8bd5\uff0c\u4f7f\u7528 IBM Cloud Private \u6216\u8005 IBM Cloud Container Service \u521b\u5efa\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u4ee5\u90e8\u7f72\u5230\u4e91\u4e2d\u3002\u672c\u6587\u4e2d\u7684\u4ee3\u7801\u4f7f\u7528 Travis \u5b9a\u671f\u4f7f\u7528 \u57fa\u4e8e IBM Cloud Container Service \u7684 Kubernetes \u96c6\u7fa4 \u8fdb\u884c\u6d4b\u8bd5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#_2","text":"1. \u521b\u5efa\u6570\u636e\u5e93\u670d\u52a1 1.1 \u5728\u5bb9\u5668\u4e2d\u4f7f\u7528 MySQL \u6216\u8005 1.2 \u4f7f\u7528 IBM Cloud MySQL\u670d\u52a1 2. \u521b\u5efa Spring Boot \u5fae\u670d\u52a1 2.1 \u4f7f\u7528 Maven \u6784\u5efa\u9879\u76ee 2.2 \u6784\u5efa\u548c\u63a8\u9001 Docker \u955c\u50cf 2.3 \u4e3a Spring Boot \u670d\u52a1\u4fee\u6539 yaml \u6587\u4ef6 2.3.1 \u5728\u901a\u77e5\u670d\u52a1\u4e2d\u4f7f\u7528\u9ed8\u8ba4\u7535\u5b50\u90ae\u4ef6\u670d\u52a1 \u6216\u8005 2.3.2 \u5728\u901a\u77e5\u670d\u52a1\u4e2d\u4f7f\u7528 OpenWhisk Actions 2.4 \u90e8\u7f72 Spring Boot \u5fae\u670d\u52a1 3. \u521b\u5efa\u524d\u7aef\u670d\u52a1 4. \u521b\u5efa\u4ea4\u6613\u751f\u6210\u5668\u670d\u52a1 5. \u8bbf\u95ee\u5e94\u7528\u7a0b\u5e8f","title":"\u6b65\u9aa4"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#_3","text":"","title":"\u6545\u969c\u6392\u9664"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#1","text":"\u540e\u7aef\u5305\u542b MySQL \u6570\u636e\u5e93\u548c Spring Boot \u5e94\u7528\u7a0b\u5e8f\u3002\u6bcf\u4e00\u9879 \u5fae\u670d\u52a1\u90fd\u5305\u542b\u4e00\u4e2a Kubernetes Deployment \u548c\u4e00\u4e2a Kubernetes Service\u3002Kubernetes Deployment \u7528\u4e8e\u7ba1\u7406\u6bcf\u4e00\u9879\u5fae\u670d\u52a1\u6240\u542f\u52a8\u7684 pod\u3002Kubernetes Service \u7528\u4e8e \u4e3a\u6bcf\u4e00\u9879\u5fae\u670d\u52a1\u521b\u5efa\u4e00\u4e2a\u7a33\u5b9a\u7684 DNS \u8bb0\u5f55\uff0c\u4ee5\u4fbf\u5b83\u4eec\u53ef\u4ee5 \u6839\u636e\u57df\u540d\u76f8\u4e92\u5f15\u7528\u3002 \u521b\u5efa MySQL \u6570\u636e\u5e93\u540e\u7aef\u7684\u65b9\u6cd5\u6709\u4e24\u79cd\uff1a \u5728\u5bb9\u5668\u4e2d\u4f7f\u7528 MySQL \u6216\u8005 \u4f7f\u7528 IBM Cloud MySQL \u670d\u52a1","title":"1.\u521b\u5efa\u6570\u636e\u5e93\u670d\u52a1"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#11-mysql","text":"$ kubectl create -f account-database.yaml service \"account-database\" created deployment \"account-database\" created \u9ed8\u8ba4\u8ba4\u8bc1\u4fe1\u606f\u5df2\u4f7f\u7528 base64 \u5728 secrets.yaml \u4e2d\u8fdb\u884c\u4e86\u7f16\u7801\u3002 base64 \u7f16\u7801\u4e0d\u4f1a\u52a0\u5bc6\u6216\u9690\u85cf\u60a8\u7684\u5bc6\u94a5\u3002\u8bf7\u52ff\u5c06\u5176\u4e0a\u4f20\u81f3\u60a8\u7684 Github \u4ed3\u5e93\u4e2d\u3002 $ kubectl apply -f secrets.yaml secret \"demo-credentials\" created \u4e0b\u4e00\u6b65\u8bf7\u53c2\u8003 \u6b65\u9aa4 2 \u3002","title":"1.1 \u5728\u5bb9\u5668\u4e2d\u4f7f\u7528 MySQL"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#12-ibm-cloud-mysql","text":"\u901a\u8fc7 https://console.ng.bluemix.net/catalog/services/compose-for-mysql \u5728 IBM Cloud \u4e2d\u4e3a MySQL \u63d0\u4f9b Provision Compose \u8f6c\u81f3 Service credentials \u5e76\u67e5\u770b\u60a8\u7684\u51ed\u8bc1\u3002\u5305\u62ec MySQL \u4e3b\u673a\u540d\u3001\u7aef\u53e3\u3001\u7528\u6237\u540d\u548c\u5bc6\u7801\u7b49\u4fe1\u606f\u4f4d\u4e8e\u51ed\u8bc1 URI \u4e0b\uff0c\u5982\u4e0b\u6240\u793a\uff1a \u60a8\u5c06\u9700\u8981\u5e94\u7528\u8fd9\u4e9b\u51ed\u8bc1\u4f5c\u4e3a Kubernetes \u96c6\u7fa4\u4e2d\u7684\u5bc6\u94a5\u3002\u8fd9\u4e9b\u4fe1\u606f\u5e94\u5df2\u88ab base64 \u7f16\u7801\u3002 \u4f7f\u7528\u811a\u672c ./scripts/create-secrets.sh \u3002\u7cfb\u7edf\u5c06\u63d0\u793a\u60a8\u8f93\u5165\u81ea\u5df1\u7684\u51ed\u8bc1\u3002\u8fd9\u5c06\u5bf9\u60a8\u8f93\u5165\u7684\u51ed\u8bc1\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u521b\u5efa Kubenetes Secret \u5bf9\u8c61\u3002 $ ./scripts/create-secrets.sh Enter MySQL username: admin Enter MySQL password: password Enter MySQL host: hostname Enter MySQL port: 23966 secret \"demo-credentials\" created \u60a8\u4e5f\u53ef\u4ee5\u7f16\u8f91 secrets.yaml \u6587\u4ef6\uff0c\u5c06\u5176\u4e2d\u7684\u6570\u636e\u503c\u7f16\u8f91\u4e3a\u81ea\u5df1\u7684 base64 \u7f16\u7801\u7684\u51ed\u8bc1\u3002\u7136\u540e\u6267\u884c kubectl apply -f secrets.yaml \u3002 \u4e0b\u4e00\u6b65\u8bf7\u53c2\u8003 \u6b65\u9aa4 2 \u3002","title":"1.2 \u4f7f\u7528 IBM Cloud MySQL \u670d\u52a1"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#2-spring-boot","text":"\u60a8\u9700\u8981 \u5b89\u88c5 Maven \u5de5\u5177\u3002 \u5982\u679c\u8981\u4fee\u6539 Spring Boot \u5e94\u7528\u7a0b\u5e8f\uff0c\u8bf7\u5728\u6784\u5efa Java \u9879\u76ee\u548c Docker \u955c\u50cf\u4e4b\u524d\u5b8c\u6210\u4fee\u6539\u3002 Spring Boot \u5fae\u670d\u52a1\u5305\u62ec Compute-Interest-API \u548c Send-Notification \u3002 Compute-Interest-API \u662f\u4e00\u4e2a\u9700\u8981\u4f7f\u7528 MySQL \u6570\u636e\u5e93\u7684 Spring Boot \u5e94\u7528\u7a0b\u5e8f\u3002\u76f8\u5173\u914d\u7f6e\u4f4d\u4e8e spring.datasource* \u4e2d\u7684 application.properties \u4e2d\u3002 compute-interest-api/src/main/resources/application.properties spring.datasource.url = jdbc:mysql://${MYSQL_DB_HOST}:${MYSQL_DB_PORT}/dockercon2017 # Username and password spring.datasource.username = ${MYSQL_DB_USER} spring.datasource.password = ${MYSQL_DB_PASSWORD} application.properties \u914d\u7f6e\u4e3a\u4f7f\u7528 MYSQL_DB_* \u73af\u5883\u53d8\u91cf\u3002\u8fd9\u4e9b\u53d8\u91cf\u5728 compute-interest-api.yaml \u6587\u4ef6\u4e2d\u5b9a\u4e49\u3002 compute-interest-api.yaml spec : containers : - image : anthonyamanse/compute-interest-api:secrets imagePullPolicy : Always name : compute-interest-api env : - name : MYSQL_DB_USER valueFrom : secretKeyRef : name : demo-credentials key : username - name : MYSQL_DB_PASSWORD valueFrom : secretKeyRef : name : demo-credentials key : password - name : MYSQL_DB_HOST valueFrom : secretKeyRef : name : demo-credentials key : host - name : MYSQL_DB_PORT valueFrom : secretKeyRef : name : demo-credentials key : port ports : - containerPort : 8080 YAML \u6587\u4ef6\u5df2\u914d\u7f6e\u4e3a\u4ece\u5148\u524d\u521b\u5efa\u7684 Kubernetes Secret \u4e2d\u83b7\u53d6\u503c\u3002\u8fd9\u4e9b\u4fe1\u606f\u5c06\u6700\u7ec8\u5199\u5165 application.properties \u5e76\u6700\u7ec8\u4e3a Spring Boot \u5e94\u7528\u7a0b\u5e8f\u6240\u7528\u3002 Send-Notification \u53ef\u914d\u7f6e\u4e3a\u901a\u8fc7 Gmail \u548c/\u6216 Slack \u53d1\u9001\u901a\u77e5\u3002\u901a\u77e5\u4ec5\u5728 MySQL \u6570\u636e\u5e93\u4e2d\u7684\u8d26\u6237\u4f59\u989d\u8d85\u8fc7 50,000 \u7f8e\u5143\u65f6\u63a8\u9001\u4e00\u6b21\u3002\u9ed8\u8ba4\u8bbe\u7f6e\u4e3a\u4f7f\u7528 Gmail \u3002\u901a\u77e5\u3002\u60a8\u8fd8\u53ef\u4ee5\u4f7f\u7528\u4e8b\u4ef6\u9a71\u52a8\u6280\u672f\uff08\u5728\u672c\u4f8b\u4e2d\u4e3a OpenWhisk \uff09 \u6765\u53d1\u9001\u7535\u5b50\u90ae\u4ef6\u548c Slack \u6d88\u606f\u3002\u8981\u5c06 OpenWhisk \u4e0e\u60a8\u7684\u901a\u77e5\u5fae\u670d\u52a1\u914d\u5408\u4f7f\u7528\uff0c\u8bf7\u5728\u6784\u5efa\u548c\u90e8\u7f72\u5fae\u670d\u52a1\u6620\u50cf\u4e4b\u524d\u9075\u5faa \u6b64\u5904 \u7684\u6b65\u9aa4\u8fdb\u884c\u64cd\u4f5c\u3002\u5426\u5219\uff0c\u53ea\u6709\u5728\u9009\u62e9\u4ec5\u4f7f\u7528\u7535\u5b50\u90ae\u4ef6\u901a\u77e5\u540e\u624d\u80fd\u7ee7\u7eed\u3002","title":"2.\u521b\u5efa Spring Boot \u5fae\u670d\u52a1"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#21-maven","text":"\u5f53 Maven \u6210\u529f\u6784\u5efa Java \u9879\u76ee\u540e\uff0c\u60a8\u9700\u8981\u4f7f\u7528\u5728\u5176\u76f8\u5e94\u6587\u4ef6\u5939\u4e2d\u63d0\u4f9b\u7684 Dockerfile \u6784\u5efa Docker \u955c\u50cf\u3002 \u5907\u6ce8\uff1acompute-interest-api \u4f1a\u5c06\u5206\u5e01\u4e58\u4ee5 100,000\uff0c\u7528\u4e8e\u6267\u884c\u6a21\u62df\u3002\u60a8\u53ef\u4ee5\u7f16\u8f91/\u79fb\u9664 src/main/java/officespace/controller/MainController.java \u4e2d\u7684 remainingInterest *= 100000 \u884c\u3002\u5f53\u4f59\u989d\u8d85\u8fc7 50,000 \u7f8e\u5143\u65f6\uff0c\u7a0b\u5e8f\u8fd8\u4f1a\u53d1\u9001\u901a\u77e5\uff0c \u60a8\u53ef\u4ee5\u7f16\u8f91 if (updatedBalance > 50000 && emailSent == false ) \u884c\u4e2d\u7684\u6570\u5b57\u3002\u4fdd\u5b58\u66f4\u6539\u540e\uff0c\u5c31\u53ef\u4ee5\u6784\u5efa\u9879\u76ee\u4e86\u3002 Go to containers/compute-interest-api $ mvn package Go to containers/send-notification $ mvn package \u6211\u4eec\u5c06\u4f7f\u7528 IBM Cloud \u5bb9\u5668\u955c\u50cf\u4ed3\u5e93\u6765\u4fdd\u5b58\u955c\u50cf\uff08\u7531\u6b64\u8fdb\u884c\u6620\u50cf\u547d\u540d\uff09\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528 Docker Hub \u4fdd\u5b58\u955c\u50cf\u3002","title":"2.1.\u4f7f\u7528 Maven \u6784\u5efa\u9879\u76ee"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#22-spring-boot-docker","text":"\u5907\u6ce8\uff1a\u672c\u6587\u4f7f\u7528 IBM Cloud \u5bb9\u5668\u955c\u50cf\u5e93\u4e2d\u4fdd\u5b58\u955c\u50cf\u3002 \u5982\u679c\u60a8\u8ba1\u5212\u4f7f\u7528 IBM Cloud \u5bb9\u5668\u955c\u50cf\u5e93\uff0c\u9700\u8981\u9996\u5148\u8bbe\u7f6e\u5e10\u6237\u3002\u8bf7\u9075\u5faa \u6b64\u5904 \u7684\u6559\u7a0b\u8fdb\u884c\u64cd\u4f5c\u3002 \u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528 Docker Hub \u4fdd\u5b58\u955c\u50cf\u3002 $ docker build -t registry.ng.bluemix.net/<namespace>/compute-interest-api . $ docker build -t registry.ng.bluemix.net/<namespace>/send-notification . $ docker push registry.ng.bluemix.net/<namespace>/compute-interest-api $ docker push registry.ng.bluemix.net/<namespace>/send-notification","title":"2.2 \u4e3a Spring Boot \u670d\u52a1\u6784\u5efa Docker \u6620\u50cf"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#23-compute-interest-apiyaml-send-notificationyaml","text":"\u6210\u529f\u63a8\u9001\u955c\u50cf\u540e\uff0c\u60a8\u5c06\u9700\u8981\u4fee\u6539 yaml \u6587\u4ef6\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u955c\u50cf\u3002 // compute-interest-api.yaml spec : containers : - image : registry.ng.bluemix.net/<namespace>/compute-interest-api # replace with your image name // send-notification.yaml spec : containers : - image : registry.ng.bluemix.net/<namespace>/send-notification # replace with your image name \u5b58\u5728\u4e24\u79cd\u53ef\u80fd\u7684\u901a\u77e5\u65b9\u5f0f\uff0c\u8bf7\u53c2\u89c1\uff1a 2.3.1 \u4f7f\u7528\u9ed8\u8ba4\u7535\u5b50\u90ae\u4ef6\u670d\u52a1 \u6216 2.3.2 \u4f7f\u7528 OpenWhisk Actions \u3002","title":"2.3 \u4e3a\u4f7f\u7528\u60a8\u6240\u6784\u5efa\u7684\u955c\u50cf\uff0c\u9700\u8981\u4fee\u6539 compute-interest-api.yaml \u548c send-notification.yaml \u6587\u4ef6"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#231-gmail","text":"\u60a8\u5c06\u9700\u8981\u4fee\u6539 send-notification.yaml \u4e2d\u7684 \u73af\u5883\u53d8\u91cf \uff1a env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # change this to the gmail that will send the email - name : GMAIL_SENDER_PASSWORD value : 'password' # change this to the the password of the gmail above - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # change this to the email of the receiver \u73b0\u5728\uff0c\u60a8\u53ef\u4ee5\u7ee7\u7eed\u6267\u884c \u6b65\u9aa4 2.4 \u3002","title":"2.3.1 \u4f7f\u7528\u9ed8\u8ba4\u7535\u5b50\u90ae\u4ef6\u670d\u52a1 (Gmail) \u6765\u5904\u7406\u901a\u77e5\u670d\u52a1"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#232-openwhisk-action","text":"\u672c\u90e8\u5206\u7684\u8981\u6c42\uff1a * \u60a8\u7684 Slack \u56e2\u961f\u4e2d\u5177\u6709 Slack Incoming Webhook \u3002 * IBM Cloud\u5e10\u6237 \uff0c\u4ee5\u4fbf\u4f7f\u7528 OpenWhisk CLI \u3002","title":"2.3.2 \u4f7f\u7528 OpenWhisk Action \u6765\u5904\u7406\u901a\u77e5\u670d\u52a1"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#2321-actions","text":"\u672c\u4ee3\u7801\u5e93\u7684\u6839\u76ee\u5f55\u4e2d\u5305\u542b\u60a8\u521b\u5efa OpenWhisk Actions \u65f6\u6240\u9700\u7684\u4ee3\u7801\u3002 \u5982\u679c\u60a8\u5c1a\u672a\u5b89\u88c5 OpenWhisk CLI\uff0c\u8bf7\u8f6c\u81f3 \u6b64\u5904 \u3002 \u60a8\u53ef\u4ee5\u4f7f\u7528 wsk \u547d\u4ee4\u6765\u521b\u5efa OpenWhisk Actions\u3002\u521b\u5efa\u64cd\u4f5c\u4f7f\u7528\u4ee5\u4e0b\u8bed\u6cd5\uff1a wsk action create < action_name > < source code for action> [add --param for optional Default parameters] * \u521b\u5efa\u7528\u4e8e\u53d1\u9001 Slack \u901a\u77e5 \u7684 Action $ wsk action create sendSlackNotification sendSlack.js --param url https://hooks.slack.com/services/XXXX/YYYY/ZZZZ Replace the url with your Slack team ' s incoming webhook url. * \u521b\u5efa\u7528\u4e8e\u53d1\u9001 Gmail \u901a\u77e5 \u7684 Action $ wsk action create sendEmailNotification sendEmail.js","title":"2.3.2.1 \u521b\u5efa Actions"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#2322-actions","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 wsk action invoke [action name] [add --param to pass parameters] \u6d4b\u8bd5 OpenWhisk Actions * \u8c03\u7528 Slack \u901a\u77e5 $ wsk action invoke sendSlackNotification --param text \"Hello from OpenWhisk\" * \u8c03\u7528\u7535\u5b50\u90ae\u4ef6\u901a\u77e5 $ wsk action invoke sendEmailNotification --param sender [ sender 's email] --param password [sender' s password ] --param receiver [ receiver ' s email ] --param subject [ Email subject ] --param text [ Email Body ] \u81f3\u6b64\uff0c\u60a8\u5e94\u5206\u522b\u6536\u5230\u4e00\u6761 Slack \u6d88\u606f\u548c\u4e00\u5c01\u7535\u5b50\u90ae\u4ef6\u3002","title":"2.3.2.2 \u6d4b\u8bd5 Actions"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#2323-actions-rest-api","text":"\u60a8\u53ef\u4ee5\u4f7f\u7528 wsk api create \u4e3a\u521b\u5efa\u7684 Action \u6620\u5c04 REST API \u7aef\u70b9\u3002\u5176\u8bed\u6cd5\u4e3a wsk api create [base-path] [api-path] [verb (GET PUT POST etc)] [action name] * \u521b\u5efa\u7528\u4e8e Slack \u901a\u77e5 \u7684 REST API \u7aef\u70b9 $ wsk api create /v1 /slack POST sendSlackNotification ok: created API /v1/email POST for action /_/sendEmailNotification https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack * \u521b\u5efa\u7528\u4e8e Gmail \u901a\u77e5 \u7684 REST API \u7aef\u70b9 $ wsk api create /v1 /email POST sendEmailNotification ok: created API /v1/email POST for action /_/sendEmailNotification https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email \u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b API \u5217\u8868\uff1a $ wsk api list ok: APIs Action Verb API Name URL /Anthony.Amanse_dev/sendEmailNotificatio post /v1 https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email /Anthony.Amanse_dev/testDefault post /v1 https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack \u8bf7\u8bb0\u5f55 \u8fd9\u4e9b API URL\uff0c\u7a0d\u540e\u6211\u4eec\u5c06\u4f7f\u7528\u5b83\u4eec \u3002","title":"2.3.2.3 \u4e3a Actions \u521b\u5efa REST API"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#2324-rest-api-url","text":"\u6d4b\u8bd5\u7528\u4e8e Slack \u901a\u77e5 \u7684 REST API \u7aef\u70b9\u3002\u8fd9\u91cc\u8bf7\u4f7f\u7528\u60a8\u81ea\u5df1\u7684 API URL\u3002 $ curl -X POST -d '{ \"text\": \"Hello from OpenWhisk\" }' https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack \u6d4b\u8bd5\u7528\u4e8e Gmail \u901a\u77e5 \u7684 REST API \u7aef\u70b9\u3002\u8fd9\u91cc\u8bf7\u4f7f\u7528\u60a8\u81ea\u5df1\u7684 API URL\u3002\u5c06\u53c2\u6570 sender\u3001password\u3001receiver \u548c subject \u7684\u503c\u66ff\u6362\u4e3a\u60a8\u81ea\u5df1\u7684\u503c\u3002 $ curl -X POST -d '{ \"text\": \"Hello from OpenWhisk\", \"subject\": \"Email Notification\", \"sender\": \"testemail@gmail.com\", \"password\": \"passwordOfSender\", \"receiver\": \"receiversEmail\" }' https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email","title":"2.3.2.4 \u6d4b\u8bd5 REST API URL"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#2325-rest-api-url-yaml","text":"\u4e00\u65e6\u786e\u8ba4\u60a8\u7684 API \u8fd0\u884c\u6b63\u5e38\uff0c\u5c31\u53ef\u4ee5\u5c06\u8fd9\u4e9b URL \u653e\u5165 send-notification.yaml \u6587\u4ef6\u4e2d\u4e86 env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # the sender's email - name : GMAIL_SENDER_PASSWORD value : 'password' # the sender's password - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # the receiver's email - name : OPENWHISK_API_URL_SLACK value : 'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack' # your API endpoint for slack notifications - name : SLACK_MESSAGE value : 'Your balance is over $50,000.00' # your custom message - name : OPENWHISK_API_URL_EMAIL value : 'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email' # your API endpoint for email notifications","title":"2.3.2.5 \u5c06 REST API URL \u6dfb\u52a0\u5230 yaml \u6587\u4ef6\u4e2d"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#24-spring-boot","text":"$ kubectl create -f compute-interest-api.yaml service \"compute-interest-api\" created deployment \"compute-interest-api\" created $ kubectl create -f send-notification.yaml service \"send-notification\" created deployment \"send-notification\" created","title":"2.4 \u90e8\u7f72 Spring Boot \u5fae\u670d\u52a1"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#3","text":"\u6b64\u7528\u6237\u754c\u9762\u662f Node.js \u5e94\u7528\u7a0b\u5e8f\uff0c\u53ef\u663e\u793a\u8d26\u6237\u603b\u4f59\u989d\u3002 \u5982\u679c\u60a8\u5728 IBM Cloud \u4e2d\u4f7f\u7528 MySQL \u6570\u636e\u5e93\uff0c\u8bf7\u8bb0\u5f97\u586b\u5145 account-summary.yaml \u6587\u4ef6\u4e2d\u73af\u5883\u53d8\u91cf\u7684\u503c\uff0c\u5426\u5219\u8bf7\u5c06\u5176\u7559\u7a7a\u3002\u8fd9\u662f\u5728 \u6b65\u9aa4 1 \u4e2d\u6267\u884c\u7684\u64cd\u4f5c\u3002 \u521b\u5efa Node.js \u524d\u7aef\uff1a $ kubectl create -f account-summary.yaml service \"account-summary\" created deployment \"account-summary\" created","title":"3.\u521b\u5efa\u524d\u7aef\u670d\u52a1"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#4","text":"\u4ea4\u6613\u751f\u6210\u5668\u662f Python \u5e94\u7528\u7a0b\u5e8f\uff0c\u53ef\u4f7f\u7528\u7d2f\u79ef\u5229\u606f\u751f\u6210\u968f\u673a\u4ea4\u6613\u3002 * \u521b\u5efa\u4ea4\u6613\u751f\u6210\u5668 Python \u5e94\u7528\u7a0b\u5e8f\uff1a $ kubectl create -f transaction-generator.yaml service \"transaction-generator\" created deployment \"transaction-generator\" created","title":"4.\u521b\u5efa\u4ea4\u6613\u751f\u6210\u5668\u670d\u52a1"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#5","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7 Kubernetes Cluster IP \u548c NodePort \u8bbf\u95ee\u5e94\u7528\u7a0b\u5e8f\u3002NodePort \u5e94\u4e3a 30080 \u3002 \u8981\u67e5\u627e Cluster IP\uff0c\u8bf7\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a $ bx cs workers <cluster-name> ID Public IP Private IP Machine Type State Status kube-dal10-paac005a5fa6c44786b5dfb3ed8728548f-w1 169 .47.241.213 10 .177.155.13 free normal Ready \u8981\u67e5\u627e\u8d26\u6237\u6458\u8981 (account-summary) \u670d\u52a1\u7684 NodePort\uff0c\u8bf7\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ... account-summary 10 .10.10.74 <nodes> 80 :30080/TCP 2d ... \u5728\u60a8\u7684\u6d4f\u89c8\u5668\u4e0a\uff0c\u8f6c\u81f3 http://<your-cluster-IP>:30080","title":"5.\u8bbf\u95ee\u5e94\u7528\u7a0b\u5e8f"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#_4","text":"\u8981\u4ece\u5934\u5f00\u59cb\uff0c\u8bf7\u5220\u9664\u6240\u6709\u5185\u5bb9\uff1a kubectl delete svc,deploy -l app=office-space","title":"\u6545\u969c\u6392\u9664"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#_5","text":"John Zaccone - \u901a\u8fc7 Docker \u90e8\u7f72\u7684 Office Space \u5e94\u7528\u7a0b\u5e8f \u7684\u539f\u59cb\u4f5c\u8005\u3002 Office Space \u5e94\u7528\u7a0b\u5e8f\u662f\u4ee5 1999 \u5e74\u8fd0\u7528\u6b64\u7406\u5ff5\u7684\u540c\u540d\u7535\u5f71\u4e3a\u57fa\u7840\u7f16\u5199\u7684\u3002","title":"\u53c2\u8003\u8d44\u6599"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#_6","text":"Apache 2.0","title":"\u8bb8\u53ef"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#_7","text":"\u53ef\u4ee5\u914d\u7f6e\u5305\u542b\u8fd9\u4e2a\u5305\u7684\u6837\u672c Kubernetes Yaml \u6587\u4ef6\uff0c\u4ee5\u8ddf\u8e2a\u5bf9 IBM Cloud \u548c\u5176\u4ed6 Kubernetes \u5e73\u53f0\u7684\u90e8\u7f72\u3002\u6bcf\u6b21\u90e8\u7f72\u65f6\uff0c\u90fd\u4f1a\u5c06\u4ee5\u4e0b\u4fe1\u606f\u53d1\u9001\u5230 Deployment Tracker \u670d\u52a1\uff1a Kubernetes \u96c6\u7fa4\u63d0\u4f9b\u8005\uff08 IBM Cloud\u3001Minikube \u7b49 \uff09 Kubernetes \u673a\u5668 ID ( MachineID ) \u8fd9\u4e2a Kubernetes \u4f5c\u4e1a\u4e2d\u7684\u73af\u5883\u53d8\u91cf\u3002 \u6b64\u6570\u636e\u662f\u4ece\u6837\u672c\u5e94\u7528\u7a0b\u5e8f\u7684 yaml \u6587\u4ef6\u4e2d\u7684 Kubernetes Job \u6536\u96c6\u800c\u6765\u3002IBM \u4f7f\u7528\u6b64\u6570\u636e\u6765\u8ddf\u8e2a\u5c06\u6837\u672c\u5e94\u7528\u7a0b\u5e8f\u90e8\u7f72\u5230 IBM Cloud \u76f8\u5173\u7684\u6307\u6807\uff0c\u4ee5\u5ea6\u91cf\u6211\u4eec\u7684\u793a\u4f8b\u7684\u5b9e\u7528\u6027\uff0c\u4ece\u800c\u8ba9\u6211\u4eec\u80fd\u591f\u6301\u7eed\u6539\u8fdb\u4e3a\u60a8\u63d0\u4f9b\u7684\u5185\u5bb9\u3002\u4ec5\u8ddf\u8e2a\u90a3\u4e9b\u5305\u542b\u4ee3\u7801\u4ee5\u5bf9 Deployment Tracker \u670d\u52a1\u6267\u884c ping \u64cd\u4f5c\u7684\u6837\u672c\u5e94\u7528\u7a0b\u5e8f\u7684\u90e8\u7f72\u8fc7\u7a0b\u3002","title":"\u9690\u79c1\u58f0\u660e"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-cn/#_8","text":"\u8bf7\u6ce8\u91ca\u6389/\u79fb\u9664 account-summary.yaml \u6587\u4ef6\u672b\u5c3e\u7684 Kubernetes Job \u90e8\u5206\u3002","title":"\u7981\u7528\u90e8\u7f72\u8ddf\u8e2a"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/","text":"\ucfe0\ubc84\ub124\ud2f0\uc2a4\uc5d0 \uc790\ubc14 Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \ube4c\ub4dc \ubc0f \ubc30\ud3ec\ud558\uae30 \u00b6 Read this in other languages: English . Spring Boot\uc740 \ub110\ub9ac \uc0ac\uc6a9\ub418\ub294 \uc790\ubc14 \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \ud504\ub808\uc784\uc6cc\ud06c \uc785\ub2c8\ub2e4. Spring Cloud\ub294 Java \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc2a4\ud0dd\uc758 \uc77c\ubd80\ub85c \ub7f0\ud0c0\uc784 \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\ub294 \ud48d\ubd80\ud55c \ud1b5\ud569 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uac16\uace0 \uc788\uc73c\uba70 \ucfe0\ubc84\ub124\ud2f0\uc2a4\ub294 polyglot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\ub97c \uc2e4\ud589\ud560 \uc218 \uc788\ub294 \ud48d\ubd80\ud55c \uae30\ub2a5\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774 \ub450 \uae30\uc220\uc740 \uc11c\ub85c\ub97c \ubcf4\uc644\ud558\uace0 \uc788\uc73c\uba70 Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud6cc\ub96d\ud55c \ud50c\ub7ab\ud3fc\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. \uc774 \ucf54\ub4dc\uc5d0\uc11c\ub294 \ucfe0\ubc84\ub124\ud2f0\uc2a4 \uc704\uc5d0 \uac04\ub2e8\ud55c Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \ubc30\ud3ec\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc778 Office Space\ub294 \uc601\ud654 Office Space \uc5d0 \ub4f1\uc7a5\ud558\ub294 Michael Bolton\uc758 \uac00\uc0c1\uc758 \uc571 \uc544\uc774\ub514\uc5b4\ub97c \ubaa8\ubc29\ud55c \uac83\uc785\ub2c8\ub2e4. \uc774 \uc571\uc740 \uc77c\ubc18\uc801\uc73c\ub85c 1\uc13c\ud2b8 \uc774\ud558\uc758 \uae08\uc561\uc744 \ubc18\uc62c\ub9bc \ud558\ub294 \ubc29\uc2dd \ub300\uc2e0\uc5d0 \ubc84\ub9bc\ud558\uc5ec \ubcc4\ub3c4\uc758 \uc740\ud589 \uacc4\uc88c\uc5d0 \uc800\uc7a5\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc774\ub4dd\uc744 \ucde8\ud569\ub2c8\ub2e4. \uc774 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 Java 8/Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc774\uc790\ub97c \uacc4\uc0b0\ud558\uace0 \uc18c\uc22b\uc810 \uc774\ud558\uc758 \uae08\uc561\uc740 \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0 \uc800\uc7a5\ud569\ub2c8\ub2e4. \ub610 \ub2e4\ub978 Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\uc778 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub294 \uacc4\uc88c\uc758 \uc794\uc561\uc774 $50,000 \uc774\uc0c1\uc774 \ub418\uba74 \uc774\uba54\uc77c\uc744 \uc804\uc1a1\ud569\ub2c8\ub2e4. \uc774\ub294 \uc774\uc790\ub97c \uacc4\uc0b0\ud558\ub294 Spring Boot \uc6f9\uc11c\ubc84\uc5d0 \uc758\ud574 \ud2b8\ub9ac\uac70 \ub429\ub2c8\ub2e4. \uc571\uc758 Frontend\ub294 Node.js \uae30\ubc18\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc84c\uace0 Spring Boot \uc571\uc5d0 \uc758\ud574 \uc313\uc778 \ud604\uc7ac \uacc4\uc88c\uc758 \uc794\uc561\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Backend\ub294 MySQL \ub370\uc774\ud130 \ubca0\uc774\uc2a4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uacc4\uc88c \uc794\uc561\uc744 \uc800\uc7a5\ud569\ub2c8\ub2e4. \uc0ac\uc804 \uc900\ube44 \uc0ac\ud56d \u00b6 \ucfe0\ubc84\ub124\ud2f0\uc2a4 \ud074\ub7ec\uc2a4\ud130\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub85c\uceec\uc5d0\uc11c \ud14c\uc2a4\ud2b8 \ud558\ub824\uba74 Minikube \uc5d0\uc11c, \ud074\ub77c\uc6b0\ub4dc\uc5d0 \ubc30\ud3ec\ud558\ub824\uba74 IBM Bluemix Container Service \uc5d0\uc11c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774 \ucf54\ub4dc\ub294 Travis\ub97c \uc0ac\uc6a9\ud558\uc5ec Kubernetes Cluster from Bluemix Container Service \uc5d0\uc11c \uc815\uae30\uc801\uc73c\ub85c \ud14c\uc2a4\ud2b8 \ud569\ub2c8\ub2e4. Steps \u00b6 \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc11c\ube44\uc2a4 \uc0dd\uc131 1.1 \ucee8\ud14c\uc774\ub108\uc5d0\uc11c MySQL \uc0ac\uc6a9 \ub610\ub294 1.2 Bluemix MySQL \uc0ac\uc6a9 Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \uc0dd\uc131 2.1 Maven\uc73c\ub85c \ud504\ub85c\uc81d\ud2b8 \ube4c\ub4dc 2.2 \ub2e5\ucee4 \uc774\ubbf8\uc9c0 \ube4c\ub4dc \ubc0f \ud478\uc2dc 2.3 Spring Boot \uc11c\ube44\uc2a4\ub97c \uc704\ud55c yaml \ud30c\uc77c \uc218\uc815 2.3.1 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub85c \uae30\ubcf8 \uc774\uba54\uc77c \uc11c\ube44\uc2a4 \uc0ac\uc6a9 \ub610\ub294 2.3.2 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub85c OpenWhisk Actions \uc0ac\uc6a9 2.4 Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \ubc30\ud3ec Frontend \uc11c\ube44\uc2a4 \uc791\uc131 \ud2b8\ub79c\uc7ad\uc158 \uc0dd\uc131 \uc11c\ube44\uc2a4 \uc791\uc131 \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc811\uadfc \ubb38\uc81c \ud574\uacb0 \u00b6 1. \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc11c\ube44\uc2a4 \uc0dd\uc131 \u00b6 \ubc31\uc5d4\ub4dc \uc2dc\uc2a4\ud15c\uc740 MySQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc640 Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\ub294 Deployment\uc640 Service\ub97c \uac16\uace0 \uc788\uc2b5\ub2c8\ub2e4. Deployment\ub294 \uac01 \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\uc5d0 \ub300\ud574 \uc2dc\uc791\ub41c Pod\ub97c \uad00\ub9ac\ud569\ub2c8\ub2e4. \uc11c\ube44\uc2a4\ub294 \uac01 \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\uc5d0 \ub300\ud574 \uc774\ub984\uc73c\ub85c dependency\ub97c \ucc38\uc870\ud558\ub3c4\ub85d \uc548\uc815\uc801\uc778 DNS\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. MySQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\ub97c \uc0dd\uc131\ud558\ub294 \ubc29\ubc95\uc740 \ub450\uac00\uc9c0\uac00 \uc788\uc2b5\ub2c8\ub2e4.: \ucee8\ud14c\uc774\ub108\uc5d0\uc11c MySQL \uc0ac\uc6a9 \ub610\ub294 Bluemix MySQL \uc0ac\uc6a9 1.1 \ucee8\ud14c\uc774\ub108\uc5d0\uc11c MySQL \uc0ac\uc6a9 \u00b6 $ kubectl create -f account-database.yaml service \"account-database\" created deployment \"account-database\" created \uae30\ubcf8 \uc2e0\uc784 \uc815\ubcf4\ub294 \uc774\ubbf8 secrets.yaml\uc5d0 base64\ub85c \uc778\ucf54\ub529 \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. base64 \uc778\ucf54\ub529\uc740 \uc2e0\uc784 \uc815\ubcf4\ub97c \uc554\ud638\ud654\ud558\uc9c0\ub294 \uc54a\uc73c\ubbc0\ub85c \uc774 \uc815\ubcf4\ub97c Github\uc5d0 \uc5c5\ub85c\ub4dc \ud558\uc9c0 \ub9c8\uc2ed\uc2dc\uc624. $ kubectl apply -f secrets.yaml secret \"demo-credentials\" created Step 2 \uc5d0\uc11c \uacc4\uc18d \uc9c4\ud589 \ud558\uc2ed\uc2dc\uc624. 1.2 Bluemix MySQL \uc0ac\uc6a9 \u00b6 https://console.ng.bluemix.net/catalog/services/compose-for-mysql\ub97c \ud1b5\ud574 Bluemix\uc5d0\uc11c Compose for MySQL\uc744 \uc791\uc131\ud558\uc2ed\uc2dc\uc624. Service Credentials\ub85c \uac00\uc11c \uc2e0\uc784 \uc815\ubcf4\ub97c \ud655\uc778\ud558\uc2ed\uc2dc\uc624. MySQL\uc758 Hostname, port, user \uadf8\ub9ac\uace0 password\uac00 \uc2e0\uc784 \uc815\ubcf4\uc758 url\uc5d0 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c\uc758 \ud615\uc2dd\uc73c\ub85c \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uc2e0\uc784\uc815\ubcf4\ub97c \ud050\ubc84\ub124\ud2f0\uc2a4 \ud074\ub7ec\uc2a4\ud130\uc5d0 Secret\uc73c\ub85c \uc801\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774 \uac12\uc740 \ubc18\ub4dc\uc2dc base64 \ub85c \uc778\ucf54\ub529 \ub418\uc5b4 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. ./scripts/create-secrets.sh \ub97c \uc0ac\uc6a9\ud558\uc2ed\uc2dc\uc624. \uc2e0\uc784 \uc815\ubcf4\ub97c \ub123\ub3c4\ub85d \uc785\ub825\uc744 \ubc1b\uc744 \uac83\uc785\ub2c8\ub2e4. \uc774 \uc2a4\ud06c\ub9bd\ud2b8\ub97c \ud1b5\ud574 \uc2e0\uc784 \uc815\ubcf4\ub97c \uc778\ucf54\ub529 \ud558\uace0 \ud074\ub7ec\uc2a4\ud130\uc5d0 Secret\uc73c\ub85c \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. $ ./scripts/create-secrets.sh Enter MySQL username: admin Enter MySQL password: password Enter MySQL host: hostname Enter MySQL port: 23966 secret \"demo-credentials\" created \ub610\ud55c secrets.yaml \ud30c\uc77c\uc744 \uc218\uc815\ud558\uc5ec base64 \uc778\ucf54\ub529\ub41c \uc2e0\uc784\uc815\ubcf4\ub97c \uc9c1\uc811 \ub123\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uacbd\uc6b0\uc5d0\ub294 kubectl apply -f secrets.yaml \ub97c \uc218\ud589\ud558\uc2ed\uc2dc\uc624. Step 2 \uc5d0\uc11c \uacc4\uc18d \uc9c4\ud589 \ud558\uc2ed\uc2dc\uc624. 2. Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \uc0dd\uc131 \u00b6 Maven\uc774 \uc124\uce58\ub418\uc5b4 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. . Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158\ub4e4\uc744 \uc218\uc815\ud558\ub824\uba74 Java \ud504\ub85c\uc81d\ud2b8\uc640 \ub2e5\ucee4 \uc774\ubbf8\uc9c0\ub97c \ube4c\ub4dc\ud558\uae30 \uc804\uc5d0 \ud574\uc57c \ud569\ub2c8\ub2e4. Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\ub294 Compute-Interest-API \uc640 Send-Notification \uc785\ub2c8\ub2e4. **Compute-Interest-API**\uc740 MySQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\ub97c \uc0ac\uc6a9\ud558\uae30\uc704\ud574 \uad6c\uc131\ub41c Spring Boot \uc571\uc785\ub2c8\ub2e4. \uad00\ub828\ud55c \uad6c\uc131\uc740 spring.datasource.* \uc5d0 \uc788\ub294 application.properties\uc5d0 \uc704\uce58\ud574 \uc788\uc2b5\ub2c8\ub2e4. compute-interest-api/src/main/resources/application.properties spring.datasource.url = jdbc:mysql://${MYSQL_DB_HOST}:${MYSQL_DB_PORT}/dockercon2017 # Username\uacfc password spring.datasource.username = ${MYSQL_DB_USER} spring.datasource.password = ${MYSQL_DB_PASSWORD} application.properties \ub294 MYSQL_DB_* \ud658\uacbd \ubcc0\uc218\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 \uad6c\uc131\ub410\uc2b5\ub2c8\ub2e4. \uc774\ub294 compute-interest-api.yaml \ud30c\uc77c\uc5d0 \uc815\uc758\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. compute-interest-api.yaml spec : containers : - image : anthonyamanse/compute-interest-api:secrets imagePullPolicy : Always name : compute-interest-api env : - name : MYSQL_DB_USER valueFrom : secretKeyRef : name : demo-credentials key : username - name : MYSQL_DB_PASSWORD valueFrom : secretKeyRef : name : demo-credentials key : password - name : MYSQL_DB_HOST valueFrom : secretKeyRef : name : demo-credentials key : host - name : MYSQL_DB_PORT valueFrom : secretKeyRef : name : demo-credentials key : port ports : - containerPort : 8080 \uc774 YAML \ud30c\uc77c\uc740 \uc774\ubbf8 \uc774\uc804 \ub2e8\uacc4\uc5d0\uc11c \uc0dd\uc131\ud55c \ucfe0\ubc84\ub124\ud2f0\uc2a4 Secret\uc73c\ub85c\ubd80\ud130 \uac12\uc744 \uc5bb\ub3c4\ub85d \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 application.properties \uc5d0 \uc788\ub294 Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0\uc11c \uc0ac\uc6a9\ub420 \uac83\uc785\ub2c8\ub2e4. Send-Notification \uc740 gmail \uadf8\ub9ac\uace0/\ub610\ub294 Slack\uc744 \ud1b5\ud574 \uc54c\ub9bc\uc744 \uc904 \uc218 \uc788\ub3c4\ub85d \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc54c\ub9bc\uc740 MySQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc758 \uacc4\uc88c \uc794\uc561\uc774 $50,000\uc744 \ub118\uc5c8\uc744 \ub54c\uc5d0 \ud55c\ubc88\ub9cc \ubc1c\uc1a1\ub429\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c\ub294 gmail \uc635\uc158\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. Event driven \uae30\uc220\uc744 \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 OpenWhisk \ub97c \uc0ac\uc6a9\ud558\uc5ec \uc774\uba54\uc77c\uacfc \uc2ac\ub799 \uba54\uc138\uc9c0\ub97c \ubcf4\ub0c5\ub2c8\ub2e4. \uc54c\ub9bc \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\ub85c OpenWhisk\ub97c \uc0ac\uc6a9\ud558\ub824\uba74 \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \uc774\ubbf8\uc9c0\ub97c \ube4c\ub4dc \ubc0f \ubc30\ud3ec \ud558\uae30 \uc804\uc5d0 \uc5ec\uae30 \uc2a4\ud15d\uc744 \ub530\ub974\uc2ed\uc2dc\uc624. \uc774\uba54\uc77c \uc54c\ub9bc \uc124\uc815\ub9cc \uc0ac\uc6a9\ud558\ub824\uba74 \uadf8\ub0e5 \uc9c4\ud589\ud558\uc2ed\uc2dc\uc624. 2.1. Maven\uc73c\ub85c \ud504\ub85c\uc81d\ud2b8 \ube4c\ub4dc \u00b6 Maven\uc774 Java \ud504\ub85c\uc81d\ud2b8\ub97c \uc131\uacf5\uc801\uc73c\ub85c \ube4c\ub4dc\ud558\uba74, \uac01 \ud3f4\ub354\uc5d0 \uc81c\uacf5\ub41c **Dockerfile**\ub85c \ub2e5\ucee4 \uc774\ubbf8\uc9c0\ub97c \ube4c\ub4dc\ud574\uc57c \ud569\ub2c8\ub2e4. Note: compute-interest-api\ub294 \uc2dc\ubbac\ub808\uc774\uc158 \ubaa9\uc801\uc73c\ub85c \ub098\uba38\uc9c0 \uae08\uc561\uc5d0 x100,000\uc744 \uacf1\ud569\ub2c8\ub2e4. src/main/java/officespace/controller/MainController.java \uc5d0\uc11c remainingInterest *= 100000 \ub77c\uc778\uc744 \uc218\uc815/\uc81c\uac70\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c \uc794\uc561\uc774 $50,000\uc744 \ub118\uae38 \ub54c \uc54c\ub9bc\uc744 \uc804\uc1a1\ud558\ub3c4\ub85d \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. if (updatedBalance > 50000 && emailSent == false ) \ub77c\uc778\uc5d0\uc11c \uc774 \uc22b\uc790\ub97c \ubcc0\uacbd\ud558\uc5ec \uc0ac\uc6a9\ud558\uc2ed\uc2dc\uc624. \ubcc0\uacbd \uc0ac\ud56d\uc744 \uc800\uc7a5\ud558\uba74 \uc774\uc81c \ud504\ub85c\uc81d\ud2b8\ub97c \ube4c\ub4dc\ud558\uc2ed\uc2dc\uc624. Go to containers/compute-interest-api $ mvn package Go to containers/send-notification $ mvn package \uc774 \uacfc\uc815\uc5d0\uc11c\ub294 Bluemix \ucee8\ud14c\uc774\ub108 \ub808\uc9c0\uc2a4\ud2b8\ub9ac\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ub2e5\ucee4 \ud5c8\ube0c \ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. 2.2 \ub2e5\ucee4 \uc774\ubbf8\uc9c0 \ube4c\ub4dc \ubc0f \ud478\uc2dc \u00b6 Note: Bluemix \ucee8\ud14c\uc774\ub108 \ub808\uc9c0\uc2a4\ud2b8\ub9ac\ub85c \uc774\ubbf8\uc9c0\ub97c \ud478\uc2dc\ud569\ub2c8\ub2e4. Bluemix \ucee8\ud14c\uc774\ub108 \ub808\uc9c0\uc2a4\ud2b8\ub9ac\ub97c \uc0ac\uc6a9\ud558\ub824\uba74 \uba3c\uc800 \uacc4\uc815\uc744 \uad6c\uc131\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc \uc744 \ub530\ub974\uc2ed\uc2dc\uc624. Docker Hub \ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. $ docker build -t registry.ng.bluemix.net/<namespace>/compute-interest-api . $ docker build -t registry.ng.bluemix.net/<namespace>/send-notification . $ docker push registry.ng.bluemix.net/<namespace>/compute-interest-api $ docker push registry.ng.bluemix.net/<namespace>/send-notification 2.3 compute-interest-api.yaml \ubc0f send-notification.yaml \uc218\uc815 \u00b6 \uc774\ubbf8\uc9c0\ub97c \uc131\uacf5\uc801\uc73c\ub85c \ud478\uc2dc\ud588\uc73c\uba74 yaml \ud30c\uc77c\uc744 \uc218\uc815\ud558\uc5ec \ud478\uc2dc\ud55c \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud558\ub3c4\ub85d \ubcc0\uacbd\ud569\ub2c8\ub2e4. // compute-interest-api.yaml spec : containers : - image : registry.ng.bluemix.net/<namespace>/compute-interest-api # \ud478\uc2dc\ud55c \uc774\ubbf8\uc9c0 \uc774\ub984\uc73c\ub85c \ub300\uccb4\ud558\uc2ed\uc2dc\uc624. // send-notification.yaml spec : containers : - image : registry.ng.bluemix.net/<namespace>/send-notification # \ud478\uc2dc\ud55c \uc774\ubbf8\uc9c0 \uc774\ub984\uc73c\ub85c \ub300\uccb4\ud558\uc2ed\uc2dc\uc624. \ub450\uac00\uc9c0 \ud0c0\uc785\uc758 \uc54c\ub9bc\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. 2.3.1 Use default email service \ub610\ub294 2.3.2 Use OpenWhisk Actions . 2.3.1 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub85c \uae30\ubcf8 \uc774\uba54\uc77c \uc11c\ube44\uc2a4 \uc0ac\uc6a9 (gmail) \u00b6 send-notification.yaml \uc5d0\uc11c **\ud658\uacbd \ubcc0\uc218**\ub97c \uc218\uc815\ud558\uc2ed\uc2dc\uc624.: env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # \uc774\uba54\uc77c\uc744 \uc804\uc1a1\ud560 gmail\ub85c \ubcc0\uacbd\ud558\uc2ed\uc2dc\uc624. - name : GMAIL_SENDER_PASSWORD value : 'password' # \uc704\uc5d0 \uc785\ub825\ud55c gmail\uc758 \ud328\uc2a4\uc6cc\ub4dc\ub85c \ubcc0\uacbd\ud558\uc2ed\uc2dc\uc624. - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # \uc218\uc2e0\uc790\uc758 \uc774\uba54\uc77c \uc8fc\uc18c\ub85c \ubcc0\uacbd\ud558\uc2ed\uc2dc\uc624. \uc774\uc81c Step 2.4 \ub97c \uc9c4\ud589\ud558\uc2ed\uc2dc\uc624. 2.3.2 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub85c OpenWhisk Actions \uc0ac\uc6a9 \u00b6 \uc774 \uc139\uc158\uc5d0\uc11c \ud544\uc694\ud55c \uac83\ub4e4: * Slack \ud300\uc758 Slack Incoming Webhook . * OpenWhisk CLI \ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 **Bluemix \uacc4\uc815**\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. 2.3.2.1 Actions \uc0dd\uc131 \u00b6 \uc774 \ub9ac\ud30c\uc9c0\ud1a0\ub9ac\uc758 \ucd5c\uc0c1\uc704 \ub514\ub809\ud1a0\ub9ac\uc5d0\ub294 OpenWhisk Actions\ub97c \uc0dd\uc131\ud558\ub294\ub370 \ud544\uc694\ud55c \ucf54\ub4dc\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc544\uc9c1 OpenWhisk CLI\ub97c \uc124\uce58\ud558\uc9c0 \uc54a\uc558\ub2e4\uba74 \uba3c\uc800 \uc5ec\uae30 \uc5d0\uc11c \uc124\uce58\ud558\uc2ed\uc2dc\uc624. wsk \uba85\ub839\uc744 \uc0ac\uc6a9\ud574\uc11c OpenWhisk Actions\ub97c \uc0c8\uc5c7\u3147\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Action\uc744 \uc0dd\uc131\ud558\ub294 \ubb38\ubc95\uc740 wsk action create < action_name > < source code for action> [add --param for optional Default parameters] \uc785\ub2c8\ub2e4. **Slack \uc54c\ub9bc**\uc744 \ubcf4\ub0b4\uae30 \uc704\ud55c Action \uc0dd\uc131 $ wsk action create sendSlackNotification sendSlack.js --param url https://hooks.slack.com/services/XXXX/YYYY/ZZZZ Replace the url with your Slack team ' s incoming webhook url. **Gmail \uc54c\ub9bc**\uc744 \uc804\uc1a1\ud558\uae30 \uc704\ud55c Action \uc0dd\uc131 $ wsk action create sendEmailNotification sendEmail.js 2.3.2.2 Actions \ud14c\uc2a4\ud2b8 \u00b6 \ub2e4\uc74c\uc744 \uc0ac\uc6a9\ud558\uc5ec OpenWhisk Actions\ub97c \ud14c\uc2a4\ud2b8\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. wsk action invoke [action name] [add --param to pass parameters] \uc2ac\ub799 \uc54c\ub9bc \ud638\ucd9c\ud558\uae30 $ wsk action invoke sendSlackNotification --param text \"Hello from OpenWhisk\" \uc774\uba54\uc77c \uc54c\ub9bc \ud638\ucd9c\ud558\uae30 $ wsk action invoke sendEmailNotification --param sender [ sender 's email] --param password [sender' s password ] --param receiver [ receiver ' s email ] --param subject [ Email subject ] --param text [ Email Body ] \uac01\uac01\uc758 \uba85\ub839\uc744 \ud1b5\ud574 \uc2ac\ub799 \uba54\uc138\uc9c0\uc640 \uc774\uba54\uc77c\uc744 \ubc1b\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. 2.3.2.3 Actions\uc5d0 \ub300\ud55c REST API \uc0dd\uc131 \u00b6 wsk api create \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec REST API \uc5d4\ub4dc\ud3ec\uc778\ud2b8\uc640 \uc0dd\uc131\ub41c Actions\ub97c \ub9f5\ud551\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\ub294 \ubb38\ubc95\uc740 wsk api create [base-path] [api-path] [verb (GET PUT POST etc)] [action name] \uc785\ub2c8\ub2e4. **Slack \uc54c\ub9bc**\uc5d0 \ub300\ud55c \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc0dd\uc131 $ wsk api create /v1 /slack POST sendSlackNotification ok: created API /v1/email POST for action /_/sendEmailNotification https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack **Gmail \uc54c\ub9bc**\uc5d0 \ub300\ud55c \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc0dd\uc131 $ wsk api create /v1 /email POST sendEmailNotification ok: created API /v1/email POST for action /_/sendEmailNotification https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email \ub2e4\uc74c \uba85\ub839\uc73c\ub85c \uc0dd\uc131\ud55c API \ubaa9\ub85d\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.: $ wsk api list ok: APIs Action Verb API Name URL /Anthony.Amanse_dev/sendEmailNotificatio post /v1 https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email /Anthony.Amanse_dev/testDefault post /v1 https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack \uac01\uc790\uc758 API URL\uc744 \ud3b8\ud55c \ubc29\ubc95\uc73c\ub85c \uae30\ub85d\ud574 \ub461\ub2c8\ub2e4. \uc774 URL\uc740 \ub2e4\uc74c \ub2e8\uacc4\uc5d0\uc11c \ub2e4\uc2dc \uc0ac\uc6a9\ub429\ub2c8\ub2e4. 2.3.2.4 REST API Url \ud14c\uc2a4\ud2b8 \u00b6 **Slack \uc54c\ub9bc**\uc758 \uc5d4\ub4dc\ud3ec\uc778\ud2b8\ub97c \ud14c\uc2a4\ud2b8 \ud569\ub2c8\ub2e4. URL \ubd80\ubd84\uc744 \uac01\uc790\uc758 API URL\ub85c \ub300\uccb4\ud558\uc2ed\uc2dc\uc624. $ curl -X POST -d '{ \"text\": \"Hello from OpenWhisk\" }' https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack Gmail \uc54c\ub9bc**\uc758 \uc5d4\ub4dc\ud3ec\uc778\ud2b8\ub97c \ud14c\uc2a4\ud2b8 \ud569\ub2c8\ub2e4. URL \ubd80\ubd84\uc744 \uac01\uc790\uc758 API URL\ub85c \ub300\uccb4\ud558\uc2ed\uc2dc\uc624. **sender, password, receiver, subject \ud30c\ub77c\ubbf8\ud130\uc758 \uac12\uc744 \uac01\uc790\uc758 \uac12\uc73c\ub85c \ub300\uccb4\ud558\uc2ed\uc2dc\uc624. $ curl -X POST -d '{ \"text\": \"Hello from OpenWhisk\", \"subject\": \"Email Notification\", \"sender\": \"testemail@gmail.com\", \"password\": \"passwordOfSender\", \"receiver\": \"receiversEmail\" }' https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email 2.3.2.5 REST API Url\uc744 yaml \ud30c\uc77c\uc5d0 \ucd94\uac00 \u00b6 API\uac00 \uc798 \uc791\ub3d9\ud558\ub294 \uac83\uc744 \ud14c\uc2a4\ud2b8 \ud55c \ud6c4\uc5d0\ub294 \uc774 URL\uc744 send-notification.yaml file \uc5d0 \uc785\ub825\ud569\ub2c8\ub2e4. env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # \ubc1c\uc2e0\uc790\uc758 \uc774\uba54\uc77c - name : GMAIL_SENDER_PASSWORD value : 'password' # \ubc1c\uc2e0\uc790 \uc774\uba54\uc77c\uc758 password - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # \uc218\uc2e0\uc790\uc758 \uc774\uba54\uc77c - name : OPENWHISK_API_URL_SLACK value : 'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack' # Slack \uc54c\ub9bc\uc758 API \uc5d4\ub4dc\ud3ec\uc778\ud2b8 URL - name : SLACK_MESSAGE value : 'Your balance is over $50,000.00' # \uba54\uc138\uc9c0 - name : OPENWHISK_API_URL_EMAIL value : 'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email' # \uc774\uba54\uc77c \uc54c\ub9bc\uc758 API \uc5d4\ub4dc\ud3ec\uc778\ud2b8 URL 2.4 Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \ubc30\ud3ec \u00b6 $ kubectl create -f compute-interest-api.yaml service \"compute-interest-api\" created deployment \"compute-interest-api\" created $ kubectl create -f send-notification.yaml service \"send-notification\" created deployment \"send-notification\" created 3. Frontend \uc11c\ube44\uc2a4 \uc791\uc131 \u00b6 UI\ub294 Node.js \uc571\uc73c\ub85c \uc804\uccb4 \uacc4\uc88c\uc758 \uc794\uc561\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube14\ub8e8\ubbf9\uc2a4\uc758 MySQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\ub97c \uc0ac\uc6a9\uc911\uc774\uba74, \ud658\uacbd \ubcc0\uc218\ub97c account-summary.yaml \ud30c\uc77c\uc5d0 \ub123\uc5b4\uc57c \ud569\ub2c8\ub2e4. \ube14\ub8e8\ubbf9\uc2a4\uc758 MySQL\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294\ub2e4\uba74 \ube48 \uc0c1\ud0dc\ub85c \ub450\uc2ed\uc2dc\uc624. \uc774\ub294 Step 1 \uc5d0\uc11c \uc774\ubbf8 \ud588\uc2b5\ub2c8\ub2e4. *Node.js**\uae30\ubc18 Frontend UI \uc0dd\uc131: $ kubectl create -f account-summary.yaml service \"account-summary\" created deployment \"account-summary\" created 4. \ud2b8\ub79c\uc7ad\uc158 \uc0dd\uc131 \uc11c\ube44\uc2a4 \uc791\uc131 \u00b6 \ud2b8\ub79c\uc7ad\uc158 \uc0dd\uc131 \uc11c\ube44\uc2a4\ub294 Python \uc571\uc73c\ub85c \ucd95\uc801\ub41c \uc774\uc790\ub85c \ub79c\ub364\ud55c \ud2b8\ub79c\uc7ad\uc158 \uc0dd\uc131\ud569\ub2c8\ub2e4. * \ud2b8\ub79c\uc7ad\uc158 \uc0dd\uc131\uc744 \uc704\ud55c Python \uc571 \uc791\uc131: $ kubectl create -f transaction-generator.yaml service \"transaction-generator\" created deployment \"transaction-generator\" created 5. \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc811\uadfc \u00b6 Cluster IP\uc640 NodePort\ub97c \ud1b5\ud574 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ud37c\ube14\ub9ad \ub124\ud2b8\uc6cc\ud06c\ub85c \uc811\uadfc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. NodePort\ub294 **30080**\uc785\ub2c8\ub2e4. IP \ucc3e\uae30: $ bx cs workers <cluster-name> ID Public IP Private IP Machine Type State Status kube-dal10-paac005a5fa6c44786b5dfb3ed8728548f-w1 169 .47.241.213 10 .177.155.13 free normal Ready account-summary \uc11c\ube44\uc2a4\uc758 NodePort \ucc3e\uae30: $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ... account-summary 10 .10.10.74 <nodes> 80 :30080/TCP 2d ... \ube0c\ub77c\uc6b0\uc800\uc5d0\uc11c, http://<your-cluster-IP>:30080 \uc5d0 \uc811\uc18d\ud558\uc2ed\uc2dc\uc624. \ubb38\uc81c \ud574\uacb0 \u00b6 \ub2e4\uc2dc \uc2dc\uc791\ud558\ub824\uba74 \ubaa8\ub450 \uc0ad\uc81c\ud558\uc2ed\uc2dc\uc624. kubectl delete svc,deploy -l app=office-space \ucc38\uc870 \u00b6 John Zaccone - office space app deployed via Docker \uc758 \uc6d0 \uc800\uc790. Office Space \uc571\uc740 1999\ub144\ub3c4 \uc601\ud654 Office Space\uc758 \ucee8\uc149\uc5d0 \uae30\ubc18\ud588\uc2b5\ub2c8\ub2e4. \ub77c\uc774\uc13c\uc2a4 \u00b6 Apache 2.0 \uc815\ubcf4 \uc0ac\uc6a9 \uc548\ub0b4 \u00b6 \uc774 \ud328\ud0a4\uc9c0\uac00 \ud3ec\ud568 \ub41c \uc0d8\ud50c \ucfe0\ubc84\ub124\ud2f0\uc2a4 Yaml \ud30c\uc77c\uc740 IBM Cloud \ubc0f \uae30\ud0c0 \ucfe0\ubc84\ub124\ud2f0\uc2a4 \ud50c\ub7ab\ud3fc\uc5d0 \ub300\ud55c \ubc30\uce58\ub97c \ucd94\uc801\ud558\ub3c4\ub85d \uad6c\uc131 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uc815\ubcf4\ub294 \ubc30\ud3ec\uc2dc\ub9c8\ub2e4 \ubc30\uce58 \ucd94\uc801 \uc11c\ube44\uc2a4 \ub85c \uc804\uc1a1\ub429\ub2c8\ub2e4.: \ucfe0\ubc84\ub124\ud2f0\uc2a4 \ud074\ub7ec\uc2a4\ud130 \uc81c\uacf5\uc790 ( Bluemix,Minikube \ub4f1 ) \ucfe0\ubc84\ub124\ud2f0\uc2a4 Machine ID ( MachineID ) \uc774 \ucfe0\ubc84\ub124\ud2f0\uc2a4 Job\uc758 \ud658\uacbd \ubcc0\uc218 \uc774 \ub370\uc774\ud130\ub294 \uc0d8\ud50c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 yaml \ud30c\uc77c\uc758 \ucfe0\ubc84\ub124\ud2f0\uc2a4 Job\uc73c\ub85c\ubd80\ud130 \uc218\uc9d1\ub429\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub294 IBM\uc5d0\uc11c \uc9c0\uc18d\uc801\uc73c\ub85c \ub354 \ub098\uc740 \ucee8\ud150\uce20\ub97c \uc81c\uacf5\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc608\uc81c\uc758 \uc720\uc6a9\uc131\uc744 \uce21\uc815\ud558\uae30 \uc704\ud574 IBM Cloud\ub85c \ubc30\ud3ec\ub418\ub294 \uc0d8\ud50c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ubc30\ud3ec\ub97c \uad00\ub828\ub41c \uce21\uc815 \ud56d\ubaa9\uc744 \ucd94\uc801\ud558\ub294\ub370\uc5d0 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\ubc30\ud3ec \ucd94\uc801 \uc11c\ube44\uc2a4\ub97c \ud551\ud558\ub294 \ucf54\ub4dc\uac00 \ud3ec\ud568 \ub41c \uc0d8\ud50c \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc758 \ubc30\ud3ec \ub9cc \ucd94\uc801\ub429\ub2c8\ub2e4. \ubc30\ud3ec \ucd94\uc801 \ube44\ud65c\uc131\ud654 \u00b6 account-summary.yaml \ud30c\uc77c\uc758 \ub05d \ubd80\ubd84\uc5d0 \uc788\ub294 \ucfe0\ubc84\ub124\ud2f0\uc2a4 Job \ubd80\ubd84\uc744 \uc8fc\uc11d \ucc98\ub9ac \ud558\uac70\ub098 \uc0ad\uc81c\ud558\uc2ed\uc2dc\uc624.","title":"README ko"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#spring-boot","text":"Read this in other languages: English . Spring Boot\uc740 \ub110\ub9ac \uc0ac\uc6a9\ub418\ub294 \uc790\ubc14 \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \ud504\ub808\uc784\uc6cc\ud06c \uc785\ub2c8\ub2e4. Spring Cloud\ub294 Java \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc2a4\ud0dd\uc758 \uc77c\ubd80\ub85c \ub7f0\ud0c0\uc784 \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\ub294 \ud48d\ubd80\ud55c \ud1b5\ud569 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uac16\uace0 \uc788\uc73c\uba70 \ucfe0\ubc84\ub124\ud2f0\uc2a4\ub294 polyglot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\ub97c \uc2e4\ud589\ud560 \uc218 \uc788\ub294 \ud48d\ubd80\ud55c \uae30\ub2a5\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774 \ub450 \uae30\uc220\uc740 \uc11c\ub85c\ub97c \ubcf4\uc644\ud558\uace0 \uc788\uc73c\uba70 Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \uc704\ud55c \ud6cc\ub96d\ud55c \ud50c\ub7ab\ud3fc\uc744 \ub9cc\ub4ed\ub2c8\ub2e4. \uc774 \ucf54\ub4dc\uc5d0\uc11c\ub294 \ucfe0\ubc84\ub124\ud2f0\uc2a4 \uc704\uc5d0 \uac04\ub2e8\ud55c Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \ubc30\ud3ec\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \uc774 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc778 Office Space\ub294 \uc601\ud654 Office Space \uc5d0 \ub4f1\uc7a5\ud558\ub294 Michael Bolton\uc758 \uac00\uc0c1\uc758 \uc571 \uc544\uc774\ub514\uc5b4\ub97c \ubaa8\ubc29\ud55c \uac83\uc785\ub2c8\ub2e4. \uc774 \uc571\uc740 \uc77c\ubc18\uc801\uc73c\ub85c 1\uc13c\ud2b8 \uc774\ud558\uc758 \uae08\uc561\uc744 \ubc18\uc62c\ub9bc \ud558\ub294 \ubc29\uc2dd \ub300\uc2e0\uc5d0 \ubc84\ub9bc\ud558\uc5ec \ubcc4\ub3c4\uc758 \uc740\ud589 \uacc4\uc88c\uc5d0 \uc800\uc7a5\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \uc774\ub4dd\uc744 \ucde8\ud569\ub2c8\ub2e4. \uc774 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc740 Java 8/Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc774\uc790\ub97c \uacc4\uc0b0\ud558\uace0 \uc18c\uc22b\uc810 \uc774\ud558\uc758 \uae08\uc561\uc740 \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0 \uc800\uc7a5\ud569\ub2c8\ub2e4. \ub610 \ub2e4\ub978 Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\uc778 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub294 \uacc4\uc88c\uc758 \uc794\uc561\uc774 $50,000 \uc774\uc0c1\uc774 \ub418\uba74 \uc774\uba54\uc77c\uc744 \uc804\uc1a1\ud569\ub2c8\ub2e4. \uc774\ub294 \uc774\uc790\ub97c \uacc4\uc0b0\ud558\ub294 Spring Boot \uc6f9\uc11c\ubc84\uc5d0 \uc758\ud574 \ud2b8\ub9ac\uac70 \ub429\ub2c8\ub2e4. \uc571\uc758 Frontend\ub294 Node.js \uae30\ubc18\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc84c\uace0 Spring Boot \uc571\uc5d0 \uc758\ud574 \uc313\uc778 \ud604\uc7ac \uacc4\uc88c\uc758 \uc794\uc561\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. Backend\ub294 MySQL \ub370\uc774\ud130 \ubca0\uc774\uc2a4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uacc4\uc88c \uc794\uc561\uc744 \uc800\uc7a5\ud569\ub2c8\ub2e4.","title":"\ucfe0\ubc84\ub124\ud2f0\uc2a4\uc5d0 \uc790\ubc14 Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \ube4c\ub4dc \ubc0f \ubc30\ud3ec\ud558\uae30"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#_1","text":"\ucfe0\ubc84\ub124\ud2f0\uc2a4 \ud074\ub7ec\uc2a4\ud130\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \ub85c\uceec\uc5d0\uc11c \ud14c\uc2a4\ud2b8 \ud558\ub824\uba74 Minikube \uc5d0\uc11c, \ud074\ub77c\uc6b0\ub4dc\uc5d0 \ubc30\ud3ec\ud558\ub824\uba74 IBM Bluemix Container Service \uc5d0\uc11c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774 \ucf54\ub4dc\ub294 Travis\ub97c \uc0ac\uc6a9\ud558\uc5ec Kubernetes Cluster from Bluemix Container Service \uc5d0\uc11c \uc815\uae30\uc801\uc73c\ub85c \ud14c\uc2a4\ud2b8 \ud569\ub2c8\ub2e4.","title":"\uc0ac\uc804 \uc900\ube44 \uc0ac\ud56d"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#steps","text":"\ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc11c\ube44\uc2a4 \uc0dd\uc131 1.1 \ucee8\ud14c\uc774\ub108\uc5d0\uc11c MySQL \uc0ac\uc6a9 \ub610\ub294 1.2 Bluemix MySQL \uc0ac\uc6a9 Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \uc0dd\uc131 2.1 Maven\uc73c\ub85c \ud504\ub85c\uc81d\ud2b8 \ube4c\ub4dc 2.2 \ub2e5\ucee4 \uc774\ubbf8\uc9c0 \ube4c\ub4dc \ubc0f \ud478\uc2dc 2.3 Spring Boot \uc11c\ube44\uc2a4\ub97c \uc704\ud55c yaml \ud30c\uc77c \uc218\uc815 2.3.1 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub85c \uae30\ubcf8 \uc774\uba54\uc77c \uc11c\ube44\uc2a4 \uc0ac\uc6a9 \ub610\ub294 2.3.2 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub85c OpenWhisk Actions \uc0ac\uc6a9 2.4 Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \ubc30\ud3ec Frontend \uc11c\ube44\uc2a4 \uc791\uc131 \ud2b8\ub79c\uc7ad\uc158 \uc0dd\uc131 \uc11c\ube44\uc2a4 \uc791\uc131 \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc811\uadfc","title":"Steps"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#_2","text":"","title":"\ubb38\uc81c \ud574\uacb0"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#1","text":"\ubc31\uc5d4\ub4dc \uc2dc\uc2a4\ud15c\uc740 MySQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc640 Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uac01 \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\ub294 Deployment\uc640 Service\ub97c \uac16\uace0 \uc788\uc2b5\ub2c8\ub2e4. Deployment\ub294 \uac01 \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\uc5d0 \ub300\ud574 \uc2dc\uc791\ub41c Pod\ub97c \uad00\ub9ac\ud569\ub2c8\ub2e4. \uc11c\ube44\uc2a4\ub294 \uac01 \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\uc5d0 \ub300\ud574 \uc774\ub984\uc73c\ub85c dependency\ub97c \ucc38\uc870\ud558\ub3c4\ub85d \uc548\uc815\uc801\uc778 DNS\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. MySQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\ub97c \uc0dd\uc131\ud558\ub294 \ubc29\ubc95\uc740 \ub450\uac00\uc9c0\uac00 \uc788\uc2b5\ub2c8\ub2e4.: \ucee8\ud14c\uc774\ub108\uc5d0\uc11c MySQL \uc0ac\uc6a9 \ub610\ub294 Bluemix MySQL \uc0ac\uc6a9","title":"1. \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc11c\ube44\uc2a4 \uc0dd\uc131"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#11-mysql","text":"$ kubectl create -f account-database.yaml service \"account-database\" created deployment \"account-database\" created \uae30\ubcf8 \uc2e0\uc784 \uc815\ubcf4\ub294 \uc774\ubbf8 secrets.yaml\uc5d0 base64\ub85c \uc778\ucf54\ub529 \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. base64 \uc778\ucf54\ub529\uc740 \uc2e0\uc784 \uc815\ubcf4\ub97c \uc554\ud638\ud654\ud558\uc9c0\ub294 \uc54a\uc73c\ubbc0\ub85c \uc774 \uc815\ubcf4\ub97c Github\uc5d0 \uc5c5\ub85c\ub4dc \ud558\uc9c0 \ub9c8\uc2ed\uc2dc\uc624. $ kubectl apply -f secrets.yaml secret \"demo-credentials\" created Step 2 \uc5d0\uc11c \uacc4\uc18d \uc9c4\ud589 \ud558\uc2ed\uc2dc\uc624.","title":"1.1 \ucee8\ud14c\uc774\ub108\uc5d0\uc11c MySQL \uc0ac\uc6a9"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#12-bluemix-mysql","text":"https://console.ng.bluemix.net/catalog/services/compose-for-mysql\ub97c \ud1b5\ud574 Bluemix\uc5d0\uc11c Compose for MySQL\uc744 \uc791\uc131\ud558\uc2ed\uc2dc\uc624. Service Credentials\ub85c \uac00\uc11c \uc2e0\uc784 \uc815\ubcf4\ub97c \ud655\uc778\ud558\uc2ed\uc2dc\uc624. MySQL\uc758 Hostname, port, user \uadf8\ub9ac\uace0 password\uac00 \uc2e0\uc784 \uc815\ubcf4\uc758 url\uc5d0 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c\uc758 \ud615\uc2dd\uc73c\ub85c \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uc2e0\uc784\uc815\ubcf4\ub97c \ud050\ubc84\ub124\ud2f0\uc2a4 \ud074\ub7ec\uc2a4\ud130\uc5d0 Secret\uc73c\ub85c \uc801\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774 \uac12\uc740 \ubc18\ub4dc\uc2dc base64 \ub85c \uc778\ucf54\ub529 \ub418\uc5b4 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. ./scripts/create-secrets.sh \ub97c \uc0ac\uc6a9\ud558\uc2ed\uc2dc\uc624. \uc2e0\uc784 \uc815\ubcf4\ub97c \ub123\ub3c4\ub85d \uc785\ub825\uc744 \ubc1b\uc744 \uac83\uc785\ub2c8\ub2e4. \uc774 \uc2a4\ud06c\ub9bd\ud2b8\ub97c \ud1b5\ud574 \uc2e0\uc784 \uc815\ubcf4\ub97c \uc778\ucf54\ub529 \ud558\uace0 \ud074\ub7ec\uc2a4\ud130\uc5d0 Secret\uc73c\ub85c \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. $ ./scripts/create-secrets.sh Enter MySQL username: admin Enter MySQL password: password Enter MySQL host: hostname Enter MySQL port: 23966 secret \"demo-credentials\" created \ub610\ud55c secrets.yaml \ud30c\uc77c\uc744 \uc218\uc815\ud558\uc5ec base64 \uc778\ucf54\ub529\ub41c \uc2e0\uc784\uc815\ubcf4\ub97c \uc9c1\uc811 \ub123\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uacbd\uc6b0\uc5d0\ub294 kubectl apply -f secrets.yaml \ub97c \uc218\ud589\ud558\uc2ed\uc2dc\uc624. Step 2 \uc5d0\uc11c \uacc4\uc18d \uc9c4\ud589 \ud558\uc2ed\uc2dc\uc624.","title":"1.2 Bluemix MySQL \uc0ac\uc6a9"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#2-spring-boot","text":"Maven\uc774 \uc124\uce58\ub418\uc5b4 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4. . Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158\ub4e4\uc744 \uc218\uc815\ud558\ub824\uba74 Java \ud504\ub85c\uc81d\ud2b8\uc640 \ub2e5\ucee4 \uc774\ubbf8\uc9c0\ub97c \ube4c\ub4dc\ud558\uae30 \uc804\uc5d0 \ud574\uc57c \ud569\ub2c8\ub2e4. Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\ub294 Compute-Interest-API \uc640 Send-Notification \uc785\ub2c8\ub2e4. **Compute-Interest-API**\uc740 MySQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\ub97c \uc0ac\uc6a9\ud558\uae30\uc704\ud574 \uad6c\uc131\ub41c Spring Boot \uc571\uc785\ub2c8\ub2e4. \uad00\ub828\ud55c \uad6c\uc131\uc740 spring.datasource.* \uc5d0 \uc788\ub294 application.properties\uc5d0 \uc704\uce58\ud574 \uc788\uc2b5\ub2c8\ub2e4. compute-interest-api/src/main/resources/application.properties spring.datasource.url = jdbc:mysql://${MYSQL_DB_HOST}:${MYSQL_DB_PORT}/dockercon2017 # Username\uacfc password spring.datasource.username = ${MYSQL_DB_USER} spring.datasource.password = ${MYSQL_DB_PASSWORD} application.properties \ub294 MYSQL_DB_* \ud658\uacbd \ubcc0\uc218\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 \uad6c\uc131\ub410\uc2b5\ub2c8\ub2e4. \uc774\ub294 compute-interest-api.yaml \ud30c\uc77c\uc5d0 \uc815\uc758\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. compute-interest-api.yaml spec : containers : - image : anthonyamanse/compute-interest-api:secrets imagePullPolicy : Always name : compute-interest-api env : - name : MYSQL_DB_USER valueFrom : secretKeyRef : name : demo-credentials key : username - name : MYSQL_DB_PASSWORD valueFrom : secretKeyRef : name : demo-credentials key : password - name : MYSQL_DB_HOST valueFrom : secretKeyRef : name : demo-credentials key : host - name : MYSQL_DB_PORT valueFrom : secretKeyRef : name : demo-credentials key : port ports : - containerPort : 8080 \uc774 YAML \ud30c\uc77c\uc740 \uc774\ubbf8 \uc774\uc804 \ub2e8\uacc4\uc5d0\uc11c \uc0dd\uc131\ud55c \ucfe0\ubc84\ub124\ud2f0\uc2a4 Secret\uc73c\ub85c\ubd80\ud130 \uac12\uc744 \uc5bb\ub3c4\ub85d \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 application.properties \uc5d0 \uc788\ub294 Spring Boot \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0\uc11c \uc0ac\uc6a9\ub420 \uac83\uc785\ub2c8\ub2e4. Send-Notification \uc740 gmail \uadf8\ub9ac\uace0/\ub610\ub294 Slack\uc744 \ud1b5\ud574 \uc54c\ub9bc\uc744 \uc904 \uc218 \uc788\ub3c4\ub85d \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \uc54c\ub9bc\uc740 MySQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc758 \uacc4\uc88c \uc794\uc561\uc774 $50,000\uc744 \ub118\uc5c8\uc744 \ub54c\uc5d0 \ud55c\ubc88\ub9cc \ubc1c\uc1a1\ub429\ub2c8\ub2e4. \uae30\ubcf8\uc801\uc73c\ub85c\ub294 gmail \uc635\uc158\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. Event driven \uae30\uc220\uc744 \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 OpenWhisk \ub97c \uc0ac\uc6a9\ud558\uc5ec \uc774\uba54\uc77c\uacfc \uc2ac\ub799 \uba54\uc138\uc9c0\ub97c \ubcf4\ub0c5\ub2c8\ub2e4. \uc54c\ub9bc \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4\ub85c OpenWhisk\ub97c \uc0ac\uc6a9\ud558\ub824\uba74 \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \uc774\ubbf8\uc9c0\ub97c \ube4c\ub4dc \ubc0f \ubc30\ud3ec \ud558\uae30 \uc804\uc5d0 \uc5ec\uae30 \uc2a4\ud15d\uc744 \ub530\ub974\uc2ed\uc2dc\uc624. \uc774\uba54\uc77c \uc54c\ub9bc \uc124\uc815\ub9cc \uc0ac\uc6a9\ud558\ub824\uba74 \uadf8\ub0e5 \uc9c4\ud589\ud558\uc2ed\uc2dc\uc624.","title":"2. Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \uc0dd\uc131"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#21-maven","text":"Maven\uc774 Java \ud504\ub85c\uc81d\ud2b8\ub97c \uc131\uacf5\uc801\uc73c\ub85c \ube4c\ub4dc\ud558\uba74, \uac01 \ud3f4\ub354\uc5d0 \uc81c\uacf5\ub41c **Dockerfile**\ub85c \ub2e5\ucee4 \uc774\ubbf8\uc9c0\ub97c \ube4c\ub4dc\ud574\uc57c \ud569\ub2c8\ub2e4. Note: compute-interest-api\ub294 \uc2dc\ubbac\ub808\uc774\uc158 \ubaa9\uc801\uc73c\ub85c \ub098\uba38\uc9c0 \uae08\uc561\uc5d0 x100,000\uc744 \uacf1\ud569\ub2c8\ub2e4. src/main/java/officespace/controller/MainController.java \uc5d0\uc11c remainingInterest *= 100000 \ub77c\uc778\uc744 \uc218\uc815/\uc81c\uac70\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub610\ud55c \uc794\uc561\uc774 $50,000\uc744 \ub118\uae38 \ub54c \uc54c\ub9bc\uc744 \uc804\uc1a1\ud558\ub3c4\ub85d \ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. if (updatedBalance > 50000 && emailSent == false ) \ub77c\uc778\uc5d0\uc11c \uc774 \uc22b\uc790\ub97c \ubcc0\uacbd\ud558\uc5ec \uc0ac\uc6a9\ud558\uc2ed\uc2dc\uc624. \ubcc0\uacbd \uc0ac\ud56d\uc744 \uc800\uc7a5\ud558\uba74 \uc774\uc81c \ud504\ub85c\uc81d\ud2b8\ub97c \ube4c\ub4dc\ud558\uc2ed\uc2dc\uc624. Go to containers/compute-interest-api $ mvn package Go to containers/send-notification $ mvn package \uc774 \uacfc\uc815\uc5d0\uc11c\ub294 Bluemix \ucee8\ud14c\uc774\ub108 \ub808\uc9c0\uc2a4\ud2b8\ub9ac\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ub2e5\ucee4 \ud5c8\ube0c \ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.","title":"2.1. Maven\uc73c\ub85c \ud504\ub85c\uc81d\ud2b8 \ube4c\ub4dc"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#22","text":"Note: Bluemix \ucee8\ud14c\uc774\ub108 \ub808\uc9c0\uc2a4\ud2b8\ub9ac\ub85c \uc774\ubbf8\uc9c0\ub97c \ud478\uc2dc\ud569\ub2c8\ub2e4. Bluemix \ucee8\ud14c\uc774\ub108 \ub808\uc9c0\uc2a4\ud2b8\ub9ac\ub97c \uc0ac\uc6a9\ud558\ub824\uba74 \uba3c\uc800 \uacc4\uc815\uc744 \uad6c\uc131\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc \uc744 \ub530\ub974\uc2ed\uc2dc\uc624. Docker Hub \ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. $ docker build -t registry.ng.bluemix.net/<namespace>/compute-interest-api . $ docker build -t registry.ng.bluemix.net/<namespace>/send-notification . $ docker push registry.ng.bluemix.net/<namespace>/compute-interest-api $ docker push registry.ng.bluemix.net/<namespace>/send-notification","title":"2.2 \ub2e5\ucee4 \uc774\ubbf8\uc9c0 \ube4c\ub4dc \ubc0f \ud478\uc2dc"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#23-compute-interest-apiyaml-send-notificationyaml","text":"\uc774\ubbf8\uc9c0\ub97c \uc131\uacf5\uc801\uc73c\ub85c \ud478\uc2dc\ud588\uc73c\uba74 yaml \ud30c\uc77c\uc744 \uc218\uc815\ud558\uc5ec \ud478\uc2dc\ud55c \uc774\ubbf8\uc9c0\ub97c \uc0ac\uc6a9\ud558\ub3c4\ub85d \ubcc0\uacbd\ud569\ub2c8\ub2e4. // compute-interest-api.yaml spec : containers : - image : registry.ng.bluemix.net/<namespace>/compute-interest-api # \ud478\uc2dc\ud55c \uc774\ubbf8\uc9c0 \uc774\ub984\uc73c\ub85c \ub300\uccb4\ud558\uc2ed\uc2dc\uc624. // send-notification.yaml spec : containers : - image : registry.ng.bluemix.net/<namespace>/send-notification # \ud478\uc2dc\ud55c \uc774\ubbf8\uc9c0 \uc774\ub984\uc73c\ub85c \ub300\uccb4\ud558\uc2ed\uc2dc\uc624. \ub450\uac00\uc9c0 \ud0c0\uc785\uc758 \uc54c\ub9bc\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4. 2.3.1 Use default email service \ub610\ub294 2.3.2 Use OpenWhisk Actions .","title":"2.3 compute-interest-api.yaml \ubc0f send-notification.yaml \uc218\uc815"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#231-gmail","text":"send-notification.yaml \uc5d0\uc11c **\ud658\uacbd \ubcc0\uc218**\ub97c \uc218\uc815\ud558\uc2ed\uc2dc\uc624.: env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # \uc774\uba54\uc77c\uc744 \uc804\uc1a1\ud560 gmail\ub85c \ubcc0\uacbd\ud558\uc2ed\uc2dc\uc624. - name : GMAIL_SENDER_PASSWORD value : 'password' # \uc704\uc5d0 \uc785\ub825\ud55c gmail\uc758 \ud328\uc2a4\uc6cc\ub4dc\ub85c \ubcc0\uacbd\ud558\uc2ed\uc2dc\uc624. - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # \uc218\uc2e0\uc790\uc758 \uc774\uba54\uc77c \uc8fc\uc18c\ub85c \ubcc0\uacbd\ud558\uc2ed\uc2dc\uc624. \uc774\uc81c Step 2.4 \ub97c \uc9c4\ud589\ud558\uc2ed\uc2dc\uc624.","title":"2.3.1 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub85c \uae30\ubcf8 \uc774\uba54\uc77c \uc11c\ube44\uc2a4 \uc0ac\uc6a9 (gmail)"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#232-openwhisk-actions","text":"\uc774 \uc139\uc158\uc5d0\uc11c \ud544\uc694\ud55c \uac83\ub4e4: * Slack \ud300\uc758 Slack Incoming Webhook . * OpenWhisk CLI \ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 **Bluemix \uacc4\uc815**\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.","title":"2.3.2 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub85c OpenWhisk Actions \uc0ac\uc6a9"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#2321-actions","text":"\uc774 \ub9ac\ud30c\uc9c0\ud1a0\ub9ac\uc758 \ucd5c\uc0c1\uc704 \ub514\ub809\ud1a0\ub9ac\uc5d0\ub294 OpenWhisk Actions\ub97c \uc0dd\uc131\ud558\ub294\ub370 \ud544\uc694\ud55c \ucf54\ub4dc\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc544\uc9c1 OpenWhisk CLI\ub97c \uc124\uce58\ud558\uc9c0 \uc54a\uc558\ub2e4\uba74 \uba3c\uc800 \uc5ec\uae30 \uc5d0\uc11c \uc124\uce58\ud558\uc2ed\uc2dc\uc624. wsk \uba85\ub839\uc744 \uc0ac\uc6a9\ud574\uc11c OpenWhisk Actions\ub97c \uc0c8\uc5c7\u3147\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. Action\uc744 \uc0dd\uc131\ud558\ub294 \ubb38\ubc95\uc740 wsk action create < action_name > < source code for action> [add --param for optional Default parameters] \uc785\ub2c8\ub2e4. **Slack \uc54c\ub9bc**\uc744 \ubcf4\ub0b4\uae30 \uc704\ud55c Action \uc0dd\uc131 $ wsk action create sendSlackNotification sendSlack.js --param url https://hooks.slack.com/services/XXXX/YYYY/ZZZZ Replace the url with your Slack team ' s incoming webhook url. **Gmail \uc54c\ub9bc**\uc744 \uc804\uc1a1\ud558\uae30 \uc704\ud55c Action \uc0dd\uc131 $ wsk action create sendEmailNotification sendEmail.js","title":"2.3.2.1 Actions \uc0dd\uc131"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#2322-actions","text":"\ub2e4\uc74c\uc744 \uc0ac\uc6a9\ud558\uc5ec OpenWhisk Actions\ub97c \ud14c\uc2a4\ud2b8\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. wsk action invoke [action name] [add --param to pass parameters] \uc2ac\ub799 \uc54c\ub9bc \ud638\ucd9c\ud558\uae30 $ wsk action invoke sendSlackNotification --param text \"Hello from OpenWhisk\" \uc774\uba54\uc77c \uc54c\ub9bc \ud638\ucd9c\ud558\uae30 $ wsk action invoke sendEmailNotification --param sender [ sender 's email] --param password [sender' s password ] --param receiver [ receiver ' s email ] --param subject [ Email subject ] --param text [ Email Body ] \uac01\uac01\uc758 \uba85\ub839\uc744 \ud1b5\ud574 \uc2ac\ub799 \uba54\uc138\uc9c0\uc640 \uc774\uba54\uc77c\uc744 \ubc1b\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","title":"2.3.2.2 Actions \ud14c\uc2a4\ud2b8"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#2323-actions-rest-api","text":"wsk api create \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\uc5ec REST API \uc5d4\ub4dc\ud3ec\uc778\ud2b8\uc640 \uc0dd\uc131\ub41c Actions\ub97c \ub9f5\ud551\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uba85\ub839\uc744 \uc0ac\uc6a9\ud558\ub294 \ubb38\ubc95\uc740 wsk api create [base-path] [api-path] [verb (GET PUT POST etc)] [action name] \uc785\ub2c8\ub2e4. **Slack \uc54c\ub9bc**\uc5d0 \ub300\ud55c \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc0dd\uc131 $ wsk api create /v1 /slack POST sendSlackNotification ok: created API /v1/email POST for action /_/sendEmailNotification https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack **Gmail \uc54c\ub9bc**\uc5d0 \ub300\ud55c \uc5d4\ub4dc\ud3ec\uc778\ud2b8 \uc0dd\uc131 $ wsk api create /v1 /email POST sendEmailNotification ok: created API /v1/email POST for action /_/sendEmailNotification https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email \ub2e4\uc74c \uba85\ub839\uc73c\ub85c \uc0dd\uc131\ud55c API \ubaa9\ub85d\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.: $ wsk api list ok: APIs Action Verb API Name URL /Anthony.Amanse_dev/sendEmailNotificatio post /v1 https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email /Anthony.Amanse_dev/testDefault post /v1 https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack \uac01\uc790\uc758 API URL\uc744 \ud3b8\ud55c \ubc29\ubc95\uc73c\ub85c \uae30\ub85d\ud574 \ub461\ub2c8\ub2e4. \uc774 URL\uc740 \ub2e4\uc74c \ub2e8\uacc4\uc5d0\uc11c \ub2e4\uc2dc \uc0ac\uc6a9\ub429\ub2c8\ub2e4.","title":"2.3.2.3 Actions\uc5d0 \ub300\ud55c REST API \uc0dd\uc131"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#2324-rest-api-url","text":"**Slack \uc54c\ub9bc**\uc758 \uc5d4\ub4dc\ud3ec\uc778\ud2b8\ub97c \ud14c\uc2a4\ud2b8 \ud569\ub2c8\ub2e4. URL \ubd80\ubd84\uc744 \uac01\uc790\uc758 API URL\ub85c \ub300\uccb4\ud558\uc2ed\uc2dc\uc624. $ curl -X POST -d '{ \"text\": \"Hello from OpenWhisk\" }' https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack Gmail \uc54c\ub9bc**\uc758 \uc5d4\ub4dc\ud3ec\uc778\ud2b8\ub97c \ud14c\uc2a4\ud2b8 \ud569\ub2c8\ub2e4. URL \ubd80\ubd84\uc744 \uac01\uc790\uc758 API URL\ub85c \ub300\uccb4\ud558\uc2ed\uc2dc\uc624. **sender, password, receiver, subject \ud30c\ub77c\ubbf8\ud130\uc758 \uac12\uc744 \uac01\uc790\uc758 \uac12\uc73c\ub85c \ub300\uccb4\ud558\uc2ed\uc2dc\uc624. $ curl -X POST -d '{ \"text\": \"Hello from OpenWhisk\", \"subject\": \"Email Notification\", \"sender\": \"testemail@gmail.com\", \"password\": \"passwordOfSender\", \"receiver\": \"receiversEmail\" }' https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email","title":"2.3.2.4 REST API Url \ud14c\uc2a4\ud2b8"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#2325-rest-api-url-yaml","text":"API\uac00 \uc798 \uc791\ub3d9\ud558\ub294 \uac83\uc744 \ud14c\uc2a4\ud2b8 \ud55c \ud6c4\uc5d0\ub294 \uc774 URL\uc744 send-notification.yaml file \uc5d0 \uc785\ub825\ud569\ub2c8\ub2e4. env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # \ubc1c\uc2e0\uc790\uc758 \uc774\uba54\uc77c - name : GMAIL_SENDER_PASSWORD value : 'password' # \ubc1c\uc2e0\uc790 \uc774\uba54\uc77c\uc758 password - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # \uc218\uc2e0\uc790\uc758 \uc774\uba54\uc77c - name : OPENWHISK_API_URL_SLACK value : 'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/slack' # Slack \uc54c\ub9bc\uc758 API \uc5d4\ub4dc\ud3ec\uc778\ud2b8 URL - name : SLACK_MESSAGE value : 'Your balance is over $50,000.00' # \uba54\uc138\uc9c0 - name : OPENWHISK_API_URL_EMAIL value : 'https://service.us.apiconnect.ibmcloud.com/gws/apigateway/api/.../v1/email' # \uc774\uba54\uc77c \uc54c\ub9bc\uc758 API \uc5d4\ub4dc\ud3ec\uc778\ud2b8 URL","title":"2.3.2.5 REST API Url\uc744 yaml \ud30c\uc77c\uc5d0 \ucd94\uac00"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#24-spring-boot","text":"$ kubectl create -f compute-interest-api.yaml service \"compute-interest-api\" created deployment \"compute-interest-api\" created $ kubectl create -f send-notification.yaml service \"send-notification\" created deployment \"send-notification\" created","title":"2.4 Spring Boot \ub9c8\uc774\ud06c\ub85c\uc11c\ube44\uc2a4 \ubc30\ud3ec"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#3-frontend","text":"UI\ub294 Node.js \uc571\uc73c\ub85c \uc804\uccb4 \uacc4\uc88c\uc758 \uc794\uc561\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. \ube14\ub8e8\ubbf9\uc2a4\uc758 MySQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\ub97c \uc0ac\uc6a9\uc911\uc774\uba74, \ud658\uacbd \ubcc0\uc218\ub97c account-summary.yaml \ud30c\uc77c\uc5d0 \ub123\uc5b4\uc57c \ud569\ub2c8\ub2e4. \ube14\ub8e8\ubbf9\uc2a4\uc758 MySQL\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294\ub2e4\uba74 \ube48 \uc0c1\ud0dc\ub85c \ub450\uc2ed\uc2dc\uc624. \uc774\ub294 Step 1 \uc5d0\uc11c \uc774\ubbf8 \ud588\uc2b5\ub2c8\ub2e4. *Node.js**\uae30\ubc18 Frontend UI \uc0dd\uc131: $ kubectl create -f account-summary.yaml service \"account-summary\" created deployment \"account-summary\" created","title":"3. Frontend \uc11c\ube44\uc2a4 \uc791\uc131"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#4","text":"\ud2b8\ub79c\uc7ad\uc158 \uc0dd\uc131 \uc11c\ube44\uc2a4\ub294 Python \uc571\uc73c\ub85c \ucd95\uc801\ub41c \uc774\uc790\ub85c \ub79c\ub364\ud55c \ud2b8\ub79c\uc7ad\uc158 \uc0dd\uc131\ud569\ub2c8\ub2e4. * \ud2b8\ub79c\uc7ad\uc158 \uc0dd\uc131\uc744 \uc704\ud55c Python \uc571 \uc791\uc131: $ kubectl create -f transaction-generator.yaml service \"transaction-generator\" created deployment \"transaction-generator\" created","title":"4. \ud2b8\ub79c\uc7ad\uc158 \uc0dd\uc131 \uc11c\ube44\uc2a4 \uc791\uc131"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#5","text":"Cluster IP\uc640 NodePort\ub97c \ud1b5\ud574 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \ud37c\ube14\ub9ad \ub124\ud2b8\uc6cc\ud06c\ub85c \uc811\uadfc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. NodePort\ub294 **30080**\uc785\ub2c8\ub2e4. IP \ucc3e\uae30: $ bx cs workers <cluster-name> ID Public IP Private IP Machine Type State Status kube-dal10-paac005a5fa6c44786b5dfb3ed8728548f-w1 169 .47.241.213 10 .177.155.13 free normal Ready account-summary \uc11c\ube44\uc2a4\uc758 NodePort \ucc3e\uae30: $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ... account-summary 10 .10.10.74 <nodes> 80 :30080/TCP 2d ... \ube0c\ub77c\uc6b0\uc800\uc5d0\uc11c, http://<your-cluster-IP>:30080 \uc5d0 \uc811\uc18d\ud558\uc2ed\uc2dc\uc624.","title":"5. \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uc811\uadfc"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#_3","text":"\ub2e4\uc2dc \uc2dc\uc791\ud558\ub824\uba74 \ubaa8\ub450 \uc0ad\uc81c\ud558\uc2ed\uc2dc\uc624. kubectl delete svc,deploy -l app=office-space","title":"\ubb38\uc81c \ud574\uacb0"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#_4","text":"John Zaccone - office space app deployed via Docker \uc758 \uc6d0 \uc800\uc790. Office Space \uc571\uc740 1999\ub144\ub3c4 \uc601\ud654 Office Space\uc758 \ucee8\uc149\uc5d0 \uae30\ubc18\ud588\uc2b5\ub2c8\ub2e4.","title":"\ucc38\uc870"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#_5","text":"Apache 2.0","title":"\ub77c\uc774\uc13c\uc2a4"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#_6","text":"\uc774 \ud328\ud0a4\uc9c0\uac00 \ud3ec\ud568 \ub41c \uc0d8\ud50c \ucfe0\ubc84\ub124\ud2f0\uc2a4 Yaml \ud30c\uc77c\uc740 IBM Cloud \ubc0f \uae30\ud0c0 \ucfe0\ubc84\ub124\ud2f0\uc2a4 \ud50c\ub7ab\ud3fc\uc5d0 \ub300\ud55c \ubc30\uce58\ub97c \ucd94\uc801\ud558\ub3c4\ub85d \uad6c\uc131 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \uc815\ubcf4\ub294 \ubc30\ud3ec\uc2dc\ub9c8\ub2e4 \ubc30\uce58 \ucd94\uc801 \uc11c\ube44\uc2a4 \ub85c \uc804\uc1a1\ub429\ub2c8\ub2e4.: \ucfe0\ubc84\ub124\ud2f0\uc2a4 \ud074\ub7ec\uc2a4\ud130 \uc81c\uacf5\uc790 ( Bluemix,Minikube \ub4f1 ) \ucfe0\ubc84\ub124\ud2f0\uc2a4 Machine ID ( MachineID ) \uc774 \ucfe0\ubc84\ub124\ud2f0\uc2a4 Job\uc758 \ud658\uacbd \ubcc0\uc218 \uc774 \ub370\uc774\ud130\ub294 \uc0d8\ud50c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 yaml \ud30c\uc77c\uc758 \ucfe0\ubc84\ub124\ud2f0\uc2a4 Job\uc73c\ub85c\ubd80\ud130 \uc218\uc9d1\ub429\ub2c8\ub2e4. \uc774 \ub370\uc774\ud130\ub294 IBM\uc5d0\uc11c \uc9c0\uc18d\uc801\uc73c\ub85c \ub354 \ub098\uc740 \ucee8\ud150\uce20\ub97c \uc81c\uacf5\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc608\uc81c\uc758 \uc720\uc6a9\uc131\uc744 \uce21\uc815\ud558\uae30 \uc704\ud574 IBM Cloud\ub85c \ubc30\ud3ec\ub418\ub294 \uc0d8\ud50c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \ubc30\ud3ec\ub97c \uad00\ub828\ub41c \uce21\uc815 \ud56d\ubaa9\uc744 \ucd94\uc801\ud558\ub294\ub370\uc5d0 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\ubc30\ud3ec \ucd94\uc801 \uc11c\ube44\uc2a4\ub97c \ud551\ud558\ub294 \ucf54\ub4dc\uac00 \ud3ec\ud568 \ub41c \uc0d8\ud50c \uc751\uc6a9 \ud504\ub85c\uadf8\ub7a8\uc758 \ubc30\ud3ec \ub9cc \ucd94\uc801\ub429\ub2c8\ub2e4.","title":"\uc815\ubcf4 \uc0ac\uc6a9 \uc548\ub0b4"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README-ko/#_7","text":"account-summary.yaml \ud30c\uc77c\uc758 \ub05d \ubd80\ubd84\uc5d0 \uc788\ub294 \ucfe0\ubc84\ub124\ud2f0\uc2a4 Job \ubd80\ubd84\uc744 \uc8fc\uc11d \ucc98\ub9ac \ud558\uac70\ub098 \uc0ad\uc81c\ud558\uc2ed\uc2dc\uc624.","title":"\ubc30\ud3ec \ucd94\uc801 \ube44\ud65c\uc131\ud654"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_con_k8s/","text":"Lab 0c. Connect to your kubernetes environment \u00b6 Complete steps in this section to connect to your Kubernetes cluster. Open a terminal window or command window. Note: use a web terminal if you attend a workshop Set an environment variable export USERNAME=user### Note: replace ### with your assigned ID Install the kubernetes and container registry plugins: ibmcloud plugin install kubernetes-service -r Bluemix ibmcloud plugin install container-registry -r Bluemix Login to Kubernetes service in IBM Cloud . Select your Kubernetes cluster. Navigate to Access tab. In the above terminal window, complete all steps in the section After your cluster provisions, gain access .","title":"Lab 0c. Connect to your kubernetes environment"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_con_k8s/#lab-0c-connect-to-your-kubernetes-environment","text":"Complete steps in this section to connect to your Kubernetes cluster. Open a terminal window or command window. Note: use a web terminal if you attend a workshop Set an environment variable export USERNAME=user### Note: replace ### with your assigned ID Install the kubernetes and container registry plugins: ibmcloud plugin install kubernetes-service -r Bluemix ibmcloud plugin install container-registry -r Bluemix Login to Kubernetes service in IBM Cloud . Select your Kubernetes cluster. Navigate to Access tab. In the above terminal window, complete all steps in the section After your cluster provisions, gain access .","title":"Lab 0c. Connect to your kubernetes environment"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/","text":"Deploying Microservices \u00b6 Deploy Office Space sample application on Kubernetes \u00b6 In this session, we demonstrate how a sample cloud native application can be deployed on top of Kubernetes. This application, Office Space, mimicks the fictitious app idea from Michael Bolton in the movie Office Space . The app takes advantage of a financial program that computes interest for transactions by diverting fractions of a cent that are usually rounded off into a seperate bank account. The application includes a few services developed in different languages. * The key coponent of the application is a Java 8/Spring Boot microservice that computes the interest then takes the fraction of the pennies to a database. * Another Spring Boot microservice is the notification service. It sends email when the account balance reach more than $50,000. It is triggered by the Spring Boot webserver that computes the interest. * The frontend user interafce is a Node.js application that shows the current account balance accumulated by the Spring Boot app. * The backend uses a MySQL database to store the account balance. * The transaction generator is a Python application that generates random transactions with accumulated interest. It's the last piece of your service mesh and used to simulate the transaction activities. The instructions were adapted from the more comprehensive tutorial found here - https://github.com/IBM/spring-boot-microservices-on-kubernetes. Develop Microservices \u00b6 During this session, we focus on deploying the microservices due to time constraint. Feel free to review the source code of the services, if you are interested. Couple of microservices were developed in Spring Boot which is one of the popular Java microservices framework. Spring Cloud has a rich set of well integrated Java libraries to address runtime concerns as part of the Java application stack, and Kubernetes provides a rich featureset to run polyglot microservices. Together these technologies complement each other and make a great platform for Spring Boot applications. Other microservices were developed in Node.js and Python . MySQL Database running in a separate container serves as persistent store. As a whole, the sample application delivers a native cloud architecture and follows 12 factors best practice. The source code are in the following subfolders * containers/compute-interest-api * containers/send-notification * containers/account-summary * containers/transaction-generator Flow \u00b6 The Transaction Generator service written in Python simulates transactions and pushes them to the Compute Interest microservice. The Compute Interest microservice computes the interest and then moves the fraction of pennies to the MySQL database to be stored. The database can be running within a container in the same deployment or on a public cloud such as IBM Cloud. The Compute Interest microservice then calls the notification service to notify the user when the total amount in the user\u2019s account reaches $50,000. The Notification service uses IBM Cloud Function to send an email message to the user. The front end user interface in Node.js retrieves the account balance and display. Included Components \u00b6 IBM Cloud Kubernetes Service : IBM Bluemix Container Service manages highly available apps inside Docker containers and Kubernetes clusters on the IBM Cloud. Compose for MySQL : Probably the most popular open source relational database in the world. IBM Cloud Functions : Execute code on demand in a highly scalable, serverless environment. Featured Technologies \u00b6 Container Orchestration : Automating the deployment, scaling and management of containerized applications. Databases : Repository for storing and managing collections of data. Serverless : An event-action platform that allows you to execute code in response to an event. Steps \u00b6 Clone the repo Modify send-notification.yaml file for email notification Deploy Database MySQL service Deploy Microservice compute-interest-api Deploy Microservice send-notification Deploy Microservice account-summary - the Frontend User Interface Deploy Microservice transaction-generator - the Transaction Generator service Access Your Application Each service in the application run in their containers. It has a Deployment and a Service. The deployment manages the pods started for each microservice. The Service creates a stable DNS entry for each microservice so they can reference their dependencies by name. 1. Clone the repo \u00b6 Clone this repository. In a terminal, run: $ cd ~ $ git clone https://github.com/lee-zhg/spring-boot-microservices-on-kubernetes $ cd spring-boot-microservices-on-kubernetes 2. Modify send-notification.yaml file for email notification \u00b6 Note: Additional Gmail security configurations may be required by Gmail to send/received email in this way. Optionally, if you like to send and receive email (gmail) notification, You will need to modify the environment variables in the send-notification.yaml file: env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # change this to the gmail that will send the email - name : GMAIL_SENDER_PASSWORD value : 'password' # change this to the the password of the gmail above - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # change this to the email of the receiver 3. Deploy MySQL Database \u00b6 Deploy MySQL database $ kubectl create -f account-database.yaml service \"account-database\" created deployment \"account-database\" created Create a secure storing credential of MySQL database Default credentials are already encoded in base64 in secrets.yaml. Note: Encoding in base64 does not encrypt or hide your secrets. Do not put this in your Github. $ kubectl apply -f secrets.yaml secret \"demo-credentials\" created 4. Deploy compute-interest-api service \u00b6 Microservice compute-interest-api is written in Spring Boot. It's deployed to your cluster as one component of your service mesh. $ kubectl apply -f compute-interest-api.yaml service \"compute-interest-api\" created deployment \"compute-interest-api\" created 5. Deploy send-notification service \u00b6 Microservice send-notification is written in Spring Boot. It's deployed to your cluster as one component of your service mesh. $ kubectl apply -f send-notification.yaml service \"send-notification\" created deployment \"send-notification\" created 6. Deploy account-summary service - the Frontend User Interface \u00b6 The Frontend User Interface is a Node.js app serving static files (HTML, CSS, JavaScript) that shows the total account balance. It's another component of your service mesh. $ kubectl apply -f account-summary.yaml service \"account-summary\" created deployment \"account-summary\" created 7. Deploy transaction-generator service - the Transaction Generator service \u00b6 The transaction generator is a Python application that generates random transactions with accumulated interest. It's the last piece of your service mesh. $ kubectl apply -f transaction-generator.yaml service \"transaction-generator\" created deployment \"transaction-generator\" created 8. Access Your Application \u00b6 One way to access your application is through Public IP and NodePort . Locate public IP address $ ibmcloud ks workers <your k8s cluster> ID Public IP Private IP Machine Type State Status kube-dal10-paac005a5fa6c44786b5dfb3ed8728548f-w1 169 .47.241.213 10 .177.155.13 free normal Ready Take note of Public IP . 169.47.241.213 in this example. Locate NodePort $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ...... account-summary 10 .10.10.74 <nodes> 80 :30080/TCP 2d ...... Take note of PORT(S) of the service account-summary . 30080 in the example. Access your application To test your application, go to http://<Public IP>:<Port> in your browser. For example, http://169.47.241.213:30080 in this example. Explore the Kubernetes Dashboard \u00b6 Great, your application is deployed and working. There are quite a few microservices within your application. As your application is working, you would expect that all services are working. Are they? Let's find out. After you have deployed all services, the Kubernetes Dashboard can provide an overview of your application and its components. Login to IBM Cloud. Locate and select your Kubernetes cluster. Click the Kubernetes Daskboard button. Kubernetes Dashboard window opens. The charts in Workloads Statuses section on the Overview page provides a high level view of your cluster status. They are color-coded. RED indicates major issue. Explore section Deployments , Pods and Replica Sets , they all indicate that the service send-notification failed. Navigate to different pages in the Kubernetes Dashboard and you may find specific information that may be more interesting to you. Debug Deployment \u00b6 One of the cloud native architecture benefits is that your application can still function even individual services are not working. As your observed in the previous sections, your application appears working fine before you identified the down service in the Kubernetes Dashboard . In this section, you learn the very basic debugging technics in the Kubernetes Dashboard . Select the Overview page in the Kubernetes Dashboard . Select send-notification-xxxxxxx entry in the Replica Sets section. This opened send-notification-xxxxxxx entry in the Replica Sets page. Click the LOGS link on the top of the page. Scan the log entries and you should find a section similar to the one below. It shows that Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" . 2019-08-20 18:07:24.209 WARN 14 --- [ main] ationConfigEmbeddedWebApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'triggerEmail': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" 2019-08-20 18:07:24.212 INFO 14 --- [ main] o.apache.catalina.core.StandardService : Stopping service Tomcat 2019-08-20 18:07:24.228 INFO 14 --- [ main] utoConfigurationReportLoggingInitializer : Error starting ApplicationContext. To display the auto-configuration report re-run your application with 'debug' enabled. 2019-08-20 18:07:24.236 ERROR 14 --- [ main] o.s.boot.SpringApplication : Application startup failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'triggerEmail': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" The service send-notification failed because it can't resolve environment variable OPENWHISK_API_URL_SLACK . There are multiple ways to fix the problem. If you prefer, you may edit the deployment.yaml file in the Kubernetes Dashboard window. For the lab exercise, you fix the problem in the original .yaml file. Locate and open file send-notification.yaml at the root folder of spring-boot-microservices-on-kubernetes repo that you downloaded. Locate the section below. --- - name: OPENWHISK_API_URL_SLACK --- value: '' --- - name: SLACK_MESSAGE --- value: '' Uncomment the entries by removing the leading --- . Save. Apply the changes. $ kubectl apply -f send-notification.yaml service/send-notification unchanged deployment.extensions/send-notification configured Go back to the Overview page of the Kubernetes Dashboard . All services are working now. Clean up \u00b6 To delete everything created during this session, kubectl delete svc,deploy -l app=office-space References \u00b6 John Zaccone - The original author of the office space app deployed via Docker . The Office Space app is based on the 1999 film that used that concept. License \u00b6 This code pattern is licensed under the Apache Software License, Version 2. Separate third party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the Developer Certificate of Origin, Version 1.1 (DCO) and the Apache Software License, Version 2 . Apache Software License (ASL) FAQ","title":"Deploying Microservices"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#deploying-microservices","text":"","title":"Deploying Microservices"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#deploy-office-space-sample-application-on-kubernetes","text":"In this session, we demonstrate how a sample cloud native application can be deployed on top of Kubernetes. This application, Office Space, mimicks the fictitious app idea from Michael Bolton in the movie Office Space . The app takes advantage of a financial program that computes interest for transactions by diverting fractions of a cent that are usually rounded off into a seperate bank account. The application includes a few services developed in different languages. * The key coponent of the application is a Java 8/Spring Boot microservice that computes the interest then takes the fraction of the pennies to a database. * Another Spring Boot microservice is the notification service. It sends email when the account balance reach more than $50,000. It is triggered by the Spring Boot webserver that computes the interest. * The frontend user interafce is a Node.js application that shows the current account balance accumulated by the Spring Boot app. * The backend uses a MySQL database to store the account balance. * The transaction generator is a Python application that generates random transactions with accumulated interest. It's the last piece of your service mesh and used to simulate the transaction activities. The instructions were adapted from the more comprehensive tutorial found here - https://github.com/IBM/spring-boot-microservices-on-kubernetes.","title":"Deploy Office Space sample application on Kubernetes"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#develop-microservices","text":"During this session, we focus on deploying the microservices due to time constraint. Feel free to review the source code of the services, if you are interested. Couple of microservices were developed in Spring Boot which is one of the popular Java microservices framework. Spring Cloud has a rich set of well integrated Java libraries to address runtime concerns as part of the Java application stack, and Kubernetes provides a rich featureset to run polyglot microservices. Together these technologies complement each other and make a great platform for Spring Boot applications. Other microservices were developed in Node.js and Python . MySQL Database running in a separate container serves as persistent store. As a whole, the sample application delivers a native cloud architecture and follows 12 factors best practice. The source code are in the following subfolders * containers/compute-interest-api * containers/send-notification * containers/account-summary * containers/transaction-generator","title":"Develop Microservices"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#flow","text":"The Transaction Generator service written in Python simulates transactions and pushes them to the Compute Interest microservice. The Compute Interest microservice computes the interest and then moves the fraction of pennies to the MySQL database to be stored. The database can be running within a container in the same deployment or on a public cloud such as IBM Cloud. The Compute Interest microservice then calls the notification service to notify the user when the total amount in the user\u2019s account reaches $50,000. The Notification service uses IBM Cloud Function to send an email message to the user. The front end user interface in Node.js retrieves the account balance and display.","title":"Flow"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#included-components","text":"IBM Cloud Kubernetes Service : IBM Bluemix Container Service manages highly available apps inside Docker containers and Kubernetes clusters on the IBM Cloud. Compose for MySQL : Probably the most popular open source relational database in the world. IBM Cloud Functions : Execute code on demand in a highly scalable, serverless environment.","title":"Included Components"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#featured-technologies","text":"Container Orchestration : Automating the deployment, scaling and management of containerized applications. Databases : Repository for storing and managing collections of data. Serverless : An event-action platform that allows you to execute code in response to an event.","title":"Featured Technologies"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#steps","text":"Clone the repo Modify send-notification.yaml file for email notification Deploy Database MySQL service Deploy Microservice compute-interest-api Deploy Microservice send-notification Deploy Microservice account-summary - the Frontend User Interface Deploy Microservice transaction-generator - the Transaction Generator service Access Your Application Each service in the application run in their containers. It has a Deployment and a Service. The deployment manages the pods started for each microservice. The Service creates a stable DNS entry for each microservice so they can reference their dependencies by name.","title":"Steps"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#1-clone-the-repo","text":"Clone this repository. In a terminal, run: $ cd ~ $ git clone https://github.com/lee-zhg/spring-boot-microservices-on-kubernetes $ cd spring-boot-microservices-on-kubernetes","title":"1. Clone the repo"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#2-modify-send-notificationyaml-file-for-email-notification","text":"Note: Additional Gmail security configurations may be required by Gmail to send/received email in this way. Optionally, if you like to send and receive email (gmail) notification, You will need to modify the environment variables in the send-notification.yaml file: env : - name : GMAIL_SENDER_USER value : 'username@gmail.com' # change this to the gmail that will send the email - name : GMAIL_SENDER_PASSWORD value : 'password' # change this to the the password of the gmail above - name : EMAIL_RECEIVER value : 'sendTo@gmail.com' # change this to the email of the receiver","title":"2. Modify send-notification.yaml file for email notification"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#3-deploy-mysql-database","text":"Deploy MySQL database $ kubectl create -f account-database.yaml service \"account-database\" created deployment \"account-database\" created Create a secure storing credential of MySQL database Default credentials are already encoded in base64 in secrets.yaml. Note: Encoding in base64 does not encrypt or hide your secrets. Do not put this in your Github. $ kubectl apply -f secrets.yaml secret \"demo-credentials\" created","title":"3. Deploy MySQL Database"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#4-deploy-compute-interest-api-service","text":"Microservice compute-interest-api is written in Spring Boot. It's deployed to your cluster as one component of your service mesh. $ kubectl apply -f compute-interest-api.yaml service \"compute-interest-api\" created deployment \"compute-interest-api\" created","title":"4. Deploy compute-interest-api service"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#5-deploy-send-notification-service","text":"Microservice send-notification is written in Spring Boot. It's deployed to your cluster as one component of your service mesh. $ kubectl apply -f send-notification.yaml service \"send-notification\" created deployment \"send-notification\" created","title":"5. Deploy send-notification service"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#6-deploy-account-summary-service-the-frontend-user-interface","text":"The Frontend User Interface is a Node.js app serving static files (HTML, CSS, JavaScript) that shows the total account balance. It's another component of your service mesh. $ kubectl apply -f account-summary.yaml service \"account-summary\" created deployment \"account-summary\" created","title":"6. Deploy account-summary service - the Frontend User Interface"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#7-deploy-transaction-generator-service-the-transaction-generator-service","text":"The transaction generator is a Python application that generates random transactions with accumulated interest. It's the last piece of your service mesh. $ kubectl apply -f transaction-generator.yaml service \"transaction-generator\" created deployment \"transaction-generator\" created","title":"7. Deploy transaction-generator service - the Transaction Generator service"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#8-access-your-application","text":"One way to access your application is through Public IP and NodePort . Locate public IP address $ ibmcloud ks workers <your k8s cluster> ID Public IP Private IP Machine Type State Status kube-dal10-paac005a5fa6c44786b5dfb3ed8728548f-w1 169 .47.241.213 10 .177.155.13 free normal Ready Take note of Public IP . 169.47.241.213 in this example. Locate NodePort $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE ...... account-summary 10 .10.10.74 <nodes> 80 :30080/TCP 2d ...... Take note of PORT(S) of the service account-summary . 30080 in the example. Access your application To test your application, go to http://<Public IP>:<Port> in your browser. For example, http://169.47.241.213:30080 in this example.","title":"8. Access Your Application"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#explore-the-kubernetes-dashboard","text":"Great, your application is deployed and working. There are quite a few microservices within your application. As your application is working, you would expect that all services are working. Are they? Let's find out. After you have deployed all services, the Kubernetes Dashboard can provide an overview of your application and its components. Login to IBM Cloud. Locate and select your Kubernetes cluster. Click the Kubernetes Daskboard button. Kubernetes Dashboard window opens. The charts in Workloads Statuses section on the Overview page provides a high level view of your cluster status. They are color-coded. RED indicates major issue. Explore section Deployments , Pods and Replica Sets , they all indicate that the service send-notification failed. Navigate to different pages in the Kubernetes Dashboard and you may find specific information that may be more interesting to you.","title":"Explore the Kubernetes Dashboard"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#debug-deployment","text":"One of the cloud native architecture benefits is that your application can still function even individual services are not working. As your observed in the previous sections, your application appears working fine before you identified the down service in the Kubernetes Dashboard . In this section, you learn the very basic debugging technics in the Kubernetes Dashboard . Select the Overview page in the Kubernetes Dashboard . Select send-notification-xxxxxxx entry in the Replica Sets section. This opened send-notification-xxxxxxx entry in the Replica Sets page. Click the LOGS link on the top of the page. Scan the log entries and you should find a section similar to the one below. It shows that Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" . 2019-08-20 18:07:24.209 WARN 14 --- [ main] ationConfigEmbeddedWebApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'triggerEmail': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" 2019-08-20 18:07:24.212 INFO 14 --- [ main] o.apache.catalina.core.StandardService : Stopping service Tomcat 2019-08-20 18:07:24.228 INFO 14 --- [ main] utoConfigurationReportLoggingInitializer : Error starting ApplicationContext. To display the auto-configuration report re-run your application with 'debug' enabled. 2019-08-20 18:07:24.236 ERROR 14 --- [ main] o.s.boot.SpringApplication : Application startup failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'triggerEmail': Injection of autowired dependencies failed; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'OPENWHISK_API_URL_SLACK' in value \"${OPENWHISK_API_URL_SLACK}\" The service send-notification failed because it can't resolve environment variable OPENWHISK_API_URL_SLACK . There are multiple ways to fix the problem. If you prefer, you may edit the deployment.yaml file in the Kubernetes Dashboard window. For the lab exercise, you fix the problem in the original .yaml file. Locate and open file send-notification.yaml at the root folder of spring-boot-microservices-on-kubernetes repo that you downloaded. Locate the section below. --- - name: OPENWHISK_API_URL_SLACK --- value: '' --- - name: SLACK_MESSAGE --- value: '' Uncomment the entries by removing the leading --- . Save. Apply the changes. $ kubectl apply -f send-notification.yaml service/send-notification unchanged deployment.extensions/send-notification configured Go back to the Overview page of the Kubernetes Dashboard . All services are working now.","title":"Debug Deployment"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#clean-up","text":"To delete everything created during this session, kubectl delete svc,deploy -l app=office-space","title":"Clean up"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#references","text":"John Zaccone - The original author of the office space app deployed via Docker . The Office Space app is based on the 1999 film that used that concept.","title":"References"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_deployment/#license","text":"This code pattern is licensed under the Apache Software License, Version 2. Separate third party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the Developer Certificate of Origin, Version 1.1 (DCO) and the Apache Software License, Version 2 . Apache Software License (ASL) FAQ","title":"License"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_java/","text":"Develop a Java Microservice with Spring Boot \u00b6 IBM Cloud provides Starter Kits as the starting point of quick cloud native development and deployment. For example * Java Microservice with Spring * Java Microservice with Eclipse MicroProfile and Java EE * Node.js Microservice with Express.js * Python Microservice with Flask In this session, you develop a Java Microservice using the Java Microservice with Spring starter kit. With this kit, you can have a basic Java microservice developed and deployed in a few minutes. You create a CI/CD pipeline in IBM Cloud which automates your service's deployment to Kubernetes cluster. The Java source code is available in the Git repo that is created as part of the CI/CD pipeline. It can serve as a starting point of your development project. You can download the repo to your local machine, develop and test your service locally, then check in the complete version of your project back to your Git repo. This will trigger a re-build and re-deployment process. The latest version of your microservice is deployed and running in Kubernetes cluster. Java Microservice with Spring Boot starter kit is used as an example to jump start a development project of Java Microservice . Other starter kits in the IBM Cloud work similarly. If you prefer other language or framework in your project, the steps in this session can still be applied. The instructions were adapted from the more comprehensive tutorial found here - https://cloud.ibm.com/docs/apps/tutorials?topic=creating-apps-tutorial-scratch#prereqs-scratch. Create a Java Microservice with Spring Boot \u00b6 To createa a Java Microservice with Spring Boot using the starter kit, Login to IBM Cloud. Select Catalog in your account dashboard. Go to Software tab. Search and select the Java Spring App . Click Create app . Accept the default settings and click Create . Create a Continuous Delivery pipeline \u00b6 Continuous Delivery automates builds, tests, and deployments of your service through Delivery Pipeline, GitLab, and more. By default, it is not enabled. To enable the continous delivery for your service, On the App details page, click the Deploy your app button in the Configure continuous delivery section. Select the IBM Cloud Kubernetes Service option. Click New to generate a new IBM Cloud API key . Click OK when prompted. Select the desired Region and Cluster . Select the Region to create your toolchain in, and then select the Resource group that provides access to your new toolchain. Accept the defaults for the rest of settings and Create . It may take a few minutes to complete the Continuous Delivery configuration. You can continue to the next step. The toolchain offers two main components. A Git repo is created for storing the source code of your service development project. It also provides version control service. A Deliver Pipeline is also created. It automates builds, tests, and deployments of your service. Click on the name of your toolchain. For example, JavaSpringAppXXXXXX to open the Delivery Pipeline window. The Delivery Pipeline should run through its stages. This may take a few minutes. After it completes, you should see Stage Passed messages for each stage. Verify your service \u00b6 Now, your microservice is running in your Kubernetes cluster on IBM Cloud. To find out how your servoice can be accessed, execute commands below $ ibmcloud ks workers <your cluster> $ kubectl get svc Locate the Public IP and Port(s) . To verify your microservice, enter http: : in a browser.","title":"Develop a Java Microservice with Spring Boot"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_java/#develop-a-java-microservice-with-spring-boot","text":"IBM Cloud provides Starter Kits as the starting point of quick cloud native development and deployment. For example * Java Microservice with Spring * Java Microservice with Eclipse MicroProfile and Java EE * Node.js Microservice with Express.js * Python Microservice with Flask In this session, you develop a Java Microservice using the Java Microservice with Spring starter kit. With this kit, you can have a basic Java microservice developed and deployed in a few minutes. You create a CI/CD pipeline in IBM Cloud which automates your service's deployment to Kubernetes cluster. The Java source code is available in the Git repo that is created as part of the CI/CD pipeline. It can serve as a starting point of your development project. You can download the repo to your local machine, develop and test your service locally, then check in the complete version of your project back to your Git repo. This will trigger a re-build and re-deployment process. The latest version of your microservice is deployed and running in Kubernetes cluster. Java Microservice with Spring Boot starter kit is used as an example to jump start a development project of Java Microservice . Other starter kits in the IBM Cloud work similarly. If you prefer other language or framework in your project, the steps in this session can still be applied. The instructions were adapted from the more comprehensive tutorial found here - https://cloud.ibm.com/docs/apps/tutorials?topic=creating-apps-tutorial-scratch#prereqs-scratch.","title":"Develop a Java Microservice with Spring Boot"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_java/#create-a-java-microservice-with-spring-boot","text":"To createa a Java Microservice with Spring Boot using the starter kit, Login to IBM Cloud. Select Catalog in your account dashboard. Go to Software tab. Search and select the Java Spring App . Click Create app . Accept the default settings and click Create .","title":"Create a Java Microservice with Spring Boot"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_java/#create-a-continuous-delivery-pipeline","text":"Continuous Delivery automates builds, tests, and deployments of your service through Delivery Pipeline, GitLab, and more. By default, it is not enabled. To enable the continous delivery for your service, On the App details page, click the Deploy your app button in the Configure continuous delivery section. Select the IBM Cloud Kubernetes Service option. Click New to generate a new IBM Cloud API key . Click OK when prompted. Select the desired Region and Cluster . Select the Region to create your toolchain in, and then select the Resource group that provides access to your new toolchain. Accept the defaults for the rest of settings and Create . It may take a few minutes to complete the Continuous Delivery configuration. You can continue to the next step. The toolchain offers two main components. A Git repo is created for storing the source code of your service development project. It also provides version control service. A Deliver Pipeline is also created. It automates builds, tests, and deployments of your service. Click on the name of your toolchain. For example, JavaSpringAppXXXXXX to open the Delivery Pipeline window. The Delivery Pipeline should run through its stages. This may take a few minutes. After it completes, you should see Stage Passed messages for each stage.","title":"Create a Continuous Delivery pipeline"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_java/#verify-your-service","text":"Now, your microservice is running in your Kubernetes cluster on IBM Cloud. To find out how your servoice can be accessed, execute commands below $ ibmcloud ks workers <your cluster> $ kubectl get svc Locate the Public IP and Port(s) . To verify your microservice, enter http: : in a browser.","title":"Verify your service"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_pre02/","text":"Lab Environment Setup \u00b6 Every time when you start a new terminal/command window, steps in the section must be performed to setup a new environment. Run these steps inside the console-in-a-browser environment provided by your instructor, or in a terminal/command window on your local machine. Open a new terminal ort command window. Login to the IBM Cloud. If prompted, enter user name and password. ibmcloud login List the clusters and locate the cluster corresponding to the userId you used to login to the console-in-a-browser environment. For example, if you are user028, your cluster will be user028-cluster. ibmcloud ks clusters Your Lite cluster should show up on the list. The cluster name should be in the format of user###-cluster , for example user003-cluster . Configure your Kubernetes client using this command. This will also configure your Kubernetes client for future login sessions by adding the command into your .bash_profile. eval $( ibmcloud ks cluster-config --cluster <your k8s cluster> --export | tee -a ~/.bash_profile ) If the command has a syntax error in your terminal (e.g. windows cmd shell), you may instead run the command ibmcloud ks cluster-config --cluster <your user>-cluster . Then, copy the output and execute it in the same terminal. You should be able to use kubectl to list kubernetes resources. Try getting the list of pods (there should be none yet) kubectl get pods No resources found. Login to the registry service ibmcloud cr login","title":"Lab Environment Setup"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/README_pre02/#lab-environment-setup","text":"Every time when you start a new terminal/command window, steps in the section must be performed to setup a new environment. Run these steps inside the console-in-a-browser environment provided by your instructor, or in a terminal/command window on your local machine. Open a new terminal ort command window. Login to the IBM Cloud. If prompted, enter user name and password. ibmcloud login List the clusters and locate the cluster corresponding to the userId you used to login to the console-in-a-browser environment. For example, if you are user028, your cluster will be user028-cluster. ibmcloud ks clusters Your Lite cluster should show up on the list. The cluster name should be in the format of user###-cluster , for example user003-cluster . Configure your Kubernetes client using this command. This will also configure your Kubernetes client for future login sessions by adding the command into your .bash_profile. eval $( ibmcloud ks cluster-config --cluster <your k8s cluster> --export | tee -a ~/.bash_profile ) If the command has a syntax error in your terminal (e.g. windows cmd shell), you may instead run the command ibmcloud ks cluster-config --cluster <your user>-cluster . Then, copy the output and execute it in the same terminal. You should be able to use kubectl to list kubernetes resources. Try getting the list of pods (there should be none yet) kubectl get pods No resources found. Login to the registry service ibmcloud cr login","title":"Lab Environment Setup"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/images/README_con_k8s/","text":"Lab 0c. Connect to your kubernetes environment \u00b6 Complete steps in this section to connect to your Kubernetes cluster. Open a terminal window or command window. Note: use a web terminal if you attend a workshop Set an environment variable export USERNAME=user### Note: replace ### with your assigned ID Install the kubernetes and container registry plugins: ibmcloud plugin install kubernetes-service -r Bluemix ibmcloud plugin install container-registry -r Bluemix Login to Kubernetes service in IBM Cloud . Select your Kubernetes cluster. Navigate to Access tab. In the above terminal window, complete all steps in the section After your cluster provisions, gain access .","title":"Lab 0c. Connect to your kubernetes environment"},{"location":"generatedContent/spring-boot-microservices-on-kubernetes/images/README_con_k8s/#lab-0c-connect-to-your-kubernetes-environment","text":"Complete steps in this section to connect to your Kubernetes cluster. Open a terminal window or command window. Note: use a web terminal if you attend a workshop Set an environment variable export USERNAME=user### Note: replace ### with your assigned ID Install the kubernetes and container registry plugins: ibmcloud plugin install kubernetes-service -r Bluemix ibmcloud plugin install container-registry -r Bluemix Login to Kubernetes service in IBM Cloud . Select your Kubernetes cluster. Navigate to Access tab. In the above terminal window, complete all steps in the section After your cluster provisions, gain access .","title":"Lab 0c. Connect to your kubernetes environment"},{"location":"generatedContent/workshop-setup/","text":"Advocacy Workshop \u00b6 This repo as generated using a template for workshops hosted on Gitbook. To view it online, go to: https://ibm-developer.gitbook.io/workshop-template/ Create a new repo based off this template, and use the folowing folders as a guide: - data (any data (CSV, JSON, etc files) to be used) - notebooks (any Jupyter notebooks can go here) - src (any application source code can go here) - workshop (this is where the workshop is documented) |_ .gitbook (images should go here) |_ <folder-n> (these are exercises for the workshop) |_README.md (the steps for the exercise, in Markdown) |_ README.md (this will appear on the gitbook home page) |_ SUMMARY.md (this dictates the Table of Contents) .gitbook.yaml (tells GitBook to only read the stuff in 'workshop') .travis.yaml (runs markdownlint by default) README.md (only used for GitHub.com) Tips and conventions \u00b6 Screenshots \u00b6 Screenshots look better if they are full page. Use ImageMagick to create a nice border around images with this command: magick mogrify -bordercolor gray -border 2 Design in GitBook \u00b6 To ensure a common design for IBM workshops please use following options: Select in Theme & Colors the theme: Bold and the color: #1C6DBC Select in Logo & Title the IBM logo Understand the major GitBook template structure \u00b6 It is useful to understand the used GitBook structure of this template . README.md : Each README.md dictates the Table of Contents on the right hand side. SUMMARY.md : This dictates the Table of Contents on the left hand side. root : Path to lookup for your documentation defaults to the root directory of the repository. .gitbook.yaml : You can configure how GitBook should parse your Git repository using the.gitbook.yaml file that must rely on the root of your repository. The image shows the file names and thier resonsibilities.","title":"Setup"},{"location":"generatedContent/workshop-setup/#advocacy-workshop","text":"This repo as generated using a template for workshops hosted on Gitbook. To view it online, go to: https://ibm-developer.gitbook.io/workshop-template/ Create a new repo based off this template, and use the folowing folders as a guide: - data (any data (CSV, JSON, etc files) to be used) - notebooks (any Jupyter notebooks can go here) - src (any application source code can go here) - workshop (this is where the workshop is documented) |_ .gitbook (images should go here) |_ <folder-n> (these are exercises for the workshop) |_README.md (the steps for the exercise, in Markdown) |_ README.md (this will appear on the gitbook home page) |_ SUMMARY.md (this dictates the Table of Contents) .gitbook.yaml (tells GitBook to only read the stuff in 'workshop') .travis.yaml (runs markdownlint by default) README.md (only used for GitHub.com)","title":"Advocacy Workshop"},{"location":"generatedContent/workshop-setup/#tips-and-conventions","text":"","title":"Tips and conventions"},{"location":"generatedContent/workshop-setup/#screenshots","text":"Screenshots look better if they are full page. Use ImageMagick to create a nice border around images with this command: magick mogrify -bordercolor gray -border 2","title":"Screenshots"},{"location":"generatedContent/workshop-setup/#design-in-gitbook","text":"To ensure a common design for IBM workshops please use following options: Select in Theme & Colors the theme: Bold and the color: #1C6DBC Select in Logo & Title the IBM logo","title":"Design in GitBook"},{"location":"generatedContent/workshop-setup/#understand-the-major-gitbook-template-structure","text":"It is useful to understand the used GitBook structure of this template . README.md : Each README.md dictates the Table of Contents on the right hand side. SUMMARY.md : This dictates the Table of Contents on the left hand side. root : Path to lookup for your documentation defaults to the root directory of the repository. .gitbook.yaml : You can configure how GitBook should parse your Git repository using the.gitbook.yaml file that must rely on the root of your repository. The image shows the file names and thier resonsibilities.","title":"Understand the major GitBook template structure"},{"location":"generatedContent/workshop-setup/workshop/","text":"Workshop Setup \u00b6 Install \u00b6 Sign up for a new IBM Cloud Connect to RedHat OpenShift Kubernetes Service (ROKS) Access a shell at CognitiveClass.ai Install CLI Tools Install Jenkins on OpenShift 4.3 Install Jenkins on OpenShift 3.11 Web-Terminal Client Environment Instructions Credits \u00b6 Remko de Knikker","title":"Index"},{"location":"generatedContent/workshop-setup/workshop/#workshop-setup","text":"","title":"Workshop Setup"},{"location":"generatedContent/workshop-setup/workshop/#install","text":"Sign up for a new IBM Cloud Connect to RedHat OpenShift Kubernetes Service (ROKS) Access a shell at CognitiveClass.ai Install CLI Tools Install Jenkins on OpenShift 4.3 Install Jenkins on OpenShift 3.11 Web-Terminal Client Environment Instructions","title":"Install"},{"location":"generatedContent/workshop-setup/workshop/#credits","text":"Remko de Knikker","title":"Credits"},{"location":"generatedContent/workshop-setup/workshop/SUMMARY/","text":"Summary \u00b6 Install \u00b6 Sign up for a new IBM Cloud Connect to RedHat OpenShift Kubernetes Service (ROKS) Access a shell at CognitiveClass.ai Install CLI Tools Install Jenkins on OpenShift 4.3 Install Jenkins on OpenShift 3.11 Web-Terminal Client Environment Instructions Workshop Resources \u00b6 Workshops Resources \u00b6 IBM Developer","title":"Summary"},{"location":"generatedContent/workshop-setup/workshop/SUMMARY/#summary","text":"","title":"Summary"},{"location":"generatedContent/workshop-setup/workshop/SUMMARY/#install","text":"Sign up for a new IBM Cloud Connect to RedHat OpenShift Kubernetes Service (ROKS) Access a shell at CognitiveClass.ai Install CLI Tools Install Jenkins on OpenShift 4.3 Install Jenkins on OpenShift 3.11 Web-Terminal Client Environment Instructions","title":"Install"},{"location":"generatedContent/workshop-setup/workshop/SUMMARY/#workshop-resources","text":"Workshops","title":"Workshop Resources"},{"location":"generatedContent/workshop-setup/workshop/SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"generatedContent/workshop-setup/workshop/admin-guide/","text":"Admin Guide \u00b6 This section is comprised of the following steps: Instructor Step 1. Instructor Step \u00b6 Things specific to instructors can go here.","title":"Admin Guide"},{"location":"generatedContent/workshop-setup/workshop/admin-guide/#admin-guide","text":"This section is comprised of the following steps: Instructor Step","title":"Admin Guide"},{"location":"generatedContent/workshop-setup/workshop/admin-guide/#1-instructor-step","text":"Things specific to instructors can go here.","title":"1. Instructor Step"},{"location":"generatedContent/workshop-setup/workshop/jenkins/SETUP_OCP311/","text":"Setup Jenkins on OpenShift 3 \u00b6 From the IBM Cloud cluster dashboard, open the OpenShift web console, Go to the Service Catalog , First we need to create a project named jenkins to deploy the Jenkins service to, Click the + Create Project button, For Name enter jenkins , For Desription enter Project for Jenkins, Click Create, Or from the cloud shell, $ oc new-project jenkins Now using project \"jenkins\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:31608\". Go back to the Service Catalog, Search the catalog for Jenkins (Ephemeral) or go to the CD/CD tab, Click the Jenkins (Ephemeral) service, note that the ephemeral version does not include persistent storage, so your data will be lost if the pod goes down, but for this tutorial it is easier to setup, For Add to Project select jenkins , Click Create, If a popup Confirm Creation appears, click Create Anyway , Click Close while Jenkins is being provisioned in jenkins , Select the Application Console , and go to your project jenkins , Go to Overview, and wait until Jenkins has been provisioned, the pod should color bright blue, If provisioning fails the first time because of a timeout during pulling of images, simply deploy the service again, To see the log for a deployment #1, go to Applications > Deployments > Jenkins > #1, Click the Logs tab, Or go to Monitoring to see a detailed status, Open Jenkins in a new tab by clicking the Jenkins route URL in the Overview , e.g. https://jenkins-ci1.cda-openshift-cluster-1c0e8bfb1c68214cf875a9ca7dd1e060-0001.us-south.containers.appdomain.cloud, Click Log in with OpenShift, Click Allow selected permissions , Jenkins web will open, Configure Jenkins Go to Jenkins > Manage Jenkins > Global Tool Configuration, Go to the Maven section, Click Maven Installations , If no Maven installer is configured, click Add Maven , Configure the Name to be maven , check the option Install automatically and select version 3.6.3 , Click Save, Go back to Lab01 to continue the Jenkins setup.","title":"Setup Jenkins on OpenShift 3"},{"location":"generatedContent/workshop-setup/workshop/jenkins/SETUP_OCP311/#setup-jenkins-on-openshift-3","text":"From the IBM Cloud cluster dashboard, open the OpenShift web console, Go to the Service Catalog , First we need to create a project named jenkins to deploy the Jenkins service to, Click the + Create Project button, For Name enter jenkins , For Desription enter Project for Jenkins, Click Create, Or from the cloud shell, $ oc new-project jenkins Now using project \"jenkins\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:31608\". Go back to the Service Catalog, Search the catalog for Jenkins (Ephemeral) or go to the CD/CD tab, Click the Jenkins (Ephemeral) service, note that the ephemeral version does not include persistent storage, so your data will be lost if the pod goes down, but for this tutorial it is easier to setup, For Add to Project select jenkins , Click Create, If a popup Confirm Creation appears, click Create Anyway , Click Close while Jenkins is being provisioned in jenkins , Select the Application Console , and go to your project jenkins , Go to Overview, and wait until Jenkins has been provisioned, the pod should color bright blue, If provisioning fails the first time because of a timeout during pulling of images, simply deploy the service again, To see the log for a deployment #1, go to Applications > Deployments > Jenkins > #1, Click the Logs tab, Or go to Monitoring to see a detailed status, Open Jenkins in a new tab by clicking the Jenkins route URL in the Overview , e.g. https://jenkins-ci1.cda-openshift-cluster-1c0e8bfb1c68214cf875a9ca7dd1e060-0001.us-south.containers.appdomain.cloud, Click Log in with OpenShift, Click Allow selected permissions , Jenkins web will open, Configure Jenkins Go to Jenkins > Manage Jenkins > Global Tool Configuration, Go to the Maven section, Click Maven Installations , If no Maven installer is configured, click Add Maven , Configure the Name to be maven , check the option Install automatically and select version 3.6.3 , Click Save, Go back to Lab01 to continue the Jenkins setup.","title":"Setup Jenkins on OpenShift 3"},{"location":"generatedContent/workshop-setup/workshop/jenkins/SETUP_OCP43/","text":"Setup Jenkins on OpenShift 4.3 \u00b6 Pre-requirements: - OpenShift 4.3 cluster From the IBM Cloud cluster dashboard, click the OpenShift web console button, First we need to create a new project named jenkins to deploy the Jenkins service to, From the terminal, oc new-project jenkins outputs, $ oc new-project jenkins Now using project \"jenkins\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:31608\". Or in the Openshift web console, go to Home > Projects , Click Create Project For Name enter jenkins , for Display Name enter jenkins , and for Description enter jenkins , Click Create , Go to Operators > OperatorHub , For Filter by keyword enter Jenkins , Select the Jenkins Operator by Community , provided by Red Hat , Click Continue to Show Community Operator , Review the operator information, and click Install , In the Create Operator Subscription window, choose A specific namespace in the cluster and select the project jenkins , select alpha under Update Channel , select Automatic under Approval Strategy , Click Subscribe , The Installed Operators page will load, wait until the Jenkins Operator has a Status of Succeeded , Click the installed operator linked Name of Jenkins Operator , In the Provided APIs section, click the Create Instance link in the Jenkins panel, In the Create Jenkins YAML definition for the new Jenkins instance, change the metadata.name to my-jenkins , accept all other specifications, Click Create , Go to Networking > Routes , and look for a new Route jenkins-my-jenkins , Click the link for jenkins-my-jenkins route in the Location column, A route to your Jenkins instance opens in a new browser window or tab, Click Log in with OpenShift , ![](../.gitbook/jenkins/login-with-openshift Click Allow selected permissions , Welcome to Jenkins ! Configure Jenkins Go to Jenkins > Manage Jenkins > Global Tool Configuration, Go to the Maven section, Click Maven Installations , If no Maven installer is configured, click Add Maven , Configure the Name to be maven , check the option Install automatically and select version 3.6.3 , Click Save,","title":"Setup Jenkins on OpenShift 4.3"},{"location":"generatedContent/workshop-setup/workshop/jenkins/SETUP_OCP43/#setup-jenkins-on-openshift-43","text":"Pre-requirements: - OpenShift 4.3 cluster From the IBM Cloud cluster dashboard, click the OpenShift web console button, First we need to create a new project named jenkins to deploy the Jenkins service to, From the terminal, oc new-project jenkins outputs, $ oc new-project jenkins Now using project \"jenkins\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:31608\". Or in the Openshift web console, go to Home > Projects , Click Create Project For Name enter jenkins , for Display Name enter jenkins , and for Description enter jenkins , Click Create , Go to Operators > OperatorHub , For Filter by keyword enter Jenkins , Select the Jenkins Operator by Community , provided by Red Hat , Click Continue to Show Community Operator , Review the operator information, and click Install , In the Create Operator Subscription window, choose A specific namespace in the cluster and select the project jenkins , select alpha under Update Channel , select Automatic under Approval Strategy , Click Subscribe , The Installed Operators page will load, wait until the Jenkins Operator has a Status of Succeeded , Click the installed operator linked Name of Jenkins Operator , In the Provided APIs section, click the Create Instance link in the Jenkins panel, In the Create Jenkins YAML definition for the new Jenkins instance, change the metadata.name to my-jenkins , accept all other specifications, Click Create , Go to Networking > Routes , and look for a new Route jenkins-my-jenkins , Click the link for jenkins-my-jenkins route in the Location column, A route to your Jenkins instance opens in a new browser window or tab, Click Log in with OpenShift , ![](../.gitbook/jenkins/login-with-openshift Click Allow selected permissions , Welcome to Jenkins ! Configure Jenkins Go to Jenkins > Manage Jenkins > Global Tool Configuration, Go to the Maven section, Click Maven Installations , If no Maven installer is configured, click Add Maven , Configure the Name to be maven , check the option Install automatically and select version 3.6.3 , Click Save,","title":"Setup Jenkins on OpenShift 4.3"},{"location":"generatedContent/workshop-setup/workshop/pre-work/","text":"Pre-work \u00b6 This section is broken up into the following steps: Sign up for a new IBM Cloud Connect to RedHat OpenShift Kubernetes Service (ROKS) Access a shell at CognitiveClass.ai Install CLI Tools","title":"Pre-work"},{"location":"generatedContent/workshop-setup/workshop/pre-work/#pre-work","text":"This section is broken up into the following steps: Sign up for a new IBM Cloud Connect to RedHat OpenShift Kubernetes Service (ROKS) Access a shell at CognitiveClass.ai Install CLI Tools","title":"Pre-work"},{"location":"generatedContent/workshop-setup/workshop/pre-work/CLOUDSHELL/","text":"","title":"CLOUDSHELL"},{"location":"generatedContent/workshop-setup/workshop/pre-work/COGNITIVECLASS/","text":"CognitiveClass.ai \u00b6 Access CognitiveClass.ai \u00b6 If you have already registered your account, you can access the lab environment at https://labs.cognitiveclass.ai/ and login. Navigate to https://labs.cognitiveclass.ai/register, Create a new account with your existing IBM Id. Alternative, you can choose to use a Social login (LinkedIn, Google, Github or Facebook), or for using your email account click the Cognitive Class button, Click Create an Account , Fill in your Email, Full Name, Public Username and password, click on the check boxes next to the Privacy Notice and Terms of Service to accept them. Then click on Create Account . You will then be taken to a page with a list of sandbox environments. Click on the option for Theia - Cloud IDE (With OpenShift) Wait a few minutes while your environment is created. You will be taken to a blank editor page once your environment is ready. What we really need is access to the terminal. Click on the Terminal tab near the top of the page and select New Terminal You can then click and drag the top of the terminal section upwards to make the terminal section bigger.","title":"CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/workshop/pre-work/COGNITIVECLASS/#cognitiveclassai","text":"","title":"CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/workshop/pre-work/COGNITIVECLASS/#access-cognitiveclassai","text":"If you have already registered your account, you can access the lab environment at https://labs.cognitiveclass.ai/ and login. Navigate to https://labs.cognitiveclass.ai/register, Create a new account with your existing IBM Id. Alternative, you can choose to use a Social login (LinkedIn, Google, Github or Facebook), or for using your email account click the Cognitive Class button, Click Create an Account , Fill in your Email, Full Name, Public Username and password, click on the check boxes next to the Privacy Notice and Terms of Service to accept them. Then click on Create Account . You will then be taken to a page with a list of sandbox environments. Click on the option for Theia - Cloud IDE (With OpenShift) Wait a few minutes while your environment is created. You will be taken to a blank editor page once your environment is ready. What we really need is access to the terminal. Click on the Terminal tab near the top of the page and select New Terminal You can then click and drag the top of the terminal section upwards to make the terminal section bigger.","title":"Access CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/workshop/pre-work/GRANTCLUSTER/","text":"Grant Cluster \u00b6 IBM Kubernetes Service (IKS) and RedHat OpenShift Kubernetes Service (ROKS) \u00b6 The grant cluster method to get access to a Kubernetes cluster will assign access permissions to a cluster or namespace in a cluster that was created prior to the request. Creating a cluster and provisioning the VMs and other resources and deploying the tools may take up to 15 minutes and longer if queued. Permissioning access to an existing cluster in contrast happens in 1 or 2 minutes depending on the number of concurrent requests. You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, register at https://cloud.ibm.com/registration, Or find instructions to create a new IBM Cloud account here , To grant a cluster, You need to be given a URL to submit your grant cluster request, Open the URL to grant a cluster, e.g. https://<workshop>.mybluemix.net , The grant cluster URL should open the following page, Log in to this IBM Cloud account using the lab key given to you by the instructor and your IBM Id to access your IBM Cloud account, Instructions will ask to Log in to this IBM Cloud account When you click the link to log in to the IBM Cloud account, the IBM Cloud overview page will load with an overview of all resources on the account. In the top right, you will see an active account listed. The active account should be the account on which the cluster is created, which is not your personal account. Click the account dropdown if you need to change the active account. Navigate to Clusters, And select the cluster assigned to you... Details for your cluster will load, Go to the Access menu item in the left navigation column, Follow the instructions to access your cluster from the client, Optionally, you can use the IBM Cloud Shell at https://shell.cloud.ibm.com/ to check access. The cloud shell is attached to your IBM Id. It might take a few moments to create the instance and a new session,","title":"Grant Cluster"},{"location":"generatedContent/workshop-setup/workshop/pre-work/GRANTCLUSTER/#grant-cluster","text":"","title":"Grant Cluster"},{"location":"generatedContent/workshop-setup/workshop/pre-work/GRANTCLUSTER/#ibm-kubernetes-service-iks-and-redhat-openshift-kubernetes-service-roks","text":"The grant cluster method to get access to a Kubernetes cluster will assign access permissions to a cluster or namespace in a cluster that was created prior to the request. Creating a cluster and provisioning the VMs and other resources and deploying the tools may take up to 15 minutes and longer if queued. Permissioning access to an existing cluster in contrast happens in 1 or 2 minutes depending on the number of concurrent requests. You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, register at https://cloud.ibm.com/registration, Or find instructions to create a new IBM Cloud account here , To grant a cluster, You need to be given a URL to submit your grant cluster request, Open the URL to grant a cluster, e.g. https://<workshop>.mybluemix.net , The grant cluster URL should open the following page, Log in to this IBM Cloud account using the lab key given to you by the instructor and your IBM Id to access your IBM Cloud account, Instructions will ask to Log in to this IBM Cloud account When you click the link to log in to the IBM Cloud account, the IBM Cloud overview page will load with an overview of all resources on the account. In the top right, you will see an active account listed. The active account should be the account on which the cluster is created, which is not your personal account. Click the account dropdown if you need to change the active account. Navigate to Clusters, And select the cluster assigned to you... Details for your cluster will load, Go to the Access menu item in the left navigation column, Follow the instructions to access your cluster from the client, Optionally, you can use the IBM Cloud Shell at https://shell.cloud.ibm.com/ to check access. The cloud shell is attached to your IBM Id. It might take a few moments to create the instance and a new session,","title":"IBM Kubernetes Service (IKS) and RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/workshop/pre-work/NEWACCOUNT/","text":"Create IBM Cloud ID / Account \u00b6 To create a new account, follow the steps below, Open a web browser and go to the IBM Cloud Sign up page - https://ibm.biz/BdqbEh In the Create an account window, enter your company email id and the password you would like to use. Click the Next button. The Verify email section will inform you that a verification code was sent to your email. Switch to your email provider to retrieve the verification code. Then enter the verification code in the Verify email section, and click the Next button. Enter your first name, last name and country in the Personal information section and click the Next button. Click the Create account button. After your account is created, review the IBM Privacy Statement . Then scroll down and click the Proceed button to acknowledge the privacy statement. You are now ready to login to the IBM Cloud. Open a web browser to the IBM Cloud console . If prompted, enter your IBM Id (the email ID you used to create the account above) followed by your password to login. The IBM Cloud dashboard page should load. You have successfully registered a new IBM Cloud account.","title":"Create IBM Cloud ID / Account"},{"location":"generatedContent/workshop-setup/workshop/pre-work/NEWACCOUNT/#create-ibm-cloud-id-account","text":"To create a new account, follow the steps below, Open a web browser and go to the IBM Cloud Sign up page - https://ibm.biz/BdqbEh In the Create an account window, enter your company email id and the password you would like to use. Click the Next button. The Verify email section will inform you that a verification code was sent to your email. Switch to your email provider to retrieve the verification code. Then enter the verification code in the Verify email section, and click the Next button. Enter your first name, last name and country in the Personal information section and click the Next button. Click the Create account button. After your account is created, review the IBM Privacy Statement . Then scroll down and click the Proceed button to acknowledge the privacy statement. You are now ready to login to the IBM Cloud. Open a web browser to the IBM Cloud console . If prompted, enter your IBM Id (the email ID you used to create the account above) followed by your password to login. The IBM Cloud dashboard page should load. You have successfully registered a new IBM Cloud account.","title":"Create IBM Cloud ID / Account"},{"location":"generatedContent/workshop-setup/workshop/pre-work/ROKS/","text":"RedHat OpenShift Kubernetes Service (ROKS) \u00b6 Login to IBM Cloud \u00b6 To login to IBM Cloud, Go to https://cloud.ibm.com in your browser and login. Make sure that you are in the correct account#. Note: you may not have access to your OpenShift cluster if you are not in the right account#. Shell \u00b6 Most of the labs are run using CLI commands. The IBM Cloud Shell available at https://shell.cloud.ibm.com is preconfigured with the full IBM Cloud CLI and tons of plug-ins and tools that you can use to manage apps, resources, and infrastructure. Another great online shell is available via the Theia - Cloud IDE (With OpenShift) at https://labs.cognitiveclass.ai. The Cognitive Class shell comes with a Docker Engine and Helm v3 at the time of writing. Connect to RedHat OpenShift Kubernetes Service (ROKS) \u00b6 In a new browser tab, go to https://cloud.ibm.com/kubernetes/clusters?platformType=openshift. Make sure the account holding the cluster is selected, Select your cluster instance and open it. Click OpenShift web console button on the top. Click on your username in the upper right and select Copy Login Command option. Click the Display Token link. Copy the contents of the field Log in with this token to the clipboard. It provides a login command with a valid token for your username. Go to the your shell terminal. Paste the oc login command in the IBM Cloud Shell terminal and run it. Verify you connect to the right cluster. oc get all oc get nodes -o wide ![oc get nodes](../.gitbook/roks/cognitiveclass-get-nodes.png) Optionally, for convenience, set an environment variable for your cluster name. export CLUSTER_NAME = <your_cluster_name>","title":"RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/workshop/pre-work/ROKS/#redhat-openshift-kubernetes-service-roks","text":"","title":"RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/workshop/pre-work/ROKS/#login-to-ibm-cloud","text":"To login to IBM Cloud, Go to https://cloud.ibm.com in your browser and login. Make sure that you are in the correct account#. Note: you may not have access to your OpenShift cluster if you are not in the right account#.","title":"Login to IBM Cloud"},{"location":"generatedContent/workshop-setup/workshop/pre-work/ROKS/#shell","text":"Most of the labs are run using CLI commands. The IBM Cloud Shell available at https://shell.cloud.ibm.com is preconfigured with the full IBM Cloud CLI and tons of plug-ins and tools that you can use to manage apps, resources, and infrastructure. Another great online shell is available via the Theia - Cloud IDE (With OpenShift) at https://labs.cognitiveclass.ai. The Cognitive Class shell comes with a Docker Engine and Helm v3 at the time of writing.","title":"Shell"},{"location":"generatedContent/workshop-setup/workshop/pre-work/ROKS/#connect-to-redhat-openshift-kubernetes-service-roks","text":"In a new browser tab, go to https://cloud.ibm.com/kubernetes/clusters?platformType=openshift. Make sure the account holding the cluster is selected, Select your cluster instance and open it. Click OpenShift web console button on the top. Click on your username in the upper right and select Copy Login Command option. Click the Display Token link. Copy the contents of the field Log in with this token to the clipboard. It provides a login command with a valid token for your username. Go to the your shell terminal. Paste the oc login command in the IBM Cloud Shell terminal and run it. Verify you connect to the right cluster. oc get all oc get nodes -o wide ![oc get nodes](../.gitbook/roks/cognitiveclass-get-nodes.png) Optionally, for convenience, set an environment variable for your cluster name. export CLUSTER_NAME = <your_cluster_name>","title":"Connect to RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/workshop/pre-work/TOOLS-CLI/","text":"CLI TOOLS \u00b6 Helm version 3 \u00b6 Refer to the Helm install docs for more details. To install Helm v3, run the following commands, In the Cloud Shell , download and unzip Helm v3.2. cd $HOME wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > $HOME/.bash_profile source $HOME/.bash_profile Verify Helm v3 installation. helm version --short outputs, $ helm version --short v3.2.0+ge11b7ce Source-to-Image (S2I) \u00b6 To install s2i CLI tool, Download tar file. curl -s https://api.github.com/repos/openshift/source-to-image/releases/latest \\ | grep browser_download_url \\ | grep linux-amd64 \\ | cut -d '\"' -f 4 \\ | wget -qi - Unzip tar file tar xvf source-to-image*.gz Make s2i CLI accessiblee. sudo mv s2i /usr/local/bin verify s2i version With that done, you can start the lab.","title":"TOOLS CLI"},{"location":"generatedContent/workshop-setup/workshop/pre-work/TOOLS-CLI/#cli-tools","text":"","title":"CLI TOOLS"},{"location":"generatedContent/workshop-setup/workshop/pre-work/TOOLS-CLI/#helm-version-3","text":"Refer to the Helm install docs for more details. To install Helm v3, run the following commands, In the Cloud Shell , download and unzip Helm v3.2. cd $HOME wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > $HOME/.bash_profile source $HOME/.bash_profile Verify Helm v3 installation. helm version --short outputs, $ helm version --short v3.2.0+ge11b7ce","title":"Helm version 3"},{"location":"generatedContent/workshop-setup/workshop/pre-work/TOOLS-CLI/#source-to-image-s2i","text":"To install s2i CLI tool, Download tar file. curl -s https://api.github.com/repos/openshift/source-to-image/releases/latest \\ | grep browser_download_url \\ | grep linux-amd64 \\ | cut -d '\"' -f 4 \\ | wget -qi - Unzip tar file tar xvf source-to-image*.gz Make s2i CLI accessiblee. sudo mv s2i /usr/local/bin verify s2i version With that done, you can start the lab.","title":"Source-to-Image (S2I)"},{"location":"generatedContent/workshop-setup/workshop/pre-work/WEBTERMINAL/","text":"Login to Web Terminal \u00b6 Please ask your instructor for the link to the terminal environment if you haven't been directed to one already. Login to IBM Cloud using ibmcloud login . If asked, choose Advowork as your target account. For convenience, export your cluster name as an environment variable. export CLUSTER_NAME = <your_cluster_name> We can now configure the kubectl cli available within the terminal for access to your cluster. If you stored your cluster name to an environment variable (ie. $CLUSTER_NAME ), use that variable, otherwise copy and paste your cluster name from the previous commands output to the $CLUSTER_NAME portion below. ibmcloud ks cluster config --cluster $CLUSTER_NAME Verify access to the Kubernetes API by getting the namespaces. kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m You should now be read to start with labs. Note: Every time you log in to the Cloud Shell (or your CLI tools), you must run the above commands to connect with the cluster in IBM Cloud.","title":"Login to Web Terminal"},{"location":"generatedContent/workshop-setup/workshop/pre-work/WEBTERMINAL/#login-to-web-terminal","text":"Please ask your instructor for the link to the terminal environment if you haven't been directed to one already. Login to IBM Cloud using ibmcloud login . If asked, choose Advowork as your target account. For convenience, export your cluster name as an environment variable. export CLUSTER_NAME = <your_cluster_name> We can now configure the kubectl cli available within the terminal for access to your cluster. If you stored your cluster name to an environment variable (ie. $CLUSTER_NAME ), use that variable, otherwise copy and paste your cluster name from the previous commands output to the $CLUSTER_NAME portion below. ibmcloud ks cluster config --cluster $CLUSTER_NAME Verify access to the Kubernetes API by getting the namespaces. kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m You should now be read to start with labs. Note: Every time you log in to the Cloud Shell (or your CLI tools), you must run the above commands to connect with the cluster in IBM Cloud.","title":"Login to Web Terminal"},{"location":"pre-work/","text":"Pre-work \u00b6 This section is broken up into the following steps: Sign up for IBM Cloud Download or clone the repo 1. Sign up for IBM Cloud \u00b6 Ensure you have an IBM Cloud ID 2. Download or clone the repo \u00b6 Various parts of this workshop will require the attendee to upload files or run scripts that we've stored in the repository. So let's get that done early on, you'll need git on your laptop to clone the repository directly, or access to GitHub.com to download the zip file. To Download, go to the GitHub repo for this workshop and download the archived version of the workshop and extract it on your laptop. Alternately, run the following command: git clone https://github.com/IBM/workshop-template cd workshop-template","title":"Pre-work"},{"location":"pre-work/#pre-work","text":"This section is broken up into the following steps: Sign up for IBM Cloud Download or clone the repo","title":"Pre-work"},{"location":"pre-work/#1-sign-up-for-ibm-cloud","text":"Ensure you have an IBM Cloud ID","title":"1. Sign up for IBM Cloud"},{"location":"pre-work/#2-download-or-clone-the-repo","text":"Various parts of this workshop will require the attendee to upload files or run scripts that we've stored in the repository. So let's get that done early on, you'll need git on your laptop to clone the repository directly, or access to GitHub.com to download the zip file. To Download, go to the GitHub repo for this workshop and download the archived version of the workshop and extract it on your laptop. Alternately, run the following command: git clone https://github.com/IBM/workshop-template cd workshop-template","title":"2. Download or clone the repo"},{"location":"resources/ADMIN/","text":"Admin Guide \u00b6 This section is comprised of the following steps: Instructor Step 1. Instructor Step \u00b6 Things specific to instructors can go here.","title":"Admin Guide"},{"location":"resources/ADMIN/#admin-guide","text":"This section is comprised of the following steps: Instructor Step","title":"Admin Guide"},{"location":"resources/ADMIN/#1-instructor-step","text":"Things specific to instructors can go here.","title":"1. Instructor Step"},{"location":"resources/CONTRIBUTORS/","text":"Contributors \u00b6 Remko de Knikker \u00b6 Github: remkohdev Twitter: @remkohdev LinkedIn: remkohdev Medium: @remkohdev Steve Martinelli \u00b6 Github: stevemar Twitter: @stevebot LinkedIn: stevemar","title":"Contributors"},{"location":"resources/CONTRIBUTORS/#contributors","text":"","title":"Contributors"},{"location":"resources/CONTRIBUTORS/#remko-de-knikker","text":"Github: remkohdev Twitter: @remkohdev LinkedIn: remkohdev Medium: @remkohdev","title":"Remko de Knikker"},{"location":"resources/CONTRIBUTORS/#steve-martinelli","text":"Github: stevemar Twitter: @stevebot LinkedIn: stevemar","title":"Steve Martinelli"},{"location":"resources/MKDOCS/","text":"mkdocs examples \u00b6 This page includes a few neat tricks that you can do with mkdocs . For a complete list of examples visit the mkdocs documentation . Code \u00b6 print ( \"hello world!\" ) Code with line numbers \u00b6 1 2 3 4 5 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Code with highlights \u00b6 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Code with tabs \u00b6 Tab Header #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } Another Tab Header #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } More tabs \u00b6 Windows If on windows download the Win32.zip file and install it. MacOS Run brew install foo . Linux Run apt-get install foo . Checklists \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst Add a button \u00b6 Launch the lab Visit IBM Developer Sign up! Call outs \u00b6 Tip You can use note , abstract , info , tip , success , question warning , failure , danger , bug , quote or example . Note A note. Abstract An abstract. Info Some info. Success A success. Question A question. Warning A warning. Danger A danger. Example A example. Bug A bug. Call outs with code \u00b6 Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim. Formatting \u00b6 In addition to the usual italics , and bold there is now support for: highlighted underlined strike-through Tables \u00b6 OS or Application Username Password Windows VM Administrator foo Linux VM root bar Emojis \u00b6 Yes, these work. Images \u00b6 Nunc eu odio eleifend, blandit leo a, volutpat sapien right align image \u00b6 Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"mkdocs examples"},{"location":"resources/MKDOCS/#mkdocs-examples","text":"This page includes a few neat tricks that you can do with mkdocs . For a complete list of examples visit the mkdocs documentation .","title":"mkdocs examples"},{"location":"resources/MKDOCS/#code","text":"print ( \"hello world!\" )","title":"Code"},{"location":"resources/MKDOCS/#code-with-line-numbers","text":"1 2 3 4 5 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Code with line numbers"},{"location":"resources/MKDOCS/#code-with-highlights","text":"def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Code with highlights"},{"location":"resources/MKDOCS/#code-with-tabs","text":"Tab Header #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } Another Tab Header #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; }","title":"Code with tabs"},{"location":"resources/MKDOCS/#more-tabs","text":"Windows If on windows download the Win32.zip file and install it. MacOS Run brew install foo . Linux Run apt-get install foo .","title":"More tabs"},{"location":"resources/MKDOCS/#checklists","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst","title":"Checklists"},{"location":"resources/MKDOCS/#add-a-button","text":"Launch the lab Visit IBM Developer Sign up!","title":"Add a button"},{"location":"resources/MKDOCS/#call-outs","text":"Tip You can use note , abstract , info , tip , success , question warning , failure , danger , bug , quote or example . Note A note. Abstract An abstract. Info Some info. Success A success. Question A question. Warning A warning. Danger A danger. Example A example. Bug A bug.","title":"Call outs"},{"location":"resources/MKDOCS/#call-outs-with-code","text":"Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.","title":"Call outs with code"},{"location":"resources/MKDOCS/#formatting","text":"In addition to the usual italics , and bold there is now support for: highlighted underlined strike-through","title":"Formatting"},{"location":"resources/MKDOCS/#tables","text":"OS or Application Username Password Windows VM Administrator foo Linux VM root bar","title":"Tables"},{"location":"resources/MKDOCS/#emojis","text":"Yes, these work.","title":"Emojis"},{"location":"resources/MKDOCS/#images","text":"Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"Images"},{"location":"resources/MKDOCS/#right-align-image","text":"Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"right align image"},{"location":"resources/RESOURCES/","text":"Additional resources \u00b6 IBM Demos \u00b6 Collection: InfoSphere Information Server Tutorial: Transforming your data with IBM DataStage Redbooks \u00b6 IBM InfoSphere DataStage Data Flow and Job Design InfoSphere DataStage Parallel Framework Standard Practices Videos \u00b6 Video: Postal codes and part numbers (DataStage) Video: Find relationships between sales, employees, and customers (Information Analyzer) Video: Clean and analyze data (Governance Catalog)","title":"Additional resources"},{"location":"resources/RESOURCES/#additional-resources","text":"","title":"Additional resources"},{"location":"resources/RESOURCES/#ibm-demos","text":"Collection: InfoSphere Information Server Tutorial: Transforming your data with IBM DataStage","title":"IBM Demos"},{"location":"resources/RESOURCES/#redbooks","text":"IBM InfoSphere DataStage Data Flow and Job Design InfoSphere DataStage Parallel Framework Standard Practices","title":"Redbooks"},{"location":"resources/RESOURCES/#videos","text":"Video: Postal codes and part numbers (DataStage) Video: Find relationships between sales, employees, and customers (Information Analyzer) Video: Clean and analyze data (Governance Catalog)","title":"Videos"}]}